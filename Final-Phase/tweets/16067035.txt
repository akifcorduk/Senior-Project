RT @omerlevy_: Recurrent Additive Networks: where we find that less is more in the land of LSTM.
https://t.co/ksefrUh2r3 [w/ @kentonctlee @‚Ä¶ RT @vorpalsmith: Hello #PyCon2017! Did you know that the NumPy project is hiring? If you're interested hit me up for details ‚Äì I'm here unt‚Ä¶ RT @cournape: the flang effort is finally open sourced: https://t.co/GElnh7kKoT. Finally a working ‚Äúnative‚Äù OSS fortran compiler for window‚Ä¶ RT @ylecun: Prediction is the essence of intelligence.

But, as this nice ICML2017 paper from the Berkeley crowd shows,... https://t.co/6Fg‚Ä¶ RT @ilyasut: https://t.co/n9iyi9AUnG -- a robot that can instantly be taught new skills in VR RT @NandoDF: Data Science Bowl 2017, Predicting Lung Cancer: Solution Write-up, Team Deep Breath https://t.co/Y26C5I3Scr RT @soumithchintala: TPU-2 looks pretty awesome. 180 TOps, floating point (?). Also has large register file? Need the whitepaper asap!
http‚Ä¶ RT @volkuleshov: Stanford researchers announced new 500TB Medical ImageNet dataset at #gtc. The project webpage is already up: https://t.co‚Ä¶ RT @PyParisFr: Last day to benefit from the early birds rates. https://t.co/lkY3X8dH5Z RT @teoliphant: Awesome to see Numba 0.33 released! https://t.co/HO1M2Y772D 2x-10x speed improvements due to benchmarks provided by #snorke‚Ä¶ @novaruth @Filastine thanks for the great show last night! I really like your new album. RT @PyTorch: Inferring and Executing Programs for Visual Reasoning https://t.co/Ln8BpwdZVD https://t.co/nCGjGEr8OL RT @quantombone: Inferring and Executing Programs for Visual Reasoning. (All-star authors too) https://t.co/VjsIS9WprM #deeplearning #compu‚Ä¶ RT @RichardSocher: A new reinforcement and deep learning model for summarization.It generates long language sequences that make sense. 
htt‚Ä¶ @pineapplepolis @CassiniSaturn I assume this is stable because this athmosphere is a reducing environment (no molecular oxygen or ozon) RT @nvidia: Announcing CUDA 9 with up to 5x faster libraries, a new programming model, support for Volta #GPUs &amp; more. #GTC17 https://t.co/‚Ä¶ RT @PyTorch: We will be supporting the @nvidia Volta line of chips, CUDA 9, CuDNN 7 and NCCL 2 from the day one. More about Volta
https://t‚Ä¶ RT @jwan584: 12x training perf. 6x inference perf. #GTC2017 https://t.co/MC4esQiomq RT @jwan584: A TensorOp appears to be a 4x4 matrix multiply-add. https://t.co/KE9dWyE8oH RT @peteskomoroch: Microsoft announces ML service for cloud training Google‚Äôs TensorFlow, Microsoft‚Äôs own Cognitive Toolkit or Caffe https:‚Ä¶ @soumithchintala @nvidia Are they floating point ops? What precision? @soumithchintala @nvidia What are "tensor tflops"? RT @soumithchintala: jen-hsun annouces @Nvidia Volta. 120 Tensor TFLOPS. o_O https://t.co/MSZF4Fkqov RT @NandoDF: Differentiable neural computers as alternatives to parallel Bayesian optimization for  hyperparameter tuning of other networks‚Ä¶ RT @peteskomoroch: FMA: A Dataset For Music Analysis. 1TB dump of the Free Music Archive (FMA), an interactive library of high-quality, leg‚Ä¶ RT @stonebigdotdot: stackoverflow trending tool shows a brutal cross-over between Python-2.7 and 3.x at mid-2016 : https://t.co/bDh3cHSbkW RT @TwitterEng: We‚Äôre using deep learning (at scale!) in our timelines. Learn more: https://t.co/MxC3YX6SJZ RT @math_rachel: Distributed @PyTorch is coming soon (multi-GPU training).
JIT compilation is being planned.
-@soumithchintala #GTC17 #Deep‚Ä¶ RT @syhw: and pre-trained models! https://t.co/5kxwKar1Jk RT @syhw: SOTA machine translation with ConvNets https://t.co/MWGAScAbtp
paper: https://t.co/ervPNa6q7o
code: https://t.co/su7A1tfdoH RT @jwan584: Stanford gave the world ImageNet. Now it's giving the world Medical ImageNet‚Äîa 0.5PB dataset for diagnostic radiology. #GTC201‚Ä¶ RT @PyTorch: A playground of vision models in @PyTorch, showcasing quantization at 12-bit, 10-bit, 8-bit and even 6-bit:
https://t.co/Jy5a5‚Ä¶ RT @guillaumeallain: @pydatalondon Slides of my talk of recommender systems with #TensorFlow https://t.co/BRcC6wDSx0 #PyDataLdn @SchibstedG‚Ä¶ RT @DataParis: New meetup May 23rd at Vente Priv√©e. GPU, Neural Nets and CRDTs. https://t.co/q3zqSQsArj RT @_rockt: Great thread on style transfer! https://t.co/YxWzdAD92O RT @jekbradbury: This is super cool and major üëè to FAIR and Jason for open-sourcing it -- wraps @PyTorch and 20+ NLP/dialog datasets https:‚Ä¶ RT @cangermueller: Fully convolutional ResNet for segmenting medical images: https://t.co/bFd8v4tZqa https://t.co/FnM2EzPb1e RT @PyParisFr: We‚Äôre urgently looking for a few more sponsors. Can you help us ? https://t.co/NrZIAG7UQL @denormalize Bonus point if it also generates a clean mobile friendly reflowable html or epub. RT @denormalize: anyone have an example github repo showing a markdown paper + pandoc build that generates a preprint ready upload (e.g. ar‚Ä¶ RT @peteskomoroch: Anyone know of good Python or command line tools for working with @wikidata dumps? Seems like a big tooling gap/need for‚Ä¶ RT @culurciello: I just published ‚ÄúNavigating the unsupervised learning landscape‚Äù https://t.co/5ANdSVn2zV Please listen &amp; appreciate: electro / bass / dub / break. https://t.co/5RLa2n8JlW RT @PyTorch: Sparse CUDA Tensors, new layers, performance and bug fixes: v0.1.12 is out, our last 0.1 release. Read more here:
https://t.co‚Ä¶ @soumithchintala Thanks anyway. @soumithchintala Any plan for a universal multilingual embedding model? (possibly using the translation links as a‚Ä¶ https://t.co/kcE1kG9PrY RT @soumithchintala: Now supports 294 languages, and introducing https://t.co/wD0mGQuIuC for smaller memory devices: https://t.co/vFXtU7n75‚Ä¶ @quasimondo nice one :) next steps is style transfer on videos with some temporal consistency RT @rasbt: And end-to-end automatic speech recognition system implemented in TensorFlow https://t.co/u3AEWY933k RT @Mvandepanne: Our most fun project yet! DeepRL for physics-based running, soccer-dribbling, and more!  https://t.co/gBkTA25I2m (sound: A‚Ä¶ RT @agramfort: [JOB] Junior engineer position to work on @scikit_learn at #INRIA Lille on  metric learning https://t.co/gkYRYLZApS #python‚Ä¶ RT @mrocklin: New blogpost on recent and ongoing changes within #Dask

https://t.co/yS6yZbGNRz

Parallel SKLearn, Joblib, Parquet, Sparse,‚Ä¶ RT @MirowskiPiotr: Directors of top French research institutes (CNRS, INRIA, INSERM, INRA, IFREMER...) will vote against Marine Le Pen: htt‚Ä¶ Interesting presentations at #ICLR2017 by @beenwrekt and others on generalization and optimization in deep nets: https://t.co/EbY3aG2YGg RT @tensorflow: TensorFlow v1.1.0 has been released! Please see the full release notes for details on added features and changes: https://t‚Ä¶ @tdhopper Great poll ; RT @xamat: "I don‚Äôt have direct deep learning (...)experience, (but what) really helped me (...) is planning &amp; building (...) lots of autom‚Ä¶ RT @Miles_Brundage: "Adversarial Neural Machine Translation," Wu et al.: https://t.co/0ANKD9Spuf RT @goodfellow_ian: #iclr2017 at 4:30 PM, come check out poster C16, "Adversarial Machine Learning at Scale" https://t.co/eVsjcgVeng https:‚Ä¶ RT @fpedregosa: Had a great time presenting our work at #AISTATS2017, can't wait for the next edition! https://t.co/W58K5IVRio RT @adnothing: Understanding #DeepLearning requires rethinking generalization https://t.co/hTz1ESg34T Best paper #ICLR2017 https://t.co/OkV‚Ä¶ RT @fhuszar: My review of "Unsupervised Learning by Predicting Noise":
An Information Maximization Interpretation
https://t.co/RLUucarnLy @fhuszar invertable, upper nound, invertability RT @clmt: That's a great way to think about sgd and why it works much better than LBFGS for DNNs. https://t.co/89SezowtNk @fhuszar LaTeX typo (truncated expression): $dim(Z)&lt; RT @yoavgo: Here we go.
https://t.co/C0EEasnpAz RT @cvondrick: amazing generations of the video future! Red border means output, green is input. https://t.co/PByeC0rtnQ https://t.co/LaHm0‚Ä¶ RT @hardmaru: Softmax GAN, by Min Lin. Yet another simple and neat GAN training trick. Seems to help with mode collapse problem. https://t.‚Ä¶ RT @kscottz: Today I get to release my latest project: the largest data set of Amazon deforestation images ever, and $60K in prize money on‚Ä¶ RT @mark_riedl: 110k+ story plots from English Wikipedia for your (neural net‚Äôs) reading pleasure https://t.co/HVbAM5MW6H

(plus code to do‚Ä¶ RT @GiorgioPatrini: A controversial talk on failures of optim for deep learning: piece-wise curves, flatness, end-to-end training https://t‚Ä¶ @mmparker @ansate In python 3.6 they have. RT @amuellerml: Yellowbricks is a visualization tool for sklearn estimators (etc) https://t.co/Wwr9vGFjKX similar to https://t.co/tcdjftmnz‚Ä¶ @jeremyphoward @PyTorch @tensorflow I observed a similar speedup ratio with a Wide Resnet on cifar. I haven't tried to profile though. RT @cvondrick: Unsupervised Learning by Predicting Noise by Bojanowski and Joulin. Cool yet simple idea that works quite well!! https://t.c‚Ä¶ RT @mrocklin: Algorithms for distributed asynchronous optimization with #Dask and #PyData

https://t.co/l6suWa2x1h

New blogpost with @mark‚Ä¶ RT @DeepMindAI: We've open sourced the Differentiable Neural Computer! Built with #Sonnet and #TensorFlow. Available here: https://t.co/sJr‚Ä¶ RT @olivercameron: Big news from @BaiduResearch: they're open sourcing their self-driving car OS in July. https://t.co/2a8A6b8vRj RT @JakeRozmaryn: WOW. @PoweredByEos is now the 1st #energystorage co to sell DC #batteries for below $100/kWh #milestone #ESACon17 https:/‚Ä¶ RT @goodfellow_ian: Differentially private ML needs to balance low error rate with low privacy cost. PATE-G (Papernot et al 2017) achieves‚Ä¶ RT @goodfellow_ian: Nice comparison of differentially private ML benchmarks put together by @npapernot to track state of the art: https://t‚Ä¶ RT @hardmaru: MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (Google) https://t.co/bOBQznvPxM https://t‚Ä¶ RT @shakir_za: Announcing the Deep Learning Indaba; to stimulate participation of fellow Africans in Machine Learning. Help spread the word‚Ä¶ RT @betatim: .@LinDigressions checkout https://t.co/JaVJLbsr0f and https://t.co/ceqSea0EpP for GANs and fair(er) ML cc @glouppe RT @SaileNav: 9th place in Kaggle's lung cancer prediction competition!
#datascibowl Read about our approach here: https://t.co/udyPZNQRbx RT @abigail_e_see: Check out the blog post "Taming Recurrent Neural Networks for Better Summarization" for our new @acl2017 paper! https://‚Ä¶ RT @ryan_sb: Great news from @awscloud -- #Python 3.6 is going to be available in AWS #Lambda in the next few days RT @nytimes: He created a database to "figure out what the government really does with the money." What he found surprised him https://t.co‚Ä¶ RT @DanielIGolden: Our paper comparing U-Net to ENet for ventricular segmentation is out! @jliemansifry @SeanPatrickSall @phelixlau @leminh‚Ä¶ RT @thephysicsHB: Stream our second record 'Mercury Fountain' in full via @Independent - https://t.co/W3rAoTXfdb @guillaumeallain @pydatalondon Thanks, indeed our two presentation will have much in common. RT @yoavgo: Our contribution to NMT: target side syntax helps. And easy to incorporate. Yay syntax! https://t.co/C77HJqExuk RT @hardmaru: Get To The Point: Summarization with Pointer-Generator Networks. By @abigail_e_see, Peter J. Liu, and @chrmanning. https://t.‚Ä¶ RT @twiecki: Stochastic Gradient Descent as Approximate Bayesian Inference https://t.co/pvHha1azXv New paper from Blei-Lab. RT @soumithchintala: ProTip from @ylecun while discussing @pytorch's upcoming feature to add Hessian-vector products (among differentiating‚Ä¶ @FlorianWilhelm @pbnsilva More about concepts. For impl, I would recommend lightfm even if not strictly neural at this point. I will give a presentation on Neural Recommender Systems at dotAI in Paris on April 25. https://t.co/t8PHDe4Mj4 RT @Miles_Brundage: :-O https://t.co/vrLMxwoct1 https://t.co/YDtby5jJgG RT @amuellerml: 4th Workshop on Fairness, Accountability, and Transparency in Machine Learning (fatml) CfP (co-located with KDD)
https://t.‚Ä¶ RT @alexjc: Incredible! A Neural Parametric Singing Synthesizer #ML https://t.co/7X5Cq3SZb7 Female Spanish voice is outstanding, mixed w/ a‚Ä¶ RT @betatim: @ogrisel you should ask your students to extend it like this: https://t.co/hJNoiJ57XU Actually it's the other way around in the tutorial. But I tested it both ways and it works to 100% test accuracy with 4000 training samples. Notebook for a simple Seq2Seq tutorial with Keras: translate digital representation of numbers into French phrases:‚Ä¶ https://t.co/WlLoIqfopt Tethered wind power startup @AmpyxPower funded to run tests in Ireland https://t.co/SNl0vCwjbj after KPS in Scotland https://t.co/SG4MDXoK0z RT @yoavgo: Is it legit to compare char-based vs. token-based LMs on perplexity? discuss. RT @chris_naesseth: AISTATS proceedings are out! https://t.co/i9f12y2G6X #AISTATS2017 #ML RT @mat_kelcey: i wish people did ablation studies more. they give me the most intuition (apart from coding myself) e.g. from cyclegan http‚Ä¶ @haldaume3 And also https://t.co/CQT8fnUQzR @haldaume3 Sampling noise or injected noise (data augmentation, dropout, vae reparametrization trick noise...)? For‚Ä¶ https://t.co/5Quo3MmaRW RT @OriolVinyalsML: Next chapter for #AlphaGo in May! Thankfully after the #NIPS deadline :) https://t.co/IJInXecaAV https://t.co/SFU9hcNLsl RT @goodfellow_ian: CycleGAN turning a horse video into a zebra video ( https://t.co/YYCsVt4rIP ) https://t.co/KlZlKG5k6W @mgershoff No pbm Iterates of SGD on the loss landscape of a 2 params ReLU net doing least squares regression on noisy linear data:‚Ä¶ https://t.co/zwBxWRuwjO RT @shoyer: Our paper on xarray is finally out! https://t.co/9hLMTXqo3K @soumithchintala @pmarca Very interesting itw (video), thanks for sharing. But I disagree: 20M people change jobs /‚Ä¶ https://t.co/JIS9wBUHiy RT @sleepinyourhat: Our new sentence understanding dataset has grown to its full size‚Äî433k sentence pairs. #proudcorpusfather https://t.co/‚Ä¶ RT @theophaneweber: Last NIPS Shakir, LB and I wrote a note with some ideas relating FD to score-function estimators- related to recent dis‚Ä¶ RT @fhuszar: more on Evolution Strategies: Variational Optimisation and Natural ES
https://t.co/F7AbEWEVEw RT @OpenAI: A neural network teaches itself to analyze sentiment after only being trained to predict the next character: https://t.co/hrITH‚Ä¶ RT @jimmfleming: Nice, apparently, TensorFlow (experimental/contrib) now supports an imperative style: https://t.co/9mreAlGC0S ‚Äî much bette‚Ä¶ RT @sedielem: I've been working on WaveNet autoencoders with @GoogleBrain Magenta. blog post: https://t.co/o4oDVRl9Rh paper: https://t.co/I‚Ä¶ RT @OriolVinyalsML: Pretending to do real science (as opposed to ML) is fun! Neural Message Passing for Quantum Chemistry by Justin&amp;Sam htt‚Ä¶ @j_dreo @davidobarber I would love to read a sequel post that contrasts this approach w/ Natural ES‚Ä¶ https://t.co/4Ruou1yzS7 RT @davidobarber: Connecting Evolutionary and Variational Optimization https://t.co/U8XS5e26Iw https://t.co/OHqcZn3qLv RT @jponttuset: DAVIS 2017 Train + Val dataset is available! üéâüéä
Multiple-object video segmentation.
https://t.co/4ZUswIQbBv @skprat https:/‚Ä¶ @fhuszar @NandoDF @ziyuwang Url? RT @jiminy_crist: Introducing Dask-SearchCV, a library for distributed hyper-parameter optimization using Dask and Scikit-Learn. https://t.‚Ä¶ RT @hardmaru: Quantifying the performance of the TPU. https://t.co/ZTEl367Bnl RT @soumithchintala: Google's TPU paper is out. 8-bit matmul processor, large on-die memory (28MB). Excited to finally read the details..
h‚Ä¶ RT @karpathy: Came to visit first class of @cs231n at Stanford. 2015: 150 students, 2016: 350, this year: 750. #aiinterestsingularity https‚Ä¶ RT @CharlesOllion: We organize Deep Learning Meetup at @heuritechdata April 19th. Sentence embedding, GANs, multimodal siamese networks htt‚Ä¶ RT @NumFOCUS: Jupyter Notebook version 5.0 has been released: https://t.co/F11HNxvH7M | pip install --upgrade notebook | Congrats to @Proje‚Ä¶ RT @distillpub: Why Momentum Really Works -- A new Distill article by @gabeeegoooh https://t.co/47DD7fzFwA https://t.co/5q2iZyFhvR RT @boredyannlecun: To reach me from now on, use a differentiable communication channel so I can provide useful feedback. https://t.co/203a‚Ä¶ RT @karpathy: GANs seem to improve on timescales of weeks; getting harder to keep track of. Another impressive paper and I just barely skim‚Ä¶ RT @alexjc: Improved Training of Wasserstein GANs https://t.co/XTbRlQSune More stable, no hyper-parameter tuning, works on 101-ResNet! #ML‚Ä¶ @arnicas @amuellerml https://t.co/3BQdAcqf5F https://t.co/Pf2vEwCdSd RT @PyTorch: CuDNN v6 integration, improved Multi-GPU perf, new layers, many bug fixes
v0.1.11 release notes: https://t.co/KjGnfc5Z6a RT @PyTorch: torchvision v0.1.8: More models (SqueezeNet, Inception, DenseNet) w/ pre-trained weights, more datasets (SVHN, etc.) https://t‚Ä¶ RT @ilblackdragon: Stunning results in face generation. https://t.co/3lqrFar8D7 "BEGAN: Boundary Equilibrium Generative
Adversarial Network‚Ä¶ @giessel The symmetry breaking argument of https://t.co/3BQdAcqf5F is interesting too. @fhuszar @CharlesOllion I think I wont be in paris that we. Keys exchange would be complicated to organize so bette‚Ä¶ https://t.co/a0M4Gysjn3 @giessel Not yet. @fhuszar @NandoDF @ziyuwang But still I recommend reading the Natural ES paper I linked. It's very interesting even‚Ä¶ https://t.co/KsAwfveen0 RT @caglar_ee: Video lectures for Deep Reinforcement Learning by Sergey Levine, John Schulman, Chelsea Finn https://t.co/UcFzeQxfig https:/‚Ä¶ @fhuszar @NandoDF @ziyuwang BTW Natural ES https://t.co/C6lH1kEXh9 does not seem to work that well (compared to e.g‚Ä¶ https://t.co/dBw0aAoZfD RT @CharlesOllion: CycleGAN. Visually impressive results. Several papers on unpaired Image-to-Image translation these days https://t.co/iUx‚Ä¶ @F_Vaggi Also no formula for conv / pooling layers, just fc layers. But verified empirically for convnets. @fhuszar @NandoDF @ziyuwang Adam uses a lagged estimate of the sqrt of the diagonal of the inverse empirical Fisher‚Ä¶ https://t.co/GRfxdCxC2z @F_Vaggi Yes with relus. Although this holds only for networks with random weights. @F_Vaggi It's not a complete theory of deep nets but explain a lot of the nature of the problems faced when trainin‚Ä¶ https://t.co/K7T3gMHriC @F_Vaggi There are closed form formulas for correlations between gradients of net outputs wrt internal units for fc nets. Shattered Gradients: great insights on why resnets and batchnorm work so well to train deep nets by Balduzzi et al.: https://t.co/Pf2vEwTOJL RT @PyTorch: For the Wavelet folks: Scattering Convolution Networks -- fully GPU powered, 225x faster over CPU. 
https://t.co/LzTegPVr2T ht‚Ä¶ RT @tarek_ziade: Once again, if you have something cool to show in Python, we are looking for speakers in Paris in June 
https://t.co/oed2i‚Ä¶ RT @jackclarkSF: We're reaching the uncanny valley of AI-generated text-to-speech. Next: REALLY CONVINCING SPAM CALLS https://t.co/RMaFC21H‚Ä¶ RT @hardmaru: ‚ÄúThisss isrealy awhsome.‚Äù Seems to work for speling mistaks. Audio samples from Tacotron: https://t.co/yLln0mxF99 https://t.c‚Ä¶ RT @cangermueller: ResNets for predicting genetic mutations from only few microscopy images: https://t.co/Mo7xEUBLjy https://t.co/G1U1k2Cs7n RT @ebelilov: Check out our new paper with scattering networks on imagenet from the Paris mob https://t.co/ElWl4rZxD7 @szagoruyko5 https://t.co/XqClQYicH1 is down... RT @hardmaru: The Vector Institute doesn't seem to have a website yet. Are they trying to be like the "OpenAI" of the North? https://t.co/2‚Ä¶ RT @VictorStinner: Blog: https://t.co/2CvSTOe7hu results, March 2017: https://t.co/cBYWQxCHZR Python 3.7 compared to Python 2.7, 
significa‚Ä¶ @cangermueller congrats! RT @kubernetesio: Kubernetes 1.6 is here! Multi-user, Multi-workloads at Scale https://t.co/DolGvzdpik #KubeCon RT @goodfellow_ian: We already knew about "universal adversarial perturbations" in 2014. https://t.co/j4Q3KWCxtd @arnicas along with (reverse) ssh tunnel configs in my .ssh/config for jupyter notebook and ratom or rcode (rmate for atom or vscode) @arnicas rtmux which is part of the autossh package on ubuntu. RT @mrocklin: Dask and Pandas and XGBoost: playing nicely between distributed systems.  https://t.co/0hNuHTGVzO #PyData RT @kastnerkyle: Sequence-to-Sequence Models Can Directly Transcribe Foreign Speech
Weiss et al , https://t.co/PlULMOKu8i RT @weballergy: 'Biologically Inspired Protection of Deep Networks from Adversarial Attacks' https://t.co/7W8XgHVkDF #deeplearning  #machin‚Ä¶ RT @albietz: my C++/Cython code for some stochastic optimization / variance reduction algorithms is now on github: https://t.co/YaTWjMIqOQ RT @ilyasut: Sim-to-real transfer for robotics perception that works pretty well: https://t.co/B6s6KKWL3c RT @hardmaru: @hardmaru VIN won a best paper award at NIPS2016. One of my favourites. Here is an article describing how it works: https://t‚Ä¶ RT @hardmaru: PyTorch implementation of Value Iteration Networks (VIN). Clean, Simple and Modular. Visualization in Visdom. https://t.co/Si‚Ä¶ @trustswz nice hat :) @F_Vaggi @trustswz actually a distribution of plausible faces. How multimodal is it in practice? RT @trustswz: SRGAN on 16x celebA SR. From left to right, NN, bicubic, ResNet, SRGAN. It is not perfect yet. https://t.co/BgeRJ0n8ZV RT @OriolVinyalsML: Our lip reading work will be presented as an oral at CVPR! You can read more at https://t.co/LKGzak5Cw9 https://t.co/dA‚Ä¶ Slides and notebooks for the deep learning course @CharlesOllion and I taught at @UnivParisSaclay https://t.co/YITTmgGehg recsys vision nlp @rishmishra @sknthla I agree but I question the "pubicly feminist implies privately sexist" assertion. @sknthla sexism in tech (and non tech) companies is not just in the Valley, it happens in many places around the world. @sknthla do you believe all militant male feminists are fake? By militant I mean people who eg promote women in tech and speak up on twitter RT @b0rk: bash tips (attached blog post: https://t.co/EPmt7BTP98) https://t.co/S1lpNsjADI @sknthla I could be wrong but I think for a man it's easier to silently ignore sexist behaviors to avoid risking to offend male colleagues @sknthla honestly I find this statement hard to believe. I don t know how to verify if it's the case or not (besides anecdotal evidence). RT @ericjang11: Excess paranoia about strong AI risk &amp; AI safety distracts discussion from real worker's issues. Causes real damage to real‚Ä¶ @rblourenco the plan is to make it the default backend for joblib so that it can be used in scikit-learn for CPU-intensive machine learning RT @OpenAI: New research release: overcoming many of Reinforcement Learning's limitations with Evolution Strategies: https://t.co/fVIStOFdoU @dovgalec it's a htop screenshot when I was load testing and benchmarking loky on a 32 cores host. Note that @tomMoral will soon contribute many of the fixes implemented in loky to upstream Python 3.7 dev https://t.co/6wTaAjvrT7 loky 0.2 released: https://t.co/fCKlISmKPR a ProcessPoolExecutor that never deadlocks (hopefully) #python https://t.co/BCriCLAv4L RT @turbodbc: Turbodbc 1.1.0 is out with improved unicode support for #MSSQL and experimental Windows sdist builds! #Python #ODBC https://t‚Ä¶ RT @mrocklin: Demonstration of Dask running well on Google's cloud infrastructure including Kubernetes and Google Cloud Storage with Parque‚Ä¶ RT @abursuc: No fuss distance metric learning using proxies 
https://t.co/4eLiWENdzE Triplets-based methods are still on RT @mrocklin: Developing Convex Optimization Algorithms in Dask.  New blogpost by Chris White and myself: https://t.co/wW741hY2EA https://t‚Ä¶ RT @abursuc: Domain randomization for transferring DNN policies from simulation to the real world (without pre-training on real) https://t.‚Ä¶ RT @fpedregosa: Thanks to @bbengfort memory_profiler can now separately track memory usage of forked processes https://t.co/LCOMLgNzM8 http‚Ä¶ RT @glouppe: Open source is a lot of underappreciated work. But some mornings you get a genuine thank you note https://t.co/NWPuDDhare @sci‚Ä¶ RT @PyTorch: OpenAI's new paper "Evolution Strategies as a Scalable Alternative to Reinforcement Learning" implemented in @PyTorch https://‚Ä¶ RT @soumithchintala: I haven't seriously read a paper in a month.
Wasserstein GAN cited 17 times since arxived &lt; 2 months
the field is over‚Ä¶ RT @gabrielpeyre: It is time to re-read a classic! #abelprize https://t.co/HlP14SSxj3 RT @kastnerkyle: Mask R-CNN https://t.co/0inQgahBcB, fast multi-task architecture for detection, segmentation, keypoints. He et. al. from F‚Ä¶ @Maciej_Kula @mElantkowski would be curious to get your feedback on: https://t.co/FYzrust69a RT @distillpub: Distill is a interactive, visual journal for machine learning research. https://t.co/BZegy07cmL https://t.co/P4qCe8wgHB @iand I don't care, this is why I said I would buy a product or a service, not labour. @iand not if you delegate to a robot you own. @iand the pbm is more about how to ensure that the majority of people get decent income levels when labour is rarified. @iand I wouldnt buy "labour" in the first place but a product or a service. @iand but I don't see how a shared robot ownership can be enforced without harming  property and freedom to use capital for investing. @iand to clarify my point: I am not a UBI advocate (so far). I don't know for sure what the solution is. @iand so you want to give capital income to people who ve lost labour income opportunities. Taxes + basic income might be able achieve that. @jessenoller @everett_toews I don't have any short term project to run on a kube cluster in mind, I was just curious. @iand you mean "people" instead of "workers" right? RT @BecomingDataSci: Started collecting my bookmarks on Bias in Machine Learning in this new @flipboard magazine. Send me good ones! https:‚Ä¶ @everett_toews @GetCarina is the fully managed kubernetes offering already available? RT @Tim_Dettmers: I updated my GPU advice blog post with the GTX 1080 Ti; also cleaned it so it is easier to find relevant information http‚Ä¶ RT @kastnerkyle: Brief "writeup" about char2wav noise https://t.co/ZSxGa6RCdL . Weirdest sounds at the end of the youtube playlist cc @hard‚Ä¶ RT @spacy_io: spaCy v1.7.0: New ~50 MB model, CLI, better downloads and lots of bug fixes. Plus, models are now Python packages üì¶  https://‚Ä¶ RT @haldaume3: Are you sick of #nlproc folks saying NLP is "solved?" Think your linguistics can poke holes in systems? Prove it! https://t.‚Ä¶ RT @googleresearch: Releasing the Skip-Thought Vectors model in @tensorflow, that can encode the semantic properties of sentences - https:/‚Ä¶ RT @nalkalchbrenner: New paper update: minimal Bytenet and vanilla search with state-of-the-art results on char-to-char translation! https:‚Ä¶ RT @soumithchintala: This has been our TensorBoard for the last year-ish. I love visdom! https://t.co/yebGeaNjGq RT @alexjc: Self-supervised tasks help pretrain CNNs with great results. https://t.co/URIzb8JfOb Colorization as Proxy Task for Visual Unde‚Ä¶ RT @googleresearch: Announcing an upgraded SyntaxNet, a parsing competition and...ParseySaurus! Learn how to bake up your own models at htt‚Ä¶ RT @thomaskipf: Sparse Tensor support is coming to PyTorch https://t.co/H8oeiW1DYm RT @jekbradbury: A good list, and my colleagues are #1 :) | ‚ÄúTen Deserving Deep Learning Papers that were Rejected at ICLR 2017‚Äù https://t.‚Ä¶ RT @karpathy: RL works so poorly that finite differences are only ~10x worse. &amp; much simpler/more scalable. New paper from OpenAI: https://‚Ä¶ RT @cvondrick: CVPR workshop on negative results! https://t.co/hrYo7D6Nni RT @marklit82: Pretty cool German/English translation model demo w/ TensorFlow. Training takes ~3 days w/ 8 Nvidia TitanX's though https://‚Ä¶ RT @fchollet: Introducing Keras 2: https://t.co/83GsSJgGfE RT @hardmaru: Evolution can solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of tr‚Ä¶ @jeremyphoward @hardmaru better wait for 1080 Ti if you train models that are limited by the size of GPU RAM. Is anyone aware of a Variance Reduced version of Adam (Vradam)? Like SVRG with the Adam debiased preconditioner &amp; momentum. RT @Smerity: Interested in machine translation? @dennybritz, Goldie, @lmthang, Le give the best hyperparams from 250k GPU hours.
https://t.‚Ä¶ @statalgo @profjsb even for those with a lot of resources. Energy is not free, especially at scale and latency is never userfriendly. RT @BigsnarfDude: Fast and simple car pedestrian detector - SqueezeDet https://t.co/AF5qxcGVxF https://t.co/hPcr2vuLeq @profjsb and per GFLOP RT @profjsb: Really like the practical emphasis here, especially on model compactness metric: "accuracy per parameter" https://t.co/mQ7PjE1‚Ä¶ RT @jnuneziglesias: SciPy's new LowLevelCallable is a game-changer. 

https://t.co/sDsvByCOuV https://t.co/0nroTPHlrM RT @PyTorch: YOLOv2 is @pjreddie's real time object detector. Checkout the PyTorch port: https://t.co/vjsxX0aUTX
Project page: https://t.co‚Ä¶ @TheEcolss @mpd37 @WIRED I think in that case they are in a small to medium data setting with large uncertainty to model RT @rsalakhu: New preprint on Controllable Text Generation by Zhiting Hu et al.
https://t.co/oWpGcyNdyD
Generate sentences with desired att‚Ä¶ RT @_onionesque: Interesting Paper: Shattered Gradients: If resnets are the answer, what is the question? https://t.co/RiauhLu8Y4 (by Baldu‚Ä¶ @jeremyphoward @ConorBMurphy to me batchnorm is more about making training (optimization) easier rather than preventing overfitting. RT @mpd37: Looks like Gaussian processes are useful:
Machine Learning Invades the Real World on Internet Balloons https://t.co/mlu4vIuEmz v‚Ä¶ RT @jeremyphoward: Picking an optimizer for Style Transfer https://t.co/Kb2ef6fBka
Surprising results comparing BFGS with Adam, from new re‚Ä¶ RT @glouppe: Congratulations to @JolyArnaud for his PhD! https://t.co/36mleh34W4 RT @udo_zillmann: Producing Energy with....Drones?!

https://t.co/Hw5xBgnsKG https://t.co/DfiKOHO0At RT @dustinvtran: Edward, now with Jupyter notebooks https://t.co/hf4t7dzz6Z RT @yaroslav_ganin: Alright folks, here is a #deepwarp web demo: https://t.co/WgBApTNPtC  (a bit unpolished). Also, eye-rolling @goodfellow‚Ä¶ @ConorBMurphy @jeremyphoward not what it means. Conv layer vs fully connected? @ConorBMurphy @jeremyphoward I cannot read #3. What is it? RT @davenielsen: What ML &amp; AI really is, is a change in the way you program‚Äù ‚Äì Eric Schmidt @ericschmidt #googlenext17 RT @vambenepe: Google acquires Kaggle.
https://t.co/j0yCHRzR3n
I'm looking forward to working with the team! RT @StrongDuality: "..training took about 80 days for 1.5 billion
samples, on 2 Nvidia K80 GPU‚Äôs (4 devices) with batch
size 64 per GPU.."‚Ä¶ RT @Smerity: While @fchollet was describing DeepMath's approach, this succinctly summarizes much of #DeepLearning. #AIByTheBay
https://t.co‚Ä¶ RT @fonnesbeck: Using scikit-learn, GPflow and PyMC3 to build Gaussian process models. 
https://t.co/MXgXchDPHz RT @amuellerml: "Following academic best practices, key imple-mentation details can be found in our private code repository"  nice one :) h‚Ä¶ RT @eddieyoon: Google is acquiring data science community¬†Kaggle https://t.co/GHOzbZadIf via @techcrunch RT @fulhack: More exciting progress on approximate nearest neighbors ‚Äì this time from Flickr. Code here: https://t.co/VNooFQnlSD https://t.‚Ä¶ RT @trevor_cox: Google has just published a huge database of everyday sounds for machine learning https://t.co/JPb76dTAbJ RT @yaringal: Bayesian neural networks uncertainty can be used to distinguish
adversarial from non-adversarial images! new results https://‚Ä¶ Interesting trick to rebalance gender in computer science degrees: https://t.co/2RDIwVvXFJ @seaandsailor maybe spam filters are just better tuned to detect and block north america spammers. @syhw @soumithchintala https://t.co/sgbCEu5O2N is down... can you please let him know if he is not already aware of it? @DRMacIver I see not much worth in the positive subjective judgement by an author on its own work. @DRMacIver it might well be true to many but it's still arrogant. Better quote positive feedback from your users. RT @hardmaru: Neural Machine Translation and Sequence-to-Sequence Models: A Tutorial, by @gneubig and @neubig. https://t.co/Plbohj4uCw RT @karpathy: Squeezed in some time over the weekend to implement discussions for arxiv-sanity (Markdown/LaTeX, tags etc.) w00t!: https://t‚Ä¶ RT @benhamner: MoleculeNet: A Benchmark for Molecular Machine Learning https://t.co/tMlktEiHHe https://t.co/R3oCbKWMFC @zooba to be fair it's slightly knowledge informed random mutations on the structure of the network. But I agree it's a waste of energy. RT @yoavgo: this. I had a feeling this is not worth reading just yet. seems like I was right. (thread) https://t.co/z5FsO8kChN RT @pycoders: A pytest plugin to trace resource leaks. ‚Äì https://t.co/y7R0oHDMES @gchrupala @CharlesOllion it's hard to make any generic statement about performance of evolutionary computation (as far as I know). 2/ @gchrupala @CharlesOllion I agree but it really depends on the fitness func, the repr of the individuals and the cross over ops. 1/ RT @pasku1: If you use `ctrl+r` to look up commands through your history do yourself a favour and install fzf

https://t.co/x8K7EhoQeA @gchrupala @CharlesOllion evo with a single individual in pop and decreasing mutation rate is called simulated annealing and can be useful @gchrupala in think @CharlesOllion is already speaking in the context of in silico evolutionary computation RT @alexjc: It's hard to express just how angry this paper is making me... Should I provide an alternate view? ;-) https://t.co/vRGllKmCxk RT @poolio: Evolution is catching up to intelligent design for neural net architectures (94.6% vs. 96.7% on CIFAR-10): https://t.co/ZCSjuBO‚Ä¶ RT @harari_yuval: a16z Podcast: Brains, Bodies, Minds ... and Techno-Religions by a16z via #soundcloud https://t.co/jLIjzHqMVP RT @amuellerml: Within the last year the number of jupyter notebooks on github using sklearn went from 40.000 to 120.000 :D @ProjectJupyter @yoavgo @F_Vaggi vs code is fast, full-featured, open source and cross platform with a very healthy extension ecosystem and good defaults. RT @ramez: China will replace all 67,000 taxis in Beijing with electric vehicles. https://t.co/9fukGunl69 Congrats @thefreemanlab ! https://t.co/wjB3EV1KZ7 RT @kastnerkyle: A nice basic example of LSGAN on MNIST https://t.co/ldalr2k6vK , relevant paper https://t.co/HQWq4qZrtK @kdeepak011 I am not familiar with the state of the art and datasets for chatbot development. I am really not interested in chatbots. RT @indicoData: MXNet = lightweight, flexible distributed #DeepLearning framewrk w/ mobile deployment capabilities. Intro tutorial: https:/‚Ä¶ RT @GaelVaroquaux: New joblib release,
with LRU cache replacement policy for the on-disk memoize &amp; faster parallel computing
https://t.co/x‚Ä¶ RT @soumithchintala: FAIR releases faiss. Many uses: text2image by searching through 1B or 100B images? RL Agent with VERY LARGE memory?
ht‚Ä¶ @annakoop @deanpomerleau @chasnote but even without human level comprehension it might be tricky to design not too annoying text captchas RT @hardmaru: Differentiable Optimization as a Layer in Neural Nets. Differentiable Quadratic Program Layer learns to play Sudoku! https://‚Ä¶ RT @chrisemoody: Tried #pytorch for the first time today. Wrote a t-SNE implementation; worked beautifully &amp; fast! https://t.co/q6NcsV1uqa‚Ä¶ @annakoop @deanpomerleau @chasnote not human level yet I agree, but research made significant progress to narrow the gap. @GiorgioPatrini @yoavgo @haldaume3 argmin_{h in H and train_loss(h) = 0} Lipschitz(h) @GiorgioPatrini @yoavgo @haldaume3 indeed. I would say we always choose H such that min_{h in H} train loss is zero, but then choose 1/ RT @chasnote: Norwegian news site requires readers to answer questions proving they read a story before commenting on it https://t.co/piW22‚Ä¶ @annakoop @deanpomerleau @chasnote not so sure any more. @yoavgo @haldaume3 I think some people promote RL to train dialogue systems aka conversational agents. RT @xtimv: @haldaume3 @yoavgo https://t.co/VxzKEO1LB5 is a really great paper @kchonyc -- clever decoder + smart RL alg (deterministic poli‚Ä¶ @OriolVinyalsML @yoavgo @haldaume3 good point. Its as if sgd is adaptatively favoring smooth solutions when they exist but sharp otherwise. @yoavgo @haldaume3 see also: On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima https://t.co/VcXPnJZJv4 RT @Smerity: QRNNs (@jekbradbury, @Smerity, @CaimingXiong, @RichardSocher) used in @BaiduResearch's production ready Deep Voice!
https://t.‚Ä¶ @yoavgo @haldaume3 also maybe related ideas: https://t.co/V6jcVXPP3m @yoavgo @haldaume3 yes exactly. We benefit from a very strong and helpful computational reachability prior. @yoavgo @haldaume3 therefore VC theory holds, it's just that the H' implicitly defined by SGD is hard to formally specify. @yoavgo @haldaume3  cannot be reached by SGD but by other solvers (not necessarily gradient-based) and with bad generalization /2 @yoavgo @haldaume3 but actually even if train loss / error is zero with SGD, there might exists many other h with same train loss that /1 @yoavgo @haldaume3 alternative second link: https://t.co/m1UMqbnlcv @yoavgo @haldaume3 good question, it's never reported. @yoavgo @haldaume3 unless final training loss is zero (ignoring l2 penalty and such). @yoavgo @haldaume3 but because loss is not convex there is off-course no guarantee to get the global minimum on the empirical risk anyway 4/ @yoavgo @haldaume3 larger batches at the end to reduce gradient variance at the end. Or a couple of LBFGS iterations when SGD stalls. 3/ @yoavgo @haldaume3 till real convergence on the empirical risk. That would require strong optimizer: learning rate annealed to zero, /2 @yoavgo @haldaume3 I don't think so. It would be interesting to do a systematic review of the true risk of the selected h when optimizing 1/ @haldaume3 @yoavgo https://t.co/QfAYAcD6WH @haldaume3 @yoavgo https://t.co/cTQvTPB3hj @haldaume3 @yoavgo therefore dl practioners never select argmin_{h in H} empirical risk. But us H' subset of H implicitly defined by val set @haldaume3 @yoavgo in my opinion the answer to generalization is sgd + early stopping on validation: we never let sgd go to zero train loss RT @weballergy: 'Billion-scale similarity search with GPUs': kNN search that is 8.5x faster than prior state-of-the-art. (by FAIR) https://‚Ä¶ RT @jakevdp: The ipywidgets 6.0 release includes tools for embedding Jupyter widgets into static websites &amp; nbviewer. This is huge! https:/‚Ä¶ RT @fhuszar: Another great paper on adversarial variational inference has just hit arXiv. And it's making its way to Edward (probabilistic‚Ä¶ New NVIDIA GeForce 1080 Ti: 11TFLOPs, 11Gbps / 11GB GDDR5X for $700 and 250W: https://t.co/nJN8Y5kPKf ELI5: debug and explain the behavior of Python Machine Learning models: https://t.co/Lsq4VkM9Xb (sklearn, xgboost, lightning, crfsuite....) RT @fchollet: Your next deep learning paper: @brettsky done: https://t.co/m9w7tlmCzs @DrLukeOR @ankurhandos the WGAN paper is way more interesting from a theoretical / pedagogical point of view too. @brettsky @zooba nice! but the -o table output is missing info from the matching azure CLI 1.0 commands and json output is not user frienly @yoavgo @nurikolan @soumithchintala I asked myself the same question, wanted to tweet it and then reread the tweet/descr. before doing so :) @yoavgo @soumithchintala wikipedia RT @soumithchintala: Pre-trained Word Embeddings for 90 languages trained using FastText, on Wikipedia. Even has my native Telugu!
https://‚Ä¶ @ankesh_anand @ankurhandos still I think we all agree that LSGAN bedrooms are the best bedrooms generated by any GAN variant (qualitatively) @ankesh_anand @ankurhandos also LSGAN training might be even more stable with several discriminator updates per genarator update. @ankesh_anand @ankurhandos that does not guarantee that statistical convergence is faster though. Loss is different so it's hard to tell. @fhuszar it's not true: those are not part of the official deep learning bibtex: https://t.co/cKUQWCdZrB DS3 Data Science Summer School on Deep Learning, Graphical Models, Bandits, Randomized Optimization:‚Ä¶ https://t.co/qaADgIosIL RT @fhuszar: Variational Inference using Implicit Distributions
https://t.co/febUIyQDrs
highlights connections between VAEs, ALI, BiGANs, O‚Ä¶ @Quesada old news (june 2016) @fhuszar 3 million samples from the same class of scenes. @ankurhandos no indeed, just a note in passing. I over interpreted it. The stability comparison is with dcgan. @heikhuttunen if you zoom and look carefully you see inconsistencies in room organization, perspective or objects melding into one another. Least Squares Generative Adversarial Networks: https://t.co/NjjMVSQdkk as stable but faster to train than WGAN and‚Ä¶ https://t.co/sX0R1HMGz8 RT @weballergy: 'Variational Inference using Implicit Distributions': prior-contrastive and joint-contrastive methods. https://t.co/IBXwqRu‚Ä¶ RT @WiMLworkshop: Our list of 830+ women working in #machinelearning is a resource for finding speakers, panelists, etc: https://t.co/yWAje‚Ä¶ RT @fchollet: If anyone wants to help with the release of Keras 2, you can do so by: (cont.) RT @fastml: Tuning hyperparams fast with Hyperband:
https://t.co/PdPlp1F9FP

Code:
https://t.co/XG2tKJU0EN RT @mrocklin: Fun hack: Launch Dask from a Spark cluster or Spark from a Dask cluster

https://t.co/DdVP6UmRX9 RT @jtaylor108: numpy now automatically avoids the temporary in a + b + c via an inplace operation https://t.co/GvXNCLj20w (numexpr is stil‚Ä¶ RT @seanjtaylor: Prophet is an open-source forecasting package implemented in both Python and R, used in production at Facebook: https://t.‚Ä¶ RT @hllo_wrld: Switched to a dynamic #deeplearning framework? @ChainerOfficial or @pytorch? Checkout https://t.co/a2IRhIfbym for experiment‚Ä¶ RT @karpathy: Shake-Shake regularization code https://t.co/5ksl6aQ2rq claims 2.72% on CIFAR-10. Fun - add more stochastic, even "break" bac‚Ä¶ RT @deliprao: Pixel Libert√©, Pixel √âgalit√©, Pixel Fraternit√© https://t.co/sNGYVQYnes #deepbait https://t.co/bhssE8QmWH RT @fhuszar: @fhuszar imagine ditching LaTeX for markdown with LaTeX markup only for maths... But I think we will colonize TRAPPIST-1 befor‚Ä¶ RT @fhuszar: I estimate that changing #icml paper format to single column would in effect result in donating ~1500 man-hours back to the ML‚Ä¶ RT @UNFCCC: CO2 keeps piling up in the atmosphere, including big bump in 2016. Scaled up #ClimateAction is required. Via @blkahn https://t.‚Ä¶ @yoavgo I meant I have not experimented with model based parallelism. @yoavgo see the video of the talk of the original seq2seq paper for an animated diagram of the MP scheme with deep LSTMs. IIRC. @yoavgo I don't see why it would but I have not tried myself, I have a single gpu on my box ;) @yoavgo out of curiosity do you use word level or char level input / outputs? Do you use sampled softmax with bucketing? @yoavgo how many layers for the encoder and decoder? If many layers you can use model based parallelism to not suffer from stalled gradients @vorpalsmith @Mbussonn %pip install RT @J_: @ApacheArrow 0.2 is out. Thank you to the contributors!
https://t.co/KSFzJWfI4E RT @WikiResearch: Automated 1-sentence "biographies" from Wikidata, preferred by readers over Wikipedia's 1st sentence in 40% of cases http‚Ä¶ RT @karpathy: When you run a big hyperparameter search and discover that your default (guessed at) hyperparams work best. Not sure if :) or‚Ä¶ RT @peteskomoroch: My cat drawing transformed: https://t.co/cs7HVG6aBx RT @Reza_Zadeh: GPUs are now available for Google Compute Engine and Cloud Machine Learning https://t.co/E4d1Po1aH4 @ogrisel and here is a shorter yet intuitive intro to the matter (with application to variational inference): https://t.co/z2vMTJjb5K @yoavgo Probably a way to recycle them. As long as AWS and Azure offer K80 instead of Pascal-based cards, they are competitive on cloud. @yoavgo probably much cheaper and more efficient to just buy a Pascal Titan X. @ogrisel here is the table of contents: https://t.co/NTxWAPEScp Excellent overview paper with rigorous yet easy to follow formalism. Fisher, Hessian, Generalized Gauss-Newton matr‚Ä¶ https://t.co/FNDejb2gQf New insights and perspectives on the natural gradient method by J. Martens, 2014, updated in 2016: https://t.co/YBD2sRvkSw RT @mrocklin: Experiment with #Dask and #XGBoost

Notebook: https://t.co/14KcDV5Ehq
Screencast: https://t.co/j6RmLFQN1T
GH Issue: https://t‚Ä¶ RT @thomaskipf: Applications for the CIFAR Deep Learning Summer School are open. Yet again a great line-up of speakers; can absolutely reco‚Ä¶ RT @Daeinar: Scientists estimate to be able to break RSA-1024 in 2 weeks (RSA-2048 in 4 months) with an energy consumption of about 5 megaw‚Ä¶ RT @NicolasPapernot: New #cleverhans feature: you can now visualize adversarial examples. See tutorial here: https://t.co/IT0PT0I0Gk RT @GiorgioPatrini: This year at NIPS, a new variant of the format: competitions https://t.co/Bfs7P18ppP RT @mrocklin: @mrocklin Recent work with Dask and XGBoost use the same approach and is probably more clear: https://t.co/1IlNVEZlCC RT @mrocklin: New blogpost: Brief experiment using #Dask and TensorFlow together for distributed pre-processing and training: https://t.co/‚Ä¶ RT @aronchick: Including windows and arm nodes! https://t.co/wD0vUY4hMX RT @thephysicsHB: Stream 'Calypso' now via @Spotify - https://t.co/JGXC5SJfLU https://t.co/8zlQ3ly8Su @thephysicsHB will you come back to Paris after the uk tour? RT @tensorflow: Now you can debug your @TensorFlow graphs with #TensorFlow Debugger (tfdbg)! https://t.co/Hv1UOqYjAD https://t.co/N6KvyOst0a @mrocklin done :) @mrocklin sorry I replied to the wrong tweet... @mrocklin maybe benchmark dask+xgboost (w/ binning + fast hist) on Higgs Boson and Criteo dataset and compare with: https://t.co/CBrO9IP1fW RT @mrocklin: Last two weeks of #Dask work: machine learning experiments, XGBoost, graph optimizations, tutorial refactor, GCS
https://t.co‚Ä¶ RT @clmt: Neat https://t.co/DuMVqj3NNb RT @ylecun: The videos of the NIPS2016 Workshop on Adversarial Training are up.

My talk is here:... https://t.co/lTpTHGhgBU @quaidmorris @Reza_Zadeh @TastanOznur as I said in another reply, the negative log likelihood loss derives from Shannon's entropy equation @quaidmorris @Reza_Zadeh @TastanOznur I agree, the Wassertein GAN loss function might well be the negative log likelihood killer ;) @Reza_Zadeh the negative log likelihood loss is equivalent to the cross entropy of model under data which is a variant of eqn #15 RT @rasbt: :empirical methods to argue that deep nets do not achieve their performance by memorizing training data ..." https://t.co/WKgrE1‚Ä¶ RT @vorpalsmith: This is huge ‚Äì for all of numpy's history, a[1:] += a[:-1] has silently returned some arbitrary junk. Now it just works! P‚Ä¶ RT @Miles_Brundage: @robinhanson context: https://t.co/W7DUyuLnvC RT @Miles_Brundage: Has anyone said why a robot tax would differ from/interact with a tax on capital or business income? Or why it'd be pre‚Ä¶ RT @Miles_Brundage: arXiv: I bet this volume of deep learning papers will overwhelm you.
OpenReview: Hold my beer.

https://t.co/UkcoQo69fY RT @cangermueller: Checkout my new code-base of #DeepCpG, including the DeepCpG a Model Zoo: https://t.co/VzJa3J2L5f, https://t.co/6t4np2pF‚Ä¶ RT @josephreisinger: In the near future, the only currency of value will be cynicism, which our GAN overlords will hoard and consume for it‚Ä¶ RT @josephreisinger: A contrarian position on AI-mediated communication design: I will never, ever, say any of these things to you ever aga‚Ä¶ RT @catherineols: @catherineols and if you work in ML/AI/data, THINK - are you SURE you want to maximize how long a user spends on your sit‚Ä¶ RT @syhw: Building Global Community https://t.co/2kOEQ4Yoa8 RT @wesmckinn: "The real work is in the details" Not always a fan of Torvalds but he's right https://t.co/GBmpouqH5B RT @erichorvitz: Just released from @MSFTResearch: Open-source AIR platform to train/test AI systems to develop safe drones &amp; robots https:‚Ä¶ RT @googleresearch: We've updated YouTube-8M, and announced a video understanding challenge with an affilliated CVPR workshop. Busy day! ht‚Ä¶ RT @hannawallach: Whoa. Fascinating thread for anyone interested in proactive government transparency and open data. https://t.co/uzAmXl2GA9 RT @KyleCranmer: Awesome example of the Kelvin-Helmholtz instability outside the physics department @nyuniversity @NYUScience https://t.co/‚Ä¶ RT @poolio: TensorFlow 1.0 announced at #TFdevsummit. Woohoo! https://t.co/Ybry5RLMzd RT @poolio: @poolio includes core support for Keras, decision trees, SVMs RT @goodfellow_ian: Drug Discovery Magazine article about GANs / adversarial autoencoders for identifying cancer drug candidates: https://t‚Ä¶ RT @ncoghlan_dev: The new "secrets" module (https://t.co/hfdsvJVCBf) also had a part in this story, since it provided a high level blocking‚Ä¶ @heyaudy if you teach datascience, it is much easier to give one set of "getting started" instructions that works almost for everybody. @heyaudy you cannot pip install scipy under windows (no upstream scipy wheel because of the lack of free fortran compiler on that platforn) @balazskegl @KyleCranmer same reasons why random forests often beat penalized linear models RT @brandondamos: Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models

https://t.co/4Qr0BeRHTt https://‚Ä¶ RT @xamat: Post on @Quora by @nikhilbd: Semantic Question Matching with Deep Learning https://t.co/huNjjphNQr RT @jedisct1: Specs for first Intel 3D XPoint SSD: so-so transfer speed, awesome random I/O https://t.co/O6uD9e4lmw RT @goodfellow_ian: Improved SMT solver approach for showing there are no adversarial examples within specific regions: https://t.co/yMmXJO‚Ä¶ RT @hardmaru: ‚ÄúOne reason I don‚Äôt like the discussions about superintelligence, is that they are a distraction from what is real.‚Äù https://‚Ä¶ @cjordansquire @goodfellow_ian designing model architectures such that the gradient and the natural gradient better align is a challenge RT @gvanrossum: The CPython source code has officially moved to https://t.co/0ax0UGzgLZ. Congrats @brettsky !!! @hugo_larochelle @deliprao it's still down for me as well, and not just me: https://t.co/uzR6fwVpSh RT @fhuszar: Variational Inference using Implicit Models Part III:
A variational derivation of ALI and BiGAN from 1st principles:
https://t‚Ä¶ RT @weballergy: On post-processing embeddings for achieving improved performance. #machinelearning #NLP #deeplearning https://t.co/Ha7myUV9‚Ä¶ RT @AndrewYNg: Kubernetes makes distributed computation easy. I'm excited about Kubernetes+Deep Learning! @coreos https://t.co/VRizUKtySG RT @hardmaru: Skip Connections as Effective Symmetry-Breaking. Interesting interpretation of resnet family. https://t.co/oLiP5IQirx https:/‚Ä¶ RT @asmeurer: Favorite feature of nan: https://t.co/k68LpTsHOZ RT @ChainerOfficial: Multi-node distributed training is coming to Chainer - 100x faster w/ 128 GPUs on ImageNet &amp; comparison w/ others http‚Ä¶ RT @indicoData: You can now download NIPS 2016 videos! https://t.co/P01lOjZgz7 RT @alexjc: Face Aging With Conditional Generative Adversarial Networks https://t.co/FAlsfJD1tw #DeepLearning https://t.co/q9KH18RN9y RT @karpathy: arxiv-sanity is now migrated &amp; has new feature: sort by hype :p - shows papers that got most tweets over last 5 days https://‚Ä¶ RT @Rob_Bishop: ‚ÄúZoom &amp; Enhance‚Äù using the SRResNet algorithm that Magic Pony / @Twitter published in https://t.co/fMuNX2XbvM ‚ú® https://t.c‚Ä¶ RT @googleresearch: #TensorFlow Fold brings batching to #deeplearning models that operate over data of varying size and structure https://t‚Ä¶ RT @Ozan__Caglayan: We've open-sourced our NMT framework #nmtpy which is a refactored and polished API based on dl4mt-tutorial.
https://t.c‚Ä¶ RT @mrocklin: New blogpost: Simple ways to use @scikit_learn with #Dask: https://t.co/rHvmzi6j60 RT @fastml_extra: Oxford Deep NLP 2017 course (videos, slides):
https://t.co/rUdI8eyZkv RT @googleresearch: Introducing YouTube-BoundingBoxes, a dataset of bounding boxes that track objects in temporally contiguous frames https‚Ä¶ RT @thomaskipf: ICLR conference decisions are out: https://t.co/3qFZcCtrel - acceptance rate at 39% this year RT @brandondamos: block: My [short] new Python library for intelligent block matrices in numpy, @PyTorch, and beyond.

https://t.co/VPmMK6O‚Ä¶ RT @OriolVinyalsML: @iclr2017 decisions are out! 15 orals, and 181 posters out of 507 submissions. I attach some stats for those curious. h‚Ä¶ @abursuc I am more concerned by the people activity reported by youtube videos in that context. I am not sure it s representative of reality @abursuc I wonder what kind of "common sense" knowledge DL systems will build up from the "toilet" label. RT @abursuc: YouTube-BB dataset: 380k short videos with bboxes for 23 object classes https://t.co/DtUyRtw0ka Pity though that only video ur‚Ä¶ RT @mat_kelcey: "so, what do you think is hardest to train? seq2seq? deep RL? or GANs?"

"hold my beer...." https://t.co/nluZulh9dr RT @amuellerml: There's still places available for the Women in Machine Learning scikit-learn sprint https://t.co/QpqZZRiOlE March 4th. Eve‚Ä¶ @karpathy @verge like the spider bots in ghost in the shell. RT @karpathy: Loss addiction: self-destructive behavior of obsessively watching &amp; reading into tiny fluctuations in loss functions of runni‚Ä¶ RT @Maciej_Kula: A neat blog post on using LightFM to match experts to projects @GoCatalant https://t.co/RWKnCo8iQv Slides and video of the NIPS tutorial on modern Variational Inference by David Blei, Rajesh Ranganath and‚Ä¶ https://t.co/FSAWONt0b4 RT @xuewei4d: ‚ÄúThe Emergence of Modular Deep Learning‚Äù by @IntuitMachine https://t.co/qNYrxtl92S RT @soumithchintala: Wasserstein GANs pretty aptly summarized in this reddit comment:
https://t.co/tOEvHG8l9f https://t.co/5DNyDE8hWC RT @tdhopper: @asmeurer https://t.co/Ah9P3egQHI Executive summary / highlights and comments for the Wasserstein GAN paper: https://t.co/SlwdUYPY6z Factorization Machines for Recommendation Systems with TensorFlow:
https://t.co/oq6X9I8zXM &amp; the tffm lib on github: https://t.co/i5TtwM2lzs RT @deliprao: After months of hard work, very satisfying to launch Stage 1 of  #FakeNewsChallenge. For dataset &amp; details, see https://t.co/‚Ä¶ RT @samim: The Boston Dynamics Robot that can jump and has wheels: https://t.co/mWuQOMk7bm RT @hardmaru: Generating Sentences from a Continuous Space. This 1yr old paper contains insights and tricks for training VAE-RNNs. https://‚Ä¶ @amuellerml checkout lightfm RT @alexjc: Integrated the new WGAN into #NeuralEnhance and watching the last column "adversarial" loss drop constantly... it's beautiful.‚Ä¶ RT @YhatHQ: A journey to make #Python with HTTP screaming fast which resulted in a new web micro-framework | https://t.co/ndWVKXEKPz RT @ThePSF: Time To Upgrade Your Python: TLS v1.2 Will Soon Be Mandatory https://t.co/0cHQ8ENYQn RT @alexey_r: Dr. Memory: a memory checker like Valgrind's memcheck, but works on Windows as well https://t.co/EvqW38XW7X #cpp (+ https://t‚Ä¶ RT @AndrewYNg: CMU just made history: AI beats top humans at Texas Hold'em poker. A stunning accomplishment, comparable to Deep Blue &amp; Alph‚Ä¶ @gglanzani @amuellerml @GaelVaroquaux @agramfort thanks for the ping. I will think about submitting something. @YACHT @fchollet LGTM ;) you could alternatively maybe use some colors. RT @polynoamial: Final scores for #BrainsVsAI: @heyitscheet -880,087, @dongerkim -85,649, @Chouchoupoker -522,857, @678DMcA -277,657, Libby‚Ä¶ RT @polynoamial: The paper on the endgame solver used in Libratus is now up on my website: https://t.co/U8hIdFSlCx #BrainsVsAI @YACHT @fchollet maybe use gray levels to emphasize "ai" on one hand and "on" on the other hand. RT @fchollet: We've recently added a new research project on AI‚Ä¢ON: multi-task RL. https://t.co/88DRLliwg5 RT @kubernetesio: Read about Fission, a new framework for Serverless Functions as a Service on Kubernetes https://t.co/yYFjEfVCfo RT @PyTorch: Code for the paper, can do LSUN, Imagenet, Faces experiments: https://t.co/jKQKHMbd7H https://t.co/zWy2UvKyK6 RT @ajmooch: @soumithchintala Looks like WGAN handles the unrolledGAN toy experiment really well! Still tweaking hyperparams but it looks p‚Ä¶ RT @T0bias_Brandt: @ogrisel Thanks for the share. Looks great. I think it might be 94 Œº$ per GPU second though. @T0bias_Brandt you are right, I tweeted too quickly... @miishke I just took that as an opportunity to tweet "Œº$" ;) The cloud pricing granularity trend is interesting though. Floyd: 9.4 Œº$ per GPU second pricing for deep learning: https://t.co/LCIUYLAY8N RT @Mbussonn: On a slightly happier tone: I'm coping by releasing #IPython. #IPython 5.2.0 (bugfix) is out: https://t.co/flBrc6KcD7 Upgrade‚Ä¶ RT @soumithchintala: @soumithchintala pytorch code for implementing WGANs, more fully fleshed out github repo will be released soon: https:‚Ä¶ RT @soumithchintala: Wasserstein GANs: loss correlates with sample quality, fix mode dropping, improved stability, sound theory: https://t.‚Ä¶ @cangermueller I think this contribution is more about framing the right hiererchical class labelling and balancing using medical expertise RT @shakir_za: Video https://t.co/2bqHkgcxzc, slides https://t.co/uqbxjzj5lZ for #NIPS2016 'Variational Inference: Foundations and Modern M‚Ä¶ RT @amuellerml: Please help me plan (and justify) future scikit-learn development by answering a short survey: https://t.co/2hWvKl9WZt RT @tensorflow: TensorFlow 1.0.0-rc0 is available - with XLA, TF-DBG, new Android demos, Java API, Python 3 Docker images, and more: https:‚Ä¶ RT @AltNatParkSer: ‚ÄúThe saddest aspect of life right now is that science gathers knowledge faster than society gathers wisdom.‚Äù
 
‚Äï Isaac A‚Ä¶ RT @tarantulae: TensorFlow 1.0.0-RC0 was released, https://t.co/V1q9FuSJST, way to go ! RT @poolio: New notebook implementing Adversarial Variational Bayes in TensorFlow: https://t.co/TM70CyMNC1 https://t.co/zWeiB5eNw1 RT @VicenteFoxQue: @realDonaldTrump's ego monument real cost is around 25 billion USD. I ask you, America, what would you prefer instead of‚Ä¶ @mat_kelcey @hardmaru +1 for little dude just because of the happy jumping in the video. RT @amuellerml: OpenML, an awesome machine learning repository with datasets and benchmarks could use help with the python interface https:‚Ä¶ RT @art_sobolev: Awesome list of ML videos by @dustinvtran 
https://t.co/6VWxtogMFk RT @VictorStinner: Is it Python 3 yet? ‚è∞
https://t.co/M0653VA4Hx
I proposed to hide Python 2 by default from the https://t.co/aI2DpD6VtE do‚Ä¶ RT @Hydrostor: How to store electricity underwater: Depths of imagination https://t.co/P3qHxBcrtp via @TheEconomist @minrk @zooba and tls RT @fhuszar: Variational Inference using Implicit Models, Part IV:
Using Denoisers Instead of Discriminators + iPython notebookü§ì
https://t.‚Ä¶ @fhuszar just a remark, the correct casing is IPython. If you speak about the notebooks it's better to refer to Jupyter notebooks. RT @RogueNASA: PSA: If you're a government scientist or employee and your work is being censored, consider this: https://t.co/GReCdOJb96 RT @KentSommer: Ported InceptionV4 to Keras + Pre-Trained Weights:
https://t.co/EdbVboiuvf

#Keras Implicit feedback recsys with Keras using triplet loss &amp; embeddings:
 https://t.co/W5Y5j1Ettx by @Maciej_Kula 0.91 AUC on movielens 100k RT @mblondel_ml: TIL: You can use Vim moves in Firefox (press j for down and k for up) to navigate a page! RT @prolificd: @ogrisel this one allows for complimentary access: https://t.co/zXls4x3Oov Dermatologist-level classification of skin cancer with deep neural networks
https://t.co/M3SdIpRSUJ (paywalled) https://t.co/EQTYpYJY4t RT @NipsConference: Videos from the 2016 Tutorials and Conference are available at https://t.co/MQAXlggjbX. Workshop videos will be posted‚Ä¶ RT @wesmckinn: Development update: High speed @ApacheParquet in Python with @ApacheArrow https://t.co/xOyiowywXY #pydata RT @freakonometrics: "Estimating the Airspeed Velocity of an Unladen Swallow" https://t.co/Yts1ZS4zPV  ht @jdcsef, references = Nature, JEB‚Ä¶ RT @ericgeller: It's happening: The White House has ordered the EPA to delete its climate change page. https://t.co/uZ4TSegfYi https://t.co‚Ä¶ RT @fchollet: Colleagues' new paper on using deep networks for guiding an automated theorem prover is up on Arxiv: https://t.co/VAVv9fDFQm RT @jakevdp: Curious why matplotlib 2.0 changed color defaults? Watch this (amazing) 2015 talk by @stefanvdwalt &amp; @vorpalsmith: https://t.c‚Ä¶ RT @xamat: Happy to announce our first @Quora Dataset Release: Question Pairs https://t.co/Fee3wHHqVA RT @isotopp: From the "Your Data Is Not Big Data"-Dept: https://t.co/vJ5Scjf3hY https://t.co/bGxn12xGEQ https://t.co/WVaJjyOEG4 RT @rasbt: "Attention Transfer for Convolutional Neural Networks " implemented in @PyTorch : https://t.co/IAvKd4g3Ty RT @YhatHQ: Fully Convolutional Networks (FCNs) for Image Segmentation | https://t.co/wBni6np39a https://t.co/aGhuSbaSfe @tarek_ziade for instance: https://t.co/xoaJUscDeI RT @turbodbc: The future has arrived: I just released #turbodbc 1.0.0 with tested support for #Python 2.7, 3.4, 3.5, and 3.6. https://t.co/‚Ä¶ RT @Smerity: For every time we see a neural network architecture with a million widgets and no ablations:
https://t.co/bn88yVtUT9 RT @FlorianWilhelm: Finally, @turbodbc has Python3 support! Thanks for resolving my issue https://t.co/JF07O2Vcgu #Python #ODBC #Exasol RT @Mbussonn: #IPython 5.x is so last year. We'll have gifts for you in 2017. Static type inference and better completion with thanks to (a‚Ä¶ @jakevdp maybe facebook is actively trying to break the filter bubble? RT @PyTorch: "Paying More Attention to Attention" improves ConvNets with Attention.
#lclr2017 submission powered by yours truly
https://t.c‚Ä¶ RT @poolio: Neat results mapping face images into a canonical space. Face Synthesis from Facial Identity Features:
https://t.co/GAIq8HqYcq‚Ä¶ RT @fhuszar: Variational Inference using Implicit Models Part II: Amortised Inference. Complete with an iPython notebook
https://t.co/hmYeg‚Ä¶ RT @dustinvtran: ‚ÄúTowards Principled Methods for Training Generative Adversarial Networks‚Äù by Martin Arjovsky and Leon Bottou https://t.co/‚Ä¶ @seaandsailor @PyTorch looking for an object detection (faster rcnn, rfcn) impl, any suggestion? @zooba I really like vscode for debugging C++ &amp; python. Do you think mixed mode debugging from PTVS could be ported as a vscode ext? RT @ianozsvald: W00t @pydatalondon CfP (for May 5-7) now on, go here https://t.co/k8b1a7VnHq to make your submission. Inspiration: https://‚Ä¶ @yoavgo @F_Vaggi @soumithchintala @apaszke the difference probably lies in careful memory management then (to avoid unnecessary mallocs) @vorpalsmith @VictorStinner no, at least not so far. Maybe that has changed / will change in the future. @yoavgo @F_Vaggi @soumithchintala @apaszke the tensor ops (written in C, Blas &amp; Cuda) are expected to dominate the py function call overhead RT @delphinel: UN urges global move to meat and dairy-free diet https://t.co/pmRsVCQyjV #vegetarian #environment #sustainability @ronanlamy @vorpalsmith @zooba I thought those were 32 bit only VMs,  has this changed? @VictorStinner @vorpalsmith +1 for local VM. You can get a official Win10 ISO for free at: https://t.co/XPpvdCM7zM RT @mrocklin: New post: Distributed NumPy on a cluster with image analysis.  #Dask #PyData

https://t.co/0HxbvjJgaW https://t.co/TYAvoj6IeP @mblondel_ml @cournape no need for tf.While and tf.Print as symbolic operators. No async forward exec makes debug like regular numpy debug. @mblondel_ml @cournape torch.autograd traces provenance while executing the forward pass, you can use python while and if test on tensor val RT @googleresearch: We are releasing word-sense annotations on the popular MASC and SemCor datasets. Get the data and learn more at https:/‚Ä¶ RT @brandondamos: Dropping into C from PyTorch is another great feature that they did right: https://t.co/5uF1esc2Ri RT @brandondamos: Why PyTorch's layer creation is powerful: Here's my layer that solves an optimization problem with a primal-dual interior‚Ä¶ RT @PyTorch: GPU Tensors, Dynamic Neural Networks and deep Python integration. Hello world!
https://t.co/b35UOLhdfo https://t.co/MnuNVqJVZK RT @soumithchintala: Been working on this for the last few months. Hope you enjoy the early release :) https://t.co/yEFegbNf0q @soumithchintala waiting for your tweet @glouppe GAN tweeting 101: always include a screenshot of the Celeb A interpolation figure: https://t.co/mNBUeQdnRL RT @glouppe: Adversarial Variational Bayes https://t.co/Ng0wNRDw9K Nice theoretically motivated connection between GANs and VAE @dovgalec @karpathy extensive ablation studies can help assess the impact of hyperparmeters and modeling choices and their impl. though. @dovgalec @karpathy then it's reproducible but I would not blindly trust the "narration on why it works better" because of impl details RT @jakevdp: Wow, didn't realize this before: this week's NumPy 1.12 release includes nearly complete support for PyPy! https://t.co/BL6Yyk‚Ä¶ RT @tqchenml: Doing flexible deep learning in imperative #numpy way, shares similar spirit with @TorchML https://t.co/2dPeiAjYw2 RT @karpathy: "Personally, I do not trust paper results at all. I tend to read papers for inspiration" A correct rant. https://t.co/mQOY7hj‚Ä¶ RT @stanfordnlp: Dynamic computation graphs like DyNet &amp; Chainer very useful for #NLProc‚Äîspeed looks good‚Äîcan either make it as mainstream‚Ä¶ Stochastic online matrix factorization in cython with sklearn compatible API by @arthurmensch: https://t.co/CdFD67wo5e dict learning &amp; nmf @ufechner7 adding freekitesim as an env for https://t.co/uLJjN0vx6Y would make it easy for Reinforcement Learning students to optimize AWE RT @tacaswell: Happy to announce matplotlib 2.0 is released!

https://t.co/6y8mdL2M4X

https://t.co/Zsl5zq6F6R

#matplotlib RT @AllenDowney: My most popular post of 2016: "There is Still Only One test" https://t.co/Bs3MJU8nRa

Coming soon: my least popular post o‚Ä¶ RT @poolio: We just released an example notebook for unrolled GANs on github! Very easy to implement using TF's graph_replace: https://t.co‚Ä¶ RT @amuellerml: We just published a new "print" of the Introduction to Machine Learning with Python e-book https://t.co/g8YEa9fZzp! Update‚Ä¶ RT @dustinvtran: ‚ÄúDeep Probabilistic Programming‚Äù now on arXiv. Foundations of https://t.co/rVnzzkDW8t with deep learning apps https://t.co‚Ä¶ RT @NandoDF: Cool new development, and in Tensorflow https://t.co/6O7zNCM1UZ RT @jeremyphoward: Big deep learning news: Google Tensorflow chooses Keras https://t.co/LVzz9AITp7
I'm really happy to see this! RT @jtaylor108: numpy 1.12.0 released https://t.co/qt9rEqkLAZ @rasbt @rfh100 0.12.1 is already on pypi RT @EricHolthaus: There is, right now (as of Jan 12th), the least area of sea ice on our planet that we've ever measured‚Äîprobably the lowes‚Ä¶ RT @mat_kelcey: comparing the tensorboard embedding viz https://t.co/2ekYfTQ5Ye to how i used to visualise embeddings in distbelief 3 yrs a‚Ä¶ @heikhuttunen @goodfellow_ian with coal based electricity? RT @fhuszar: Variational Inerence with Implicit Probabilistic Models (part 1 of what I hope will be a series, + iPython notebook)
https://t‚Ä¶ RT @profjsb: Faster shuffles and Parquet support. 

This out of core, multinode dataframe thing is looking more and more functional in #Pyt‚Ä¶ RT @OpenAI: GTA V + Universe: https://t.co/2bpZ6QNv4R RT @amuellerml: Dear @Google. Glad you do multilingual keyboards in Android now. But please! guess the language based on chat history. Righ‚Ä¶ RT @glouppe: The boosting strikes back! "AdaGAN: boosting generative models" https://t.co/R2qrtfl8Jw RT @karpathy: TensorFlow 1.0.0-alpha https://t.co/crOlSAId9C many numpy API compatibility changes are very welcome, seems could still go ev‚Ä¶ @betatim I don't think so as wheels do not embark python itself. Portable python + wheels / venv might work though. RT @jakevdp: As a Python user, one thing I really envy from the #Rstats world is R Markdown &amp; bookdown. They're both really incredible publ‚Ä¶ RT @random_forests: If you want to caption images with Show and Tell https://t.co/w4SR516rMF this pre-trained model makes it easy to try ht‚Ä¶ RT @johnrobb: Air dropped micro-UAV swarm maneuvering and encircling target. Sound of swarm is eerie cool (2:20)

https://t.co/J4nTYnVYbC RT @twiecki: Today, after 5 years of continuous development, we released #PyMC3 3.0 final! Get it via pip or conda-forge. https://t.co/RDoW‚Ä¶ RT @petewarden: Video showing Qualcomm's HVX DSP giving an 8x speedup over the CPU for @TensorFlow, and using 25x less power: https://t.co/‚Ä¶ RT @minrk: Released nbdime 0.1.1 with some good fixes and improvements. Thanks for the early testing and feedback! https://t.co/0BTmS4hXo8 @dribnet @liorshkiller @dumoulinv related: dont you think the model is under fitting on the age axis? Samples all look in their thirties. @dribnet @liorshkiller @dumoulinv I don't know wh. you plan to do w/ this model but it's not really representative of the general population RT @johnplattml: #NeuralNetwork beats experts at heads-up no-limit Texas hold'em poker, for the first time. Congratulations! https://t.co/o‚Ä¶ Nice blog post on Bayesian optimization with scikit-learn (with GPs) https://t.co/CxKNozAH9n Alternative impl. in: https://t.co/GubkjXU3LG @michaelaye basically it's like cython but with a pure python syntax. RT @dribnet: neural face grids with variable scale https://t.co/qfftEoynaD @dribnet impressive results. Have you checked if the model is overfitting ? pythran 0.8.0 (python to C++ compiler)  is out with experimental support for Python 3: https://t.co/sKpMJVrRhh RT @betatim: Data for those of you needing to persuade others to stop using legacy python https://t.co/4K08pD02hg RT @carolynporco: Just as a footnote: Should the entire Antarctic ice sheet ever melt, sea level would rise to the road deck of the Golden‚Ä¶ RT @vgoklani: A Practical Guide for Debugging Tensorflow Codes

An excellent set of notes for debugging your tensorflow code‚Ä¶

https://t.co‚Ä¶ RT @antonioregalado: Quotes by top players defeated by Google's "AlphaGo" in 2,500 year old game hugely foreshadowing of AI impact https://‚Ä¶ RT @smolix: #mxnet is joining Apache @TheASF. Delighted to see this come to fruition. Kudos to Mu, Tianqi, Eric and the rest! https://t.co/‚Ä¶ RT @raymondh: #python news:  InfoWorld write-up on the what there is to love about Python 3.6: 
https://t.co/5M7cr951fj RT @GaelVaroquaux: Python 3.6 great for science:
‚óè faster function call with Cython support https://t.co/wT9ocgeLdH
‚óè clean JIT support htt‚Ä¶ RT @CharlesOllion: Would anyone recommend an object detection NN based on tensorflow for teaching purposes? RT @NandoDF: Nvidia invents self-driving supercomputer - LipNet strikes again on car data! @BrendanShilling @iassael  https://t.co/f6iK69Kf‚Ä¶ RT @cjmaddison: New blog! First post w/ @numbercrunching builds Gumbel machinery for understanding the Gumbel-Max trick: https://t.co/BkQvT‚Ä¶ @fastml_extra it's a version of alphago: https://t.co/6FjD7ZsN1C RT @demishassabis: Excited to share an update on #AlphaGo! https://t.co/IT5HGBmYDr @le_roux_nicolas @hugo_larochelle congrats! RT @wesmckinn: New post: Native Hadoop file system (HDFS) connectivity in Python https://t.co/CRCulfIbjN #pydata RT @mrocklin: #Dask 0.13.0 released 

https://t.co/nL5vJavJgS RT @GiorgioPatrini: Great summary / remarks from #nips2016 https://t.co/5Ia85sP7tX RT @goodfellow_ian: A tech report summarizing my NIPS tutorial on GANs https://t.co/v1rAvkjMC1 RT @YadFaeq: neat 5x faster + sota "FastMask: Segment Multi-scale Object Candidates in One Shot"https://t.co/HhK3cKjJXa  Code: https://t.co‚Ä¶ @egrefen @haldaume3 @yoavgo if the integer components of the rational numbers are large they cannot be represented with int64 or float64 @haldaume3 @yoavgo @egrefen siegelmann and sontag 92 considered rational numbers. Executive summary here: https://t.co/gfQNdfv0RL @yoavgo @haldaume3 someone in the audience of @egrefen's nips talk told that RNNs are not turing complete with finite precision floats. RT @rasbt: TensorDebugger (TDB), a visual debugger extending Tf with breakpoints + real-time visualization of the data flow https://t.co/hu‚Ä¶ RT @ch402: Outrageously Large Neural Networks - state of the art LM using 12% compute to train, 6% / inference (but 28x params) https://t.c‚Ä¶ RT @rasbt: "2017 Outlook: pandas, Arrow, Feather, Parquet, Spark, Ibis" -Pandas 2.0 sounds a conv thing to have! via @wesmckinn https://t.c‚Ä¶ RT @edersantana: "YOLO9000: Better, Faster, Stronger. (arXiv:1612.08242v1 [cs.CV])" https://t.co/lzljGbs2DO RT @sedielem: Lots of interesting work on "fixing" GANs right now: https://t.co/EhDppNh0RW https://t.co/CNT82rTuHt https://t.co/PQvz5fJPYD‚Ä¶ RT @fastml_extra: Value Iteration Networks:
https://t.co/9v24pdY6FU

Theano Code:
https://t.co/cBJDMVd2m5

Unofficial TF code:
https://t.co‚Ä¶ RT @Smerity: Gated Convolutional Networks beat LSTM on LM (WikiText-103 &amp; One Billion LM for single GPU), faster than CuDNN LSTM
https://t.‚Ä¶ RT @AlexSteffen: Trying to make sense of the return of climate denialism, pipeline plans, Trump, Tillerson, Arctic Oil and Russia?
https://‚Ä¶ RT @hugo_larochelle: Trick when using REINFORCE for structured output: use test-time inference (e.g. greedy max) to get reward baseline: ht‚Ä¶ RT @dennybritz: Highlights of NIPS 2016: Adversarial Learning, Meta-learning and more - Nice summary by @seb_ruder https://t.co/pDd3nWL8sS RT @soumithchintala: Code &amp; Data for Key-Value Memory Nets and 3 more papers on Dialogue-interaction by #FAIR's Jason Weston et. al.: https‚Ä¶ RT @sedielem: Harmonic networks (@deworrall92 et al.) are fully rotation equivariant convnets. Very cool! https://t.co/4DHkF9LKLM https://t‚Ä¶ RT @cvondrick: Learning Features by Watching Objects Move by Pathak et al https://t.co/q3KmNzpF1f https://t.co/OAMF0qg05U @kchonyc I would init with https://t.co/R6zD8PPYN6 followed by one or two passes of mini batch kmeans then batch EM fine tuning till conv. RT @mblondel_ml: Enjoyed the sparsemax paper: starts from a not-so-obvious insight and develops it in several interesting directions https:‚Ä¶ RT @karpathy: A short/quick blog post: ‚ÄúYes you should understand backprop‚Äù https://t.co/fOsrsiU2HA RT @harvardnlp: Excited to introduce OpenNMT (https://t.co/gouHVyUsDS), an open-source neural machine translation developed for industrial‚Ä¶ RT @hardmaru: Implementation of Phased LSTM in TF (https://t.co/TlvMkNeUVl). Code from original author also available (https://t.co/hIIAjB8‚Ä¶ @betatim @amuellerml at least a uniform prior between chance level accuracy and 100% accuracy should inject a healthy level of uncertainty @betatim @amuellerml bootstrap is poor to estimate uncertainty with 5 samples. In this case a prior + bayesian credible interval is better. RT @soumithchintala: Yet again, @scottgray76 provides faster GeMM kernels than what NVIDIA provides in CuBLAS. This looks very useful.
http‚Ä¶ RT @xamat: My @Quora answer to "What were the main advances in machine learning/artificial intelligence in 2016?" https://t.co/kvbuMdtOgp RT @minrk: Took a while, but we shipped nbdime 0.1. Give it a try, let us know how it goes, and help out if you're interested! https://t.co‚Ä¶ RT @agramfort: It looks like I forgot to tell one student about the existence of GridSearchCV #teaching #datascience #python #scikit_learn‚Ä¶ RT @ankurhandos: releasing scenenet rgb-d, hope we can do online and active machine learning within a physics engine than just CV https://t‚Ä¶ RT @wesmckinn: PSA: @intel Threading Building Blocks (TBB) shed copyleft (to Apache 2.0) in September of this year https://t.co/hWc3Okjw3o RT @brendandburns: Everyone has there favorite k8s 1.5 feature.

Mine is "kubectl cp pod:/remote-file /local/file" RT @goodfellow_ian: There are now so many GANs that we have both "StackGAN" and "Stacked Generative Adversarial Networks" (different papers‚Ä¶ RT @jeffclune: We've open sourced the code for Plug &amp; Play Generative Networks (PPGNs), which produced these images: https://t.co/n4ClR4Jje‚Ä¶ RT @fhuszar: My summary of the #NIPS2016 Adversarial Training Workshop: Great talks, lots of theory, great progress
https://t.co/6XQ3YMiskj @vorpalsmith you can use mkdir to invalidate the directory listing negative cache of a folder iirc RT @bigdata: Simple &amp; fast seeding algorithm for k-Means: produces provably good
clusterings even without assumptions on the data https://t‚Ä¶ RT @macjshiggins: The open-source SDC community is now using Grand Theft Auto to test algorithms for image-based localization. This is gett‚Ä¶ RT @fastml_extra: Phased LSTM: Accelerating RNN Training for Long or Event-based Sequences
https://t.co/U1Z4LPQYBE

Code:
https://t.co/rqoi‚Ä¶ RT @petewarden: Here's a great slide deck on Google's latest object localization model: https://t.co/sMy3iUDThB RT @fastml_extra: Weight Normalization:
https://t.co/42rQkFJ6Ro

Keras, Lasagne and Tensorflow code from OpenAI:
https://t.co/725mH4tdsm RT @rasbt: "You can now resolve simple merge conflicts on GitHub right from in the PR, saving you a trip to the command line" https://t.co/‚Ä¶ RT @fperez_org: Excellent, detailed breakdown of the reasons for avoiding any deals with Elsevier @costofknowledge, by @talyarkoni 
https:/‚Ä¶ @samim haha, I just did almost the same tweet with screenshots in a different order (I put the architecture diagram first :) StackGAN: Text to Photo-realistic Image Synthesis with Stacked  Generative Adversarial Networks by Han Zhang et al:‚Ä¶ https://t.co/TDOjU4lF2Y @danilobzdok congrats! RT @wenmingye: AMD Introduces Radeon Instinct Machine Intelligence Accelerators @slashdot - https://t.co/vIKvzaeFk0 RT @ch402: Comparison of different GAN variants. I couldn't fit the full version in our paper. https://t.co/G0gVDincX9 RT @jiminy_crist: @ogrisel Emphasis on the "experimental" - there are still known bugs. Please don't use this yet. crick: experimental t-digest approximate out-of-core &amp; associative quantile estimation in Cython: https://t.co/uDShCqAPGR RT @PaulMineiro: Machined Learnings: NIPS 2016 Reflections #NIPS2016 https://t.co/e9JW6qAqeo RT @Smerity: Potentially favourite talk of #nips2016 - @egrefen's "Limitations of RNNs".
Sometimes the best way to make progress is to take‚Ä¶ RT @quantombone: NIPS 2016 trends: learning-to-learn, GANification of X, Reinforcement learning for 3D navigation, RNNs, and Creating/Selli‚Ä¶ Spatially Adaptive Computation Time for Residual Networks: https://t.co/iSwqGNfKZR #nips2016 https://t.co/dHIvf6HaFL @egrefen very nice talk, thanks. RT @karpathy: John Schulman's slides from today's "nuts and bolts of RL", great practical advice for getting RL to work https://t.co/HYgL68‚Ä¶ RT @soumithchintala: Content from my talk today: "How to train a GAN?" in a readable format. More contributions welcome: https://t.co/xFG6G‚Ä¶ RT @MirowskiPiotr: UNREAL deep RL agent presented by @VladMnih from @DeepMindAI #nips2016. Solving reinforcement learning in 3D games with‚Ä¶ RT @simon_jegou: @abursuc @ogrisel The code is now available here : https://t.co/6ilHoqa3j1 RT @fastml_extra: Learning to learn by gradient descent by gradient descent:
https://t.co/cCCX62tGzM

TensorFlow code:
https://t.co/AcocyIW‚Ä¶ @KyleCranmer @glouppe congrats on the excellent talk and very impressive results. RT @DeepMindAI: #DeepMindLab is now LIVE! For the code, maps and level scripts to get started, visit: https://t.co/JZXiMiAgik https://t.co/‚Ä¶ RT @vorpalsmith: New blog post: Why does calloc exist? https://t.co/OyXcy0CEa4 RT @GiorgioPatrini: @OpenAI releases Universe "a software platform for measuring and training an AI's general intelligence" https://t.co/BA‚Ä¶ RT @deviparikh: Introducing VQA v2.0! A more balanced and bigger VQA dataset.
https://t.co/hS2a1bWez5
@yashgoyal_ @tjskhot @DhruvBatraDB ht‚Ä¶ RT @karpathy: We're going to NIPS to talk about ICLR papers RT @cdixon: Stephen Hawking: "This is the most dangerous time for our planet." https://t.co/lQQVcgIu29 https://t.co/3fKFJsrX4c RT @MirowskiPiotr: Our work on navigation in 3D mazes from raw sensory data: deep reinforcement learning with auxiliary tasks (depth predic‚Ä¶ RT @jeffclune: Introducing Plug &amp; Play Generative Networks. These are images synthetically generated by #DeepNeuralNetworks. More: https://‚Ä¶ RT @gneubig: Fun fact: the amount of power to train Google translate (~12,600 kWh) is a bit more than the average US household uses in a ye‚Ä¶ RT @adnothing: Faster-R-CNN seems often on top! Plus: "We develop a unified framework (in Tensorflow)". By Google's K Murphy &amp; Co. Want :-)‚Ä¶ @dtunkelang @VinFL balancing powers in favor of attackers / trolls instead of defenders / regular people. @dtunkelang @VinFL it will be much cheaper to produce vast quantities of fake or biased contents shift opinions or do phishing at scale. RT @goodfellow_ian: reddit AMA for the #nips2016 adversarial training workshop panel: https://t.co/9GNVXGRKm5 (Ask now through Dec 8, panel‚Ä¶ @fhuszar have you seen Towards Principled Methods for Training GANs by Arjovsky &amp; Bottou https://t.co/OaQHiFlJUR ? In line w your blog post. @jaidevd faster rcnn in tf https://t.co/VQJHW5ppYw RT @samim: "Fast Face-swap Using Convolutional Neural Networks" - by @iskorna : https://t.co/7o4pIR209s &amp; https://t.co/B0p6gS0WgG https://t‚Ä¶ RT @gutelius: Google‚Äôs Hand-Fed AI Now Gives Answers, Not Just Search Results https://t.co/n972X3Hca9 sausagemaking #deeplearning RT @random_forests: TensorBoard now supports visualizing embeddings. Will make sure we have some even better tutorials soon. https://t.co/u‚Ä¶ RT @abursuc: The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for  Semantic Segmentation https://t.co/2IbRbS68xQ RT @thomaskipf: In case you missed it: TensorFlow v0.12 (RC0) is now available - via @ericjang11 https://t.co/o87zjF8VMn RT @jakevdp: Love this: Why are barns red? It comes down to simple stellar astrophysics. https://t.co/19GYJ865Qy RT @strataconf: .@googlecloud's tech lead for data &amp; #machinelearning shares how to pull data from #BigQuery into ML-ready data sets https:‚Ä¶ RT @haldaume3: New #nlproc blog post on Workshops and Mini-conferences (aka ACL-style vs NIPS-style workshops) https://t.co/D5CrvfhIEd RT @tarantulae: ImageNet embeddings with different tSNE perplexities. https://t.co/uFvrYbGlE9 RT @b0rk: floating point https://t.co/x83WTFeSV4 RT @johnmyleswhite: These animations of Nelder-Mead in 1D and 2D are probably the best Nelder-Mead viz I've ever seen: https://t.co/GnwGJRc‚Ä¶ RT @glouppe: We should really think about deserving an award to backpropagation, for working despite bugs in our code RT @NandoDF: If you care about big brother surveillance and the erosion of privacy and human dignity, please retweet this. Thanks https://t‚Ä¶ @glouppe that can also happen if you do exponential moving average to compute a smooth training loss while training &amp; the val loss at epoch @glouppe that happens if you use dropout when computing the training loss and not the validation loss RT @ojahnn: ConceptNet will be what protects us from AI going rogue, due to its sophisticated ethics. https://t.co/IRHipkntnN Speech-to-Text-WaveNet: End-to-end sentence level English speech recognition in tensorflow: code &amp; pre-trained model https://t.co/EB4Ooh2Kwq RT @dabeaz: I'd just like to thank all of the Python developers for making a really fun programming language.  Looking forward to Python 3.‚Ä¶ RT @thomaskipf: We are releasing the code for Semi-Supervised Classification with Graph Convolutional Networks (in TensorFlow): https://t.c‚Ä¶ RT @mat_kelcey: sweet jesus! a neural net has learnt an internal representation!!! nuke the site from orbit!!! https://t.co/fy0F6Ri0JJ RT @ylecun: Nice work from folks at INRIA, CNRS, and NeuroSpin in which the activity in the human visual cortex measured by... https://t.co‚Ä¶ RT @Smerity: The WikiText dataset page includes new SotA results on both WT-2 and WT-103 from FB's Neural LM w Continuous Cache
https://t.c‚Ä¶ RT @mat_kelcey: "FluidNet: Accelerating Eulerian Fluid Simulation With Convolutional Networks" https://t.co/mUuUBAQ0gy RT @mikeloukides: A new startup for deep learning in the cloud. Based on NVidia Pascal GPUs
https://t.co/nNJrlyvkxn @gctaylor @aronchick yes you can have more control by using a preemptible node pool and making it part of a cluster: https://t.co/tCzcOzJuNW RT @raymondh: #python news:  The final Python 3.6 beta is out.
You get extra credit for early adoption and feedback.
https://t.co/xstItTkkE1 RT @cvondrick: Fantastic conditional GAN results by Isola et al https://t.co/omdWEyfGF6 https://t.co/zRmknPgoTu RT @fulhack: FYI: The half-life of a line of code is very close to 2.0 years (for the top Python projects on Github... more analysis coming‚Ä¶ RT @Smerity: @yoavgo Sorry, Eq 6 was broken, fixed on ICLR now, arXiv soon. CUDA kernel code's also up for Chainer btw. Detail:
https://t.c‚Ä¶ RT @Smerity: @yoavgo Here's a link to the updated paper with correct Eq 6 on ICLR:
https://t.co/MGBCuPC5Nr RT @mblondel_ml: New in scikit-learn-contrib: "categorical-encoding", transformers for encoding categorical variables https://t.co/H5vrcKa5‚Ä¶ RT @hugo_larochelle: Excited to be coming back to Montreal! I owe Montreal so much, and am glad I get this chance to contribute back: https‚Ä¶ RT @thomaskipf: Google opens Deep Learning &amp; AI research group in Montreal, led by @hugo_larochelle https://t.co/xZYvzFAV1k RT @chris_brockett: .@yoavgo's NNs for NLP primer that we have all been secretly consulting has passed peer review and is now officially pu‚Ä¶ RT @goodfellow_ian: NIPS attendees: what would you like to see in my two hour tutorial on GANs? what would make a tutorial most useful to y‚Ä¶ @goodfellow_ian @edersantana convergence failure modes and their  mitigation tricks. RT @karpathy: Btw ~week ago we released PixelCNN++, a nice/efficient multi-GPU TensorFlow code, SOTA generative model on CIFAR-10 https://t‚Ä¶ RT @cstross: More evidence we‚Äôre living in the back-story of a 1960s ‚Äúcolonize space because Earth is fucked‚Äù future history novel. And it‚Ä¶ @Dominik_Onagrin @espoirdansbas @Gjpvernant @Merome_net ex les media ont tendance a donner plus la parole a ceux qui ont chance de gagner @dovgalec @fchollet then just financially support the journals you judge of highest quality: I support mediapart, the guardian and le monde. RT @Smerity: What started as a summary of Google's Neural MT paper ended as a "from ground up" description of the architecture :)
https://t‚Ä¶ @yoavgo @kastnerkyle @github this function needs a doctest with a couple of examples to explain what it does. RT @nteractio: Checkout this post by Hydrogen maintainer @_lgeiger on the 1.0.0 release of Hydrogen!

https://t.co/erIVOvQ7yX RT @glouppe: My slides on how to build a classifier independent of nuisance parameters https://t.co/wk8zKbsStG Excited reactions from physi‚Ä¶ @haldaume3 @lousylinguist for compviz, feature eng is not as important as it used to be. Arch / loss engineering and labeling seem important @graphific @edersantana @Swayson here is some doc for the HoverTool with custom tooltips: https://t.co/M7ejHsnOPQ @edersantana @Swayson it should be doable with bokeh. We also have static mpl based rendering in the manifold examples of sklearn. @kastnerkyle @honnibal @explosion_ai @yoavgo another NLP model with deep DenseNet-style skip connections: Quasi-RNN https://t.co/4JR4mtSm5f RT @kchonyc: Last year's NIPS workshop on RAM, I talked about the importance and emergence of multilingual machine... https://t.co/UtVUFD5m‚Ä¶ RT @Smerity: Quasi-Recurrent Neural Network (QRNN)
elementwise recurrence for similar/better results to LSTM but up to 16x faster
https://t‚Ä¶ @tarek_ziade I like the xps 13 very much. I only miss keynote. RT @jackclarkSF: Fei-Fei Li is partially responsible for the deep learning boom &amp; her group created the ImageNet dataset which kickstarted‚Ä¶ RT @johnplattml: Excellent theoretical support for residual networks by @mrtz https://t.co/zbwsHXckNX #deeplearning RT @Smerity: GNMT applied multilingually (first token specifies target language) improves all language pairs and nudges zero shot
https://t‚Ä¶ RT @GiorgioPatrini: The other #nips2016 paper award: in usual non-convex objectives for martix completion, all local minima are global http‚Ä¶ RT @GiorgioPatrini: Value Iteration Network gets a best paper award at #nips2016, a differentiable planning algorithm for RL https://t.co/8‚Ä¶ @honnibal @explosion_ai @yoavgo https://t.co/4Goia04Nbs @honnibal @explosion_ai @yoavgo deep nets with residual connections are used by google translate (both for the encoder &amp; the decoder). RT @explosion_ai: New blog post: The new 4-step deep learning formula for state-of-the-art NLP models https://t.co/IeO5sdTKnQ RT @NandoDF: LipNet for lip reading - Oxford news - a better more measured way of reporting. https://t.co/yHyIlixh61 RT @Maciej_Kula: A very thorough introduction to learning-to-rank recommendations using LightFM (read other posts in the series for alterna‚Ä¶ RT @tdeboissiere: Reproducing some of the results of (https://t.co/ZOkJOoF0IX) aka the new Eve optimizer.
https://t.co/A0bEtcJEq6 https://t‚Ä¶ RT @alexjc: More evidence there are always multiple labs working on big ideas; conference deadlines synchronize the publication times. #zei‚Ä¶ RT @alexjc: End-to-end Optimized Image Compression https://t.co/mZ3BjFxqDB Applying convolution to beat JPEG2000 (again) with perceptual mo‚Ä¶ RT @kchonyc: Very interesting observation and solution: See Tables 3 and 5 https://t.co/XQZBS8dD6R RT @chrisemoody: Reverse grads: 1 *character* code for adversarial nn. invariance over diff domains https://t.co/uZMgGvHlNn @yaroslav_ganin‚Ä¶ RT @BigsnarfDude: My code for Faster R-CNN pedestrian-and-car-detection blog post https://t.co/XS7cdkFdDo @Karos_fr only followers of the VentureBeat account will see this tweet in their timeline because of the leading @-mention. RT @dennybritz: Hunting through the ICLR 2017 submissions by @Smerity https://t.co/wk6Ebs9AUN RT @demishassabis: #AlphaGo update: we've been hard at work improving AG, delighted to announce that more games will be played in early 201‚Ä¶ RT @fchollet: Snapshot ensembles - a new take on an old idea... I'd like to see a comparison with Polyak averaging. https://t.co/6XfPbU0q3A RT @lawrennd: Careful result is misleading. It's a limited vocabulary &amp; grammar dataset developed by colleagues at @shefcompsci, I know cos‚Ä¶ RT @fhuszar: Fresh from our lab
Lossy Image Compression with Compressive Autoencoders
beats JPG+on par with or better than JPEG2k
https://t‚Ä¶ RT @prostheticknowl: LipNet: DL research from @BrendanShilling @iassael &amp; @NandoDF can deduce sentences from visual analysis of speech http‚Ä¶ RT @Miles_Brundage: "Tracking the World State w/ Recurrent Entity Networks," Henaff et al., Facebook: https://t.co/nGvxcyWar7

SOTA on bAbI‚Ä¶ RT @fchollet: Deep learning is getting out of control. Out of ~500 ICLR submissions, there over 100 that I should probably read. Definitely‚Ä¶ RT @DeepMindAI: BLOG: Our collaboration with @Blizzard_Ent &amp; what makes Starcraft II such an interesting environment for AI research https:‚Ä¶ RT @Smerity: I felt a great disturbance in the Force, as if millions of voices suddenly cried out in terror and ICLR was suddenly silenced.‚Ä¶ RT @DeepMindAI: We're collaborating with @Blizzard_Ent to open up StarCraft II as an AI research environment for the global community in Q1‚Ä¶ RT @heuritechdata: #Deeplearning #Meetup #7 @heuritechdata on #convolutionnal #neuralnetworks: a brief report https://t.co/yX7O55ihlu https‚Ä¶ RT @kchonyc: We have human evaluation in our revision of fully char-level NMT (https://t.co/ebBfhmvJdZ) thanks to @yvette_graham! @jasonlee‚Ä¶ RT @NandoDF: Deep Learning - a physics perspective https://t.co/RMyhQNHEoy This is old news, but worth reading if you missed it. RT @johnplattml: Face recognition via #deeplearning can be fooled by printing adversarial patterns on plastic eyeglass frames https://t.co/‚Ä¶ RT @Jeffrey04: wao, now annoy is in gensim https://t.co/X6mJrijNbW RT @jackclarkSF: StarCraft as new AI battleground? Researchers propose 'TorchCraft' AI framework https://t.co/gNPQ6x8Q2E &lt; Brood War. Torch‚Ä¶ RT @iclr2017: Submissions for @iclr2017 are open (using OpenReview): https://t.co/kPEy00dukm. See also a FAQ here: https://t.co/JzdW3pgUrx‚Ä¶ RT @hardmaru: Revisiting Distributed Synchronous SGD. Scalable distributed training of Inception, PixelCNN. #GoogleBrain #ICLR2017 https://‚Ä¶ RT @kchonyc: The code and pretrained models for fully character-level NMT now available at https://t.co/QcW5Cp0JoE great work by @jasonleei‚Ä¶ RT @nalkalchbrenner: New neural net for Language and Machine Translation! Fast and simple way of capturing very long range dependencies htt‚Ä¶ RT @karpathy: Awesome work by @crizcraig on DeepDrive: https://t.co/anhUMikPjM Using GTA 5 simulator to benchmark self-driving car models.‚Ä¶ RT @GiorgioPatrini: We are starting a new research project under https://t.co/1Eix5NXUYv on incremental training of ResNets. An experiment‚Ä¶ @syhw @kchonyc https://t.co/1Zodjzt4U4 https://t.co/aeImoEFnyj @syhw @kchonyc It works for me... Recurrent highway networks with torch and tensorflow code: https://t.co/siT84bIM1G RT @mattturck: Some of the greatest minds in AI predict the next 12 months: @rsalakhu, Hinton, Sutton, Bengio, moderated by @dfjsteve #mkt4‚Ä¶ RT @quantombone: Time to put your extras up on stubhub. NIPS 2016 main conference sold out. #nips #deeplearning https://t.co/Qxt8INUbmT RT @rgbkrk: PySpark users, please weigh in on making `import pyspark` and `pip install pyspark` easier. https://t.co/wz8BYZap2z @haldaume3 @geomblog why not just minimize the likelihood of a car crash (and remove the kill the driver vs other people from the equation). RT @AndrewYNg: Why do people think the trolley problem is critical for self-driving cars? The trolley problem wasn't critical even for trol‚Ä¶ RT @_olivier_: Like every year, I just donated a few euros to @Wikipedia. What about you? #keepitfree https://t.co/X4KJjJba7E RT @npinto: Super heavy-weight #deeplearning talent at @kindredai! @rsalakhu @jabergT https://t.co/gvIdtcDTAh ! Exciting... TIL: it's possible to collect memory and exec time statistics for any tensorflow op and use tensorboard for viz:  https://t.co/hrgDuUVKt8 RT @McGillU: AI pioneer Yoshua Bengio is launching deep learning incubator‚Äìin #Montreal https://t.co/5Fvj7lDLZA via @WIRED RT @soumithchintala: a fun post with softmax approximation code, getting 40 perplexity on 1-Billion-words with 1 GPU in 6 days: https://t.c‚Ä¶ RT @fastml_extra: Microsoft CNTK 2.0 (now called Cognitive Toolkit) with Python API:
https://t.co/41E9iMvORl

HT @xdh RT @fchollet: Using deep learning to learn a latent space of molecules, allowing to design new chemicals with specific properties: https://‚Ä¶ RT @mat_kelcey: "nice to have: 2+ yrs experience with tensorflow" it's your lucky day recruiter cause there's not many people that could sa‚Ä¶ RT @SebastienBubeck: Already 8000 registered for NIPS 2016, it's insane... RT @zephoria: Geeks - Nov 18 in NYC: Fairness, Accountability &amp; Transparency in Machine Learning event. Register here: https://t.co/mQIS1ef‚Ä¶ RT @seanmcarroll: ‚ÄúThe best way to attract an audience for political content is ‚Ä¶ false or misleading information that tells people what th‚Ä¶ RT @aronchick: DUDE. This is rad! Preemptible VMs with Google Container Engine | gcloud container clusters create preemp-vm-cluster --preem‚Ä¶ RT @fulhack: Cool w better support for 16 and 8 bit floating point support by Nvidia https://t.co/aYtARzssfl https://t.co/dlvEwHQx44 RT @Mbussonn: Add 8.8.8.8 and 208.67.222.222 to your DNS servers to get back @GitHub RT @GiorgioPatrini: Neural abstract machines and program induction @nips2016 workshop looks rather awesome, as well as the webpage https://‚Ä¶ RT @fchollet: Code &amp; ImageNet weights for the Xception model are available as part of keras.applications: https://t.co/H2n5PQzSTO (TensorFl‚Ä¶ RT @fchollet: Not a bad comparison, but to be realistic you'd have to reduce it to ~100ms. There's a lot of magic that can happen in the br‚Ä¶ @fchollet @yoavgo will the AI golem protect humanity against its own evil? @amuellerml @DSI_Columbia congrats! RT @haldaume3: Castro (google) on nnets=legos: most of what u see at ML confs is new ways of putting legos together; rare anyone even inven‚Ä¶ RT @DmitryUlyanovML: Released multicore t-SNE implementation with Python and @TorchML wrappers. Still much room for improvement. https://t.‚Ä¶ RT @AndrewYNg: Pretty much anything that a normal person can do in &lt;1 sec, we can now automate with AI. RT @fchollet: Introducing the Artificial Intelligence Open Network: a 100% open-source AI research community. https://t.co/i6G27YUgsF TIL: it's possible to use chrome to visualize the execution trace of some tensorflow code for perf profiling: https://t.co/s19y5vzayf RT @AndrewYNg: Government investments in AI should dedicate significant effort to creating big, open datasets. This lifts the whole industr‚Ä¶ RT @JCoutoNLP: Amazing results using convolutional and #LSTM NN: Achieving Human Parity in Conversational Speech Recognition https://t.co/8‚Ä¶ pyflame: a low overhead ptrace-based sampling profiler for Python on POSIX systems: https://t.co/SS7phfy1Ej /cc‚Ä¶ https://t.co/Qs1OZDAeBu Why Deep Neural Networks? by Liang &amp; Srikant: https://t.co/IKwwg2vMSX deep vs shallow nets for univariate and multi‚Ä¶ https://t.co/NiQyrKDoCA @edersantana 42 RT @F_Vaggi: That feeling when 5 pages of math simplify down to 10 lines of numpy code, and they work on the first try. https://t.co/v2WaLc‚Ä¶ RT @rsalakhu: Excited about joining Apple as a director of AI research in addition to my work at CMU. Apply to work with my team
https://t.‚Ä¶ RT @VictorStinner: corefreq: Linux kernel module monitoring the true processor frequencies, Turbo Boost, C-states, temperatures, ... https:‚Ä¶ RT @wesmckinn: PSA: The pandas 2.0 work has moved to https://t.co/21Vmyddq80 with git repo at https://t.co/hA2w7m2uUE #pydata RT @alxndrkalinin: LightGBM: Light Gradient Boosting Machine ‚Äì new #xgboost competitor by @MLatMSFT #machinelearning https://t.co/ViiqH8Jw84 RT @dstufft: Today I was notified that my position at HPE is affected by layoffs so I am looking for a new role ‚Äî https://t.co/A9XLekuX25‚Ä¶ RT @sedielem: Why deconvolution artifacts exist and how to get rid of them. Loving the interactive widgets :) https://t.co/2CpdpuMivA RT @googleresearch: Check out an interactive article on how to effectively use and interpret t-SNE visualizations - https://t.co/OrAHm6GlHQ RT @fulhack: There should be some kind of Bonferroni correction effect if you run the unit test suite too much RT @amuellerml: "Introduction to Machine Learning with Python" with @sarah_guido is out now! https://t.co/g8YEa9fZzp - applied machine lear‚Ä¶ RT @astrofrog: Now THIS is a hockey stick graph I'm happy about #python https://t.co/awRa8kuzFt RT @jakevdp: Quick thought for the morning while writing an awkward Python generator expression... anyone know if something like this has b‚Ä¶ RT @fchollet: Awesome project from @transcranial: Keras.js - run trained models in the browser, with GPU support via WebGL. https://t.co/Pb‚Ä¶ @fchollet it uses concat merges therefore not modelling the residuals as in resnets though. RT @fchollet: A Keras implementation of DenseNet: https://t.co/kTskwZeP5b - an extension of ResNet where every block is connected to all pr‚Ä¶ RT @yaringal: New blog post! "Uncertainty in Deep Learning" - also my PhD thesis and lots of new results https://t.co/PPrX1gsg7N https://t.‚Ä¶ RT @soumithchintala: Linux kernel push confirms, Intel to add dedicated neural network hardware ops into their future processors
https://t.‚Ä¶ RT @TEDxBoston: @hugo_larochelle of @TwitterEng talks ‚ÄòThe Deep End of #DeepLearning‚Äô at #TEDxBoston2016 Vid at https://t.co/BN92DQBYTc htt‚Ä¶ @karpathy also could you please estimate the Bayes rate on CIFAR-10 for us? ;) @karpathy you should have trained harder. RT @MirowskiPiotr: Alex Graves and Greg Wayne from @DeepMindAI explain today's Nature paper on Differentiable Neural Computers:
https://t.c‚Ä¶ @demishassabis @DeepMindAI @nature maybe you should consider publishing in an openaccess journal instead. @tjungblut still that proves that @DataRobot tools can help to get competitive results very quickly. RT @fchollet: The updated version of the Xception paper is up: https://t.co/D876HseFDo RT @pfau: New work explaining GANs in the context of density ratio estimation from colleagues @shakir_za and @balajiln: https://t.co/Fc3dFb‚Ä¶ @p_eev @Cafedeladanse bravo et merci, une tres belle decouverte. RT @jackclarkSF: 300 lines of code to train an RL agent to drive a (simple) car in a (simple) world! https://t.co/zbA48rt7ly @kchonyc looks very interesting. Will the code and the pre trained models be released? RT @kchonyc: Finally, fully character-level NMT! When trained as an N-to-1 model, automatically handles intra-sent code-switching https://t‚Ä¶ RT @SylvainCorlay: Run C++ in the #Jupyter notebook now! With the #cling interpreter and #mybinder:

https://t.co/XJmRw6imfZ

@thefreemanla‚Ä¶ RT @glouppe: A new perspective of GANs through density ratio estimation https://t.co/y0Pt7xzgrv Nice paper! Fractional Max Pooling has been implemented in TensorFlow 0.11:
https://t.co/H2J6j0WE8P Statea by Vanessa Wagner and Murcof is a beautiful album of reinterpretations pieces by Erik Satie, Philip Glass, Aphex Twin and others. RT @fchollet: New paper: "Deep learning with separable convolutions". https://t.co/D876HseFDo - exploring what's next in convnet design aft‚Ä¶ RT @M_Steinbuch: robots take over https://t.co/RIHFruGizt RT @fperez_org: Brilliant,@Mbussonn shows how to remap existing plot made with Jet to Viridis (under some assumptions) https://t.co/ckTgBMY‚Ä¶ Connecting Generative Adversarial Networks and Actor-Critic Methods by
@pfau &amp; @OriolVinyalsML: https://t.co/mx9Hhp2lEZ RT @yoavram: py-earth: Multivariate Adaptive Regression Splines algorithm, in the style of scikit-learn
https://t.co/nqP8aBIJO4 @amuellerml @functiontelechy you can measure the rest to gain insights on the problem. RT @amuellerml: What interface would you like from a GridSearchCV w/ multiple metrics?(see https://t.co/CQ3nkDZyvh).How to select the param‚Ä¶ RT @alexjc: If you ever worried there won't be jobs in the future, watch this guy helping robots train‚Äîforever. (Next decade in a nutshell?‚Ä¶ RT @amuellerml: I started on the deprecation library that I always wish someone else already wrote: https://t.co/OnL3BDbh1q feedback welcom‚Ä¶ RT @nalkalchbrenner: 2/5 A strong baseline on the Moving MNIST benchmark produces samples like this https://t.co/4y4cATN82H RT @nalkalchbrenner: 3/5 On other hand for the Video Pixel Network the generated digits look like this https://t.co/1IS52CY6vP RT @nalkalchbrenner: 5/5 More videos generated from the #VideoPixelNet showing how the network generalizes to novel objects not seen during‚Ä¶ RT @sedielem: After PixelRNN/CNN and WaveNet, VPNs: more state of the art results with autoregressive models! https://t.co/z2XC1lGJ2I RT @googleresearch: The Google Brain team, DeepMind and X discuss research towards general-purpose skill learning across multiple robots ht‚Ä¶ RT @kleinsound: The 2-bit Resnet result is notable. ~90-75% sparsity is in line with previous work I've been involved with. https://t.co/KX‚Ä¶ @agramfort I wish I could "unlike" a tweet sometimes ;) RT @jreback: pandas 0.19.0 released. has lots of goodies, see https://t.co/fV5oYBfDrs #pandas Interesting tl;dr + analysis in the comments of this tweet: https://t.co/0rrNk0umOk @yoavgo @googleresearch indeed. @yoavgo @googleresearch the blog makes it explicit that its annotated with an inception net. @yoavgo @googleresearch if they put the infra in place to improve the labeling quality over time that can become an excellent reference. RT @deliprao: An under appreciated fact: @google is doing what no major AI company is doing -- sharing massive datasets, and models pretrai‚Ä¶ RT @mrocklin: Screenshots using #Dask with JupyterLab https://t.co/Iqg0IMNhvp RT @fchollet: Google releases the Open Images dataset, a strong alternative to ImageNet --larger-scale, with a better class set: https://t.‚Ä¶ RT @samim: Finally Amazon EC2 is stepping up their ML-GPU game, nice! https://t.co/dS6UBM5HyN RT @johnplattml: Soft weight sharing by having small network generate weights of larger one https://t.co/l379vc6X3g #deeplearning @yoavgo @Smerity @jacobeisenstein I think the mapreduce architecture was novel when that paper was published. @yoavgo @Smerity @jacobeisenstein it's not a research paper but rather an engineering tech report on a production system. RT @machinalis: PostgreSQL 9.6 Released! 
https://t.co/AN7XBuwXKZ
#PostgreSQL https://t.co/btoQ4uUZbL RT @BrendanShilling: Google's GNMT blog post. Also lots of interesting practical findings in the paper: https://t.co/BJDct1UNqt https://t.c‚Ä¶ RT @fchollet: Google releases YouTube-8M, a labelled dataset of visual features extracted from the frames of 8M YouTube videos: https://t.c‚Ä¶ RT @hannawallach: New paper on noun phrase extraction for phrase-based text analysis!!! https://t.co/xJYGJ6c9QK @AbeHandler @MatthewJDenny‚Ä¶ RT @amuellerml: Scikit-learn 0.18 is out! Update now with conda or pip! Check out the changelog here: https://t.co/za27b0cqJm #sklearn (at‚Ä¶ RT @karpathy: ImageNet ILSVRC 2016 results are out https://t.co/Gj9KzvI8vh congrats to Trimps-Soushen (0.02991 error), &amp; FAIR team with 0.0‚Ä¶ @teddyknox @Miles_Brundage there are a couple of en to fr samples at the very end of the pdf. RT @Miles_Brundage: OK, the debate's over, let's go back to being impressed by this. https://t.co/g12jyZKCxc @benoitc https://t.co/7CcxvAJjpY I don't remember who is in charge of setting up new lists. @benoitc https://t.co/Dfd8S796Gt RT @fchollet: Generating faces with deconvolution networks: https://t.co/UJRacIJp2w - largest resolution I've seen yet. Video: https://t.co‚Ä¶ RT @pchapuis: Catching up on XKCD. Horse brains as a metric for AI sounds like a decent idea! https://t.co/i5QRIcNBMj RT @ellisonbg: Exciting taste of things to come for @ProjectJupyter in JupyterLab - text editors hooked up to kernels+code consoles: https:‚Ä¶ RT @mrocklin: New blogpost about deploying #Dask with cluster resource managers like Kubernetes and Marathon

https://t.co/HiaSlKs0MN RT @googleresearch: We‚Äôve made the latest version of our image captioning system available as an open source model in #TensorFlow - https:/‚Ä¶ @ankurhandos I wonder if dni with synthetic gradients is not a better solution to that memory issue. RT @ankurhandos: has anyone seen any open source implementation of memory efficient backpropagation https://t.co/uBKz2SDbZX? either torch o‚Ä¶ RT @vambenepe: Global Historical Daily Weather Data now available in BigQuery https://t.co/BGGUUcYZbn Not just a one-off drop, it's continu‚Ä¶ RT @mxlearn: FastBDT: GBDT C++/Python Library (code and paper). Claims fit speed superior to Xgboost https://t.co/TrzckWC4Rz RT @ajratner: Dump hand-labeled training data- use weak supervision w data programming! https://t.co/3tkHNytL73 In new Snorkel 0.4 https://‚Ä¶ RT @Alexis_Verger: When success rate drops below 20-25%, grant selection process becomes a lottery. Peer-review system fails and is not bui‚Ä¶ @NandoDF @DouglasCarswell @Nightingale_P @DuncanWeldon on the contrary: everybody incl. politicians should have basic scientific education. RT @cvaartjes: Bquery v0.2 is out! Bringing big data aggregations over billions of records to your laptop.
https://t.co/0EedWL1GNI RT @dustinvtran: Slides for my Edward talk at Twitter are available here https://t.co/mSeRCAI2qo https://t.co/tpaihAPK94 RT @europython: Budapest BI Conference (Oct 25-27) has a Python data track,
CfP closes Friday! 

https://t.co/EFndUrxLuC RT @notmisha: Playing FPS games with Deep RL https://t.co/ZHiUaa9J7S Videos: https://t.co/MOgB5pht3U RT @karpathy: Video of DQN playing Doom deathmatch https://t.co/FBUZ8NgAJG fun to watch! Superhuman aim is well within DQN capabilities RT @fchollet: On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima https://t.co/a4QDtWL86g - Keras code: https://‚Ä¶ RT @haldaume3: dialoguing about dialogue with dialogue+ML experts at @nipsconference ... let's discuss! workshop https://t.co/geX1HMLH4h, s‚Ä¶ RT @fchollet: Keras 1.1.0 is out. Note that TensorFlow is now the default backend for new installs. RT @KyleCranmer: Damn! @kchonyc we were beaten to the punch! Thanks @functiontelechy https://t.co/VIWbxc5WEK RT @hugo_larochelle: Sounds like a key result and insight to know about. https://t.co/BUSOHL6B6c @domonkostikk @abellogin not all businesses have millions of daily users. Is recsys useless for them? @domonkostikk @abellogin honnest question: how many interactions / item or user do you need to make recsys useful in a service / product? RT @fchollet: The secret sauce behind @dribnet's latent face spaces: https://t.co/AnpfDq4OsL RT @KyleCranmer: The deep learning donut symbol for repeated composition https://t.co/qX0vLxI7Fv @maosbot @NandoDF are there public datasets for this task? That would make a great kaggle contest. @deliprao what's wrong with perplexity? Do you suggest another metric? @deliprao if it's pretrained... RT @npinto: Smart. @Snips is using barometers to help localize yourself underground without sacrificing your privacy! https://t.co/ozG4H9cJ‚Ä¶ RT @sedielem: Slides for my talk about content-based music recommendation at the @dlrs2016 workshop this morning: https://t.co/xZTizeN7YF #‚Ä¶ RT @arxiv_cscv: Fully-Convolutional Siamese Networks for Object Tracking https://t.co/rx2fvAcCYl @yoavgo @majortal @tallinzen Please stop tweeting such things. You are all breaking our beautiful language. RT @alexjc: 3D Face Reconstruction by Learning from Synthetic Data https://t.co/5vE0SPqS11 (Hybrid DCNN/rendering architecture.) https://t.‚Ä¶ RT @keunwoochoi: ‚ÄúConvolutional Recurrent Neural Networks for Music Classification‚Äù by @keunwoochoi G Fazekas @markbsandler  @kchonyc https‚Ä¶ RT @yaringal: I put the code from Dropout Uncertainty paper on Github.
Playing w it &amp; setting 10x epochs got big RMSE improvement!
https://‚Ä¶ RT @amuellerml: Use "pip install scikit-learn==0.18.rc2" or "conda install -c https://t.co/FdxPtSjYRe scikit-learn=0.18rc2" https://t.co/EG‚Ä¶ RT @goodfellow_ian: Amazing computer assisted art demo from Berkeley. Draw a triangle, a GAN turns it into a photo-realistic mountain. http‚Ä¶ RT @samim: China has a 5000y old (food) culture. Shifting in fascinating ways lately. First this: https://t.co/MOfEC5Tl4C now: https://t.co‚Ä¶ Very nice intuitions on the reversibility of a hierarchical generative process and polynomial log probabilities. https://t.co/JgODuN9Sxq RT @fchollet: Direct feedback alignment for training neural networks: https://t.co/o4uaxyluZJ - can train NNs with random projections of th‚Ä¶ @jakevdp @ellisonbg maybe @hadleywickham lives on a cargo ship that always travels east to west. RT @smilevector: üòÄ‚¨Ü #NeuralPuppet https://t.co/L14vWQT7BQ @ianozsvald it's not released yet though :) RT @soumithchintala: Energy-based Generative Adversarial Network. Recasting GANs to an energy-based framework by Zhao, Mathieu &amp; @ylecun ht‚Ä¶ RT @mrocklin: Brief comparison of Dask and Celery

https://t.co/w8WIFRqq8g RT @yoavgo: Finally pre-trained LM release! (though I wish someone would release a good-but-not-huge one also...) https://t.co/kqpKI9EiWA RT @Mbussonn: Finally ! #Python 3.6 has a ModuleNotFoundError subclass of ImportError ! We might be able to give users better error message‚Ä¶ RT @jackclarkSF: Next frontier of AI VS Human competition after AlphaGo could be Starcraft. DeepMind also working on this. https://t.co/7ud‚Ä¶ @jtaylor108 @rgbkrk alternatively you can use docker on travis with a 32 bit image such as https://t.co/UYH25nf5EP RT @mrocklin: Notes and new features for a new release of #Dask's distributed scheduler 

https://t.co/yLBbYFCGuA RT @dennybritz: Stacked Approximated Regression Machine manuscript withdrawn. 

https://t.co/NPIKgcVFQB

Was too good to be true :/ @fperez_org I think this is a new result but not very familiar with the early neural nets literature. Very nice notebook! RT @Maciej_Kula: I'm a convert to the Keras functional API. RT @raymondh: #python3.6 news:  OrderedDict is dead. Long live dicts that are ordered.
Regular dicts are ordered and more compact: https://‚Ä¶ @jakevdp @DeepMindAI actually some of them sound scandinavian to me. @OnyxSixFour @mat_kelcey @graphific like an upgraded version of Miyawaki et al. 2008 https://t.co/bcCNTlG5P2 RT @DeepMindAI: Excited to share our latest research on speech and music generation! Hear our #WaveNets speak for themselves: https://t.co/‚Ä¶ RT @rasbt: "Creating Pandas DataFrames from Lists and Dicts" from_dict vs from_records from_items https://t.co/ennLNiqXBA https://t.co/r49p‚Ä¶ @AlecRad this is exactly what I thought. If inference is cheap it would be a great infinite animated painting on the wall of a waiting room. @graphific like a vision of a precog in the minority report movie for instance. @graphific this feels like a scene inspired from a philip k dick novel. RT @graphific: Impressive work generating videos with a GAN #dlearn https://t.co/rtP7MR8KvO https://t.co/iwVoGixvcu @vnfrombucharest I misready your question. I guess bz2 was not updated in time to be consistent and now it's too late to do it... @vnfrombucharest In Python 2 default open() not decode the text encoding into a unicode object by default. RT @jorisvdbossche: Testing very welcome! Can be installed with "conda install -c pandas pandas=0.19.0rc1" or "pip install --pre pandas" ht‚Ä¶ RT @jnuneziglesias: Whoa. GitHub just added an option for maintainers to push directly to PR branches:

https://t.co/m6yLbh8Amm

This is li‚Ä¶ @F_Vaggi lbfgs handles it internally: you just pass the obj func and its gradient. BTW TF is not just for deep learning. @DeepSpiker @fhuszar exactly: powerful function approximation is a generic tool that can be embedded in many kinds of inference frameworks. TensorfFlow now comes with an interface to use any scipy optimizer (e.g. LBFGS) to minimize TF loss functions: https://t.co/p412YjFHOf @nufa204 @amuellerml it depends on the kind of preprocessing. Plz ask more specific question on stackoverflow with the [scikit-learn] tag. RT @NandoDF: Hierarchical Multiscale Recurrent Neural Networks -- this is great work! https://t.co/wjW4emqqOr @fchollet I need to read the pcanet paper to understand how lda is used to init conv filters. This should be explained in supplementary mat. @fchollet indeed the paper is hard to follow. The sparse coding unrolling story seems decoupled from the experiments. @fchollet there might be an important technical detail that you missed, eg the "skip connections" or in the way pca/lda is used to get F. @fonnesbeck Tip use the "--pre" flag to install pre-releases: pip install --upgrade --pre PyMC3 RT @fonnesbeck: A release candidate of PyMC3 has been posted to PyPI. Please give it a try! https://t.co/eWC3xzYH0k RT @mxlearn: The 9 Deep Learning Papers You Need To Know About! https://t.co/KyahVwYMIk RT @sdouche: New PEP 530: Asynchronous Comprehensions #python - 
https://t.co/17o41sPuUU Deep Neural Networks for YouTube Recommendations https://t.co/Cc7cqh4nSD 1 net for candidate retrieval &amp; 1 for ranking w/ engineering tricks RT @kaggle: Leaf classification launch! Are you missing the random forest for the leaves? https://t.co/h7x2RWM8dM https://t.co/Gu84uxXvrj RT @NOAAClimate: Teaching #Climate? Check out this animated map of global wind and weather https://t.co/eCaBvGhRxx https://t.co/nliJ298TcD RT @ylecun: Video prediction with adversarial training, now in TensorFlow.

Matt Cooper from Brown University wrote a... https://t.co/hmU5r‚Ä¶ @talbaumel @yoavgo I think alex graves did use a similar trick in his old paper on generating hand written chars. @yoavgo @F_Vaggi increase the entropy of the softmax distribution by biasing it towards a uniform prior. @fchollet if there are a few powerful AIs that create a lot wealth and AIs are owned by few people then we have a concentration issue. @ncroc2004 you can have a deep autoencoder where some hidden units can be clamped to one-hot ended class labels during training. @ncroc2004 if you feed the same data both as input and output you get an autoencoder. @ncroc2004 neural nets with an l2 loss can be used both for supervised and unsupervised without being explicitly pro abilitic models. @ivanov great news, congrats! RT @haldaume3: this is really nice! https://t.co/hgA5KOnvBI RT @dloss: TIL: Orange version 3.0 switched to NumPy and Scikit-learn instead of dedicated C++ components. https://t.co/wxpT6gHZ17 RT @bigdata: üëè @UCBerkeley for setting up a new Center for Human-compatible #AI https://t.co/u9SfeqbQ9x üôè Director Stuart Russell https://t‚Ä¶ RT @fchollet: "Machine comprehension" using Match-LSTM --current state of the art on the Stanford QA dataset https://t.co/Z1vBKpq6eX @fpedregosa @vborghesani congrats! Slides of my #euroscipy talk on docker &amp; kubernetes for Python compute clusters for ML w/ joblib &amp; dask/distributed https://t.co/Ix5HOs47GT RT @karpathy: New OpenAI post on our Deep Learning / experiments infrastructure https://t.co/EvbrhZOZN5 most of which I'm busy learning rig‚Ä¶ RT @AstroKatie: It's OK to be sad when a beautiful hypothesis is killed by ruthless data. But take comfort: its sacrifice makes the herd st‚Ä¶ RT @thomasdarimont: Jupyter Kernel for Java 9 Kulla:
https://t.co/J7fh9B4pRV https://t.co/QEXK1j39iJ RT @maxjaderberg: Check this new blog post which sheds some light on our new paper decoupling neural networks https://t.co/igJd4ZncFP https‚Ä¶ RT @amuellerml: You can now search over the steps in a pipeline in scikit-learn (in the dev version): https://t.co/J2wTsYrb4I wohoo! (cc @r‚Ä¶ RT @dabeaz: So, here's the screencast from my PyData Chicago 2016 talk yesterday.   Have fun ;-).  https://t.co/XivkU15Gu6 RT @sedielem: @sedielem Monetary rewards are nice of course, but being able to share solutions is what got me my current job. RT @sedielem: @sedielem I've found solution sharing to be the most valuable aspect of Kaggle, both sharing own solutions and learning from‚Ä¶ RT @sedielem: Apparently @kaggle have been asking high-ranking contestants not to blog/share their solution. Very disappointing! https://t.‚Ä¶ RT @ThePracticalDev: When you write a Python wrapper for Hadoop https://t.co/H0viqyl1ee RT @icmlconf: #icml2016 videos are now available (5 plenary talks &amp; 9 tutorials).  https://t.co/TtNgg9YnKB RT @dublab: FROM THE VAULTS: Amon Tobin - Live D&amp;B Set at Konkrete Jungle ... - https://t.co/9UcdhcXnx4 @amontobinhq @ninjatune https://t.c‚Ä¶ RT @fchollet: Our collection of models with pre-trained weights is now baked directly into Keras, as the Applications module: https://t.co/‚Ä¶ RT @gvanrossum: We've released mypy 0.4.4: https://t.co/beNquklP4b (adding e.g. async/await support and NewType()!) RT @OfirPress: How to improve language models by building on a footnote from a 2014 paper by @yoavgo and @omerlevy_  https://t.co/9bsEAGTBN‚Ä¶ RT @samim: Sneak Peak of very cool new generative music ideas/results by @kastnerkyle  https://t.co/JOXsHwgs4f RT @sedielem: tl;dr: connect every CNN layer to every other layer. Simple but effective idea, well-written paper. Worth a read! https://t.c‚Ä¶ RT @betatim: #euroscipy ‚ö°Ô∏è-talks later today! Come and listen to me tell you about https://t.co/iH7ZnbQCKI pretty pictures and black-box op‚Ä¶ @alexjc the quality is so good it suspiciously looks like overfitting. RT @soumithchintala: We've released code for DeepMask, SharpMask and MultiPathNet. Object segmentation goodies :)
https://t.co/ijpGq3pf5t h‚Ä¶ @dwf you can just use the subset useful for papers: headers, paragraph, figures, latex math, bibliography and that's it. RT @hugo_larochelle: Also, some of the talks don't yet have the synced slides, but that's coming soon. https://t.co/FCqXONX9mB @dwf I wish arxiv papers would be written in markdown to get both a latex generated pdf and a mobile friendly html version. RT @abhi1thakur: You won't believe this one simple trick to detecting click bait! The results will shock you.... https://t.co/ETdxEpAIxN RT @sdouche: Huawei Launches a Kubernetes-based Container Engine #Kubernetes - https://t.co/HwYPc2OdUZ @fchollet mnist ;) RT @amuellerml: In case anyone else is writing an O'Reilly book with Jupyter: I added an asciidoc output format: https://t.co/kE7rEmiwvG Fe‚Ä¶ RT @jeremyphoward: JIT native code generation for TensorFlow computation graphs using Python and LLVM  https://t.co/zSWUUcXetK RT @NipsConference: List of NIPS 2016 accepted papers is available at https://t.co/4pZl59ddPe RT @petewarden: HP Labs have open-sourced OVL, a library for writing fast, pure Python custom TensorFlow ops: https://t.co/eIlB9SUR9n - gre‚Ä¶ RT @DeepSpiker: Summary of Stochastic Backpropagation through Mixture Density Distributions https://t.co/4gXTJSHSFT #shortscience RT @deliprao: So, are Synthetic Gradients cool or Meh? You decide https://t.co/SKqdxH5CKY https://t.co/xajmVQQO0s @foob @VictorStinner I read it but what is the impact of that change for the end user? @nikkostrom for distributed training of large models? Or for something else? @VictorStinner what is this all about? RT @dribnet: experiments decoupling heavily correlated attributes with new debiasing routine. here: x=smiling, y=mouth open https://t.co/5u‚Ä¶ RT @Miles_Brundage: "Mollifying Networks," Gulcehre et al.: https://t.co/ZPhYUSsTjZ https://t.co/itVAgKQRIP RT @mcmc_stan: Paris: 3-day short course w/ focus on PKPD models. 19-22 Sept. https://t.co/v7Wc5duXct
Taught by @betanalpha, @djsyclik, and‚Ä¶ @lothiraldan do you know https://t.co/nQjVgNrDDq ? @lothiraldan this issue has disappeared in python 3, no? RT @alexjc: As @sedielem points out on Reddit, there are missing references to related architectures: https://t.co/7CfWS9B8Bn https://t.co/‚Ä¶ @ocefpaf @sgillies building the conda system was heroic as well. RT @soumithchintala: "Any state-of-the-art neural network trains in 4 days. Improve training speed 10x, and it still trains in 4 days :/ "-‚Ä¶ RT @alexjc: Factorized Convolutional Neural Networks https://t.co/RCicRmdL3V New layer representation, reduces compute by 3.5x. https://t.c‚Ä¶ @jakevdp @GaelVaroquaux you can pip install pyqt5 manylinux wheels in venvs and matplotlib 2 beta now come as wheels thanks to Matthew Brett RT @tarantulae: It seems that we are approaching (or already there) to the end of the fine-tuning era on Deep Learning. https://t.co/aPiDGo‚Ä¶ RT @karpathy: Google Brain AMA on Reddit https://t.co/fEdStO3uEF a lot of good reading by a lot of awesome people RT @shakir_za: I'll be giving a tutorial on variational inference with @davidblei and Rajesh Ranganath at #NIPS2016. Will be fun! https://t‚Ä¶ RT @mblondel_ml: Preprint of our #NIPS2016 paper on higher-order factorization machines https://t.co/rtXR9lVzwN RT @mat_kelcey: cartpole++ https://t.co/lf6AlG3W6F ( tinkering with a 3d cartpole gym env with bullet physics &amp; a dqn trained with keras-rl‚Ä¶ @CharlesOllion @alexjc nice! RT @rsalakhu: My slides on Learning Deep Generative Models from the Deep Learning Summer School in Montreal:

https://t.co/6GoJETQJir RT @xamat: What is the advantage of generative adversarial networks compared with other generative models? by Ian Goodfellow https://t.co/S‚Ä¶ @sirotenko_m @NandoDF fractalnets have at least one non linearity in each subpath and no identity mappings. Center loss: A Discriminative Feature Learning Approach for Deep Face Recognition
https://t.co/k5JOk1ismp https://t.co/xOWucIDLrK RT @jakevdp: To those who's been waiting on this: sorry for the long delay... I'm delivering the final manuscript this week! https://t.co/R‚Ä¶ RT @elprans: Python 3 on Google App Engine now in beta https://t.co/rwTTu78SNz RT @ajlavin: Here is the simple python script I use to generate Winograd convolution algorithms for convnets: https://t.co/XW5IWsvmcI @kastnerkyle it's fun to read the last sentence of section 12 in light of the Residual Network of Residual Network paper :) RT @kastnerkyle: Vapnik's Principles of Risk Minimization for Learning Theory seems timeless (except the zipcode recognition :) ) https://t‚Ä¶ @aronchick @vambenepe then add new node to the group: they show up as preemptible and it seems to work. 2/2 @aronchick @vambenepe thanks, I tried again: create GKE cluster then create a template for its node group with the preempt flag on 1/2 New SOTA on CIFAR-10: 3.77% test error with Residual Networks of (Wide) Residual Networks and Stochastic Depth: https://t.co/faLDqpBkkX RT @honnibal: displaCy, the spaCy parse visualizer, is now open-source üéâ
https://t.co/1fAV0MSGcV @aronchick @vambenepe thanks. I think I tried that a few months ago and cound not get it to work but cannot remember the details. RT @nervanasys: Breaking news! #Nervana is planning to join #Intel!! Read the full story in this post by our CEO: https://t.co/3gC4l4Pvof @‚Ä¶ RT @vambenepe: "80% cheaper than the equivalent, non-preemptible instance, with no bidding or guesswork." https://t.co/WJUsw9NuPR https://t‚Ä¶ @vambenepe nice! Is there a way to provision preemptible nodes for a GKE cluster? RT @soumithchintala: Superb tutorial on adversarial nets by @brandondamos
Bonus section: thoughts on Torch vs TF
https://t.co/uwG8l2qytd ht‚Ä¶ RT @Maciej_Kula: Published a new version of Rustlearn, with factorization machines and parallel model fitting! @rustlang https://t.co/UDlwQ‚Ä¶ RT @betatim: Have you been wondering what the heck these plots show? Wonder no more: https://t.co/n9haVApbIG visualising results! https://t‚Ä¶ RT @mat_kelcey: "A Neural Knowledge Language Model" https://t.co/9B85jbnqit RT @nicolastorzec: WikiReading: large-scale NLU task &amp; public dataset to predict Wikidata property values from Wikipedia articles' text htt‚Ä¶ RT @karpathy: Read Google Brain WikiReading paper (https://t.co/n6s5kq0i5J); nice read, took some notes, might as well post them: https://t‚Ä¶ RT @plamere: NLP is hard - ‚ÄúI‚Äôm a huge metal fan‚Äù - #ismir2016 #nlptutorial https://t.co/ARoR8dlkmk RT @shakir_za: My slides on Building Machines that Imagine and Reason. On state-of-art generative models. https://t.co/3C5zAJpnWp https://t‚Ä¶ RT @hugo_larochelle: And follow @iclr2017 for future updates on ICLR! https://t.co/6U7itLyjFk @lelayf trying it now. It seems to do a good job for me as well. It this based on what @sedielem did in the past? RT @soumithchintala: Microsoft Azure offers GPU boxes. $1845 / month for 4-GPU M60 box at "preview discount" pricing :D
https://t.co/A21RIY‚Ä¶ @petewarden BTW what changes would be required in tensorflow to get fast inference with quantized TF models on the new titan X? RT @haldaume3: New #nlproc/#MachineLearning blog post "Fast &amp; easy baseline text categorization with vw" https://t.co/WC4pkoaUTX RT @alxndrkalinin: #ICLR2017 is up: 5th International Conference on Learning Representations - in France in April! Deadline Nov, 4th https:‚Ä¶ @dohmatobelvis what's FEP? RT @arnaudbertrand_: Hey, @TensorFlo r0.10 is out! https://t.co/aiDznNW6wI RT @EGouillart: Less than 3 weeks left before @EuroSciPy in Erlangen! Still time to register for tutorials and conference on https://t.co/h‚Ä¶ RT @soumithchintala: Code for FastText is released and here. Superfast text classification. VowpalWobbit can also do most of this FYI. http‚Ä¶ @deliprao I would have thought the output is click vs not click. And inputs are user features + item features. @deliprao it sounds useful to build recsys: https://t.co/YrE2DF2wDA I have not tried myself though. @deliprao have had a look at factorization machines and variants like lightfm? And google's "wide and deep learning" models? RT @fchollet: I made available VGG16, VGG19, ResNet50: Keras code +ImageNet weights for both TF and Theano. https://t.co/WcpBRT87Ww RT @fchollet: You can now literally do:
```
model = VGG16(weights='imagenet')
preds = model.predict(imgs)
print(decode_predictions(preds))‚Ä¶ RT @Deep_Hub: Keras based self-driving car, 7.25 hrs of driving data
https://t.co/lEQ6VfBrs6 by @edersantana et al. @comma_ai https://t.co/‚Ä¶ RT @jseabold: I just donated to PyPI. I've also studied behavorial economics. Act accordingly. https://t.co/NJw7RPx0bc #Python #pydata RT @cangermueller: Impressive image inpainting results with DCGANs: https://t.co/fI1jm2x2wI https://t.co/7craJimCXx RT @stanfordnlp: At #acl2016berlin‚ÄîGetting the best of both worlds: Efficient hybrid word-character models for neural MT by @lmthang https:‚Ä¶ RT @fchollet: Keras-RL: a new library to do reinforcement learning in Keras. https://t.co/19lklht9zm RT @turbodbc: Native #numpy support has landed in turbodbc 0.4.0. Don't wait minutes for results, just blink ;-). https://t.co/EbOGSC2e4w #‚Ä¶ @esc___ @TensorFlo that eould require to have the cuda / gpu depency as a runtime dep rather than a build time dependency. RT @glouppe: Want a scatter matrix of the 2D partial dependence of your objective function? @betatim made it for you in `skopt`! https://t.‚Ä¶ RT @seb_ruder: More highlights of #icml2016 with a focus on RL and #deeplearning https://t.co/EWdUQm8sdX #MachineLearning @mrocklin @shoyer @jiminy_crist there is already some dask-based data loader in the tf code base although i have not played with it. RT @googleresearch: ML models with both memorization and generalization with Wide &amp; Deep Learning in #TensorFlow https://t.co/dAPlvkfegP ht‚Ä¶ RT @twiecki: Great new additions to #PyMC3 by Taku: Variational GMM https://t.co/fBN6dh7c5U Autoencoding Variational LDA https://t.co/u9L77‚Ä¶ RT @TorchML: Language modeling a billion words! using Noise Contrastive Estimation and multiple GPUs
https://t.co/QUFrC1NPD2 https://t.co/x‚Ä¶ RT @soumithchintala: Fast smoke / fluid simulation using ConvNets by Jono Tompson. Next: graphics engine=convnet https://t.co/drtI7KTDON ht‚Ä¶ RT @EGouillart: Early bird registration of @EuroSciPy extended until next Wednesday! Check the awesome program of tutorials &amp; talks https:/‚Ä¶ RT @jnuneziglesias: Learn image processing in Python with @stefanvdwalt's and my #SciPy2016 #skimage tutorial:

https://t.co/y83paA9tfE RT @jtaylor108: posted the numpy vector math branch: https://t.co/JL2l3F6MnP RT @planetpython: Stefan Behnel: Cython for async networking https://t.co/rhddsf5DIP RT @ankurhandos: Amazing, within a day we have torch, tensorflow and keras implementations of layer normalisation... https://t.co/EBD6Jhl1S2 RT @Udibr: Keras GRU with Layer Normalization https://t.co/dfdyV7agaT @petewarden those are TOP/s ;) RT @petewarden: The new TitanX will give 40 TFLOP/s when running int8: https://t.co/TmrNh2Yrwp - Can't wait to try quantized graphs on this! @arnicas @amuellerml @GaelVaroquaux that could be tackled by a hierarchical nmf of  some sort ? ;) RT @soumithchintala: Still no HBM2 memory, all being reserved for P100 looks like it...
So memory bandwidth of 320 GB/s instead of 1TB/s. h‚Ä¶ RT @alexjc: An image query engine that morphs outputs to match input: https://t.co/8pDR9DZbki "Transfiguring Portraits" [PDF] https://t.co/‚Ä¶ @GaelVaroquaux nice topics: it gives a very good overview of the gist of the conference &amp; the ability to drill down to individual talks. RT @GaelVaroquaux: The topic extraction on #EuroPython
https://t.co/t0RhJWKTxm
will be explained in my keynote this morning to show what #p‚Ä¶ RT @glouppe: TIL Mondrian process := distribution over binary partitions. Name due to the Dutch artist Piet Mondriaan who painted https://t‚Ä¶ @fulhack supervised and unsupervised models are not necessarily probabilistic models by default. RT @sdouche: rkt 1.10.0 released (with kvm hypervisors support!) #rkt #coreos - https://t.co/kcfDmEr6WE RT @VictorStinner: Nice usage of the new Python 3.5 "async with": asynchronous database transactions! by @1st1 #EuroPython https://t.co/Q9R‚Ä¶ RT @mblondel_ml: Creating a world without disease using three pillars: privacy, deep learning and causality TEDx talk by Max Welling https:‚Ä¶ RT @reddit_ml: High Quality, High Performance Clustering with HDBSCAN https://t.co/2glSXoaTf0 RT @fchollet: My paper on label embeddings is on Arxiv. Many Nvidia K80s worked hard to bring you these numbers https://t.co/D2LL8HCAxa @mblondel_ml @alexip @mrocklin yet another theano/tensorflow :) RT @alexip: Dask Parallel and Distributed Computing | SciPy 2016 | Matthew Rocklin
https://t.co/nOhqMgAAGN RT @jiminy_crist: Further experiments parallelizing scikit-learn with dask - common data parallelism patterns. https://t.co/xFTVjHZxXY http‚Ä¶ RT @rasbt: For those who asked about the vids: @amuellerml &amp; my scikit-learn #ML tutorial at #SciPy2016 is now online :) https://t.co/s7l1s‚Ä¶ RT @VictorStinner: #asyncpg: new PostgreSQL driver for #asyncio faster than Go &amp; node.js, pure Python faster than C (libpg)! by @1st1 https‚Ä¶ RT @glouppe: Bayesian optimisation for tuning scikit-learn hyper-parameters https://t.co/gocziW9ZNX Feedback welcome before the 1st release‚Ä¶ RT @fulhack: Source code for "Visualizing Large-scale and High-dimensional Data" ‚Äì claims to be substantially better than t-SNE: https://t.‚Ä¶ RT @twiecki: #numba 0.27 is out https://t.co/Wxn7C1hXtB Support for more numpy linear algebra routines like leastsq, solve and pinv. @Conti‚Ä¶ RT @karpathy: 10 Papers from ICML and CVPR https://t.co/OGz0sGbi3M nice roundup! @GaelVaroquaux @jnuneziglesias @SciPyConf what about the usual https://t.co/zodHjt4xCD RT @brandondamos: Python script for illustrating CNNs: https://t.co/pfj01GKbwb https://t.co/818IB2jA5o @jakevdp @pwang @amuellerml @GaelVaroquaux and the target training data is also part of the training data. @jakevdp @pwang @amuellerml @GaelVaroquaux also as others said "labels" is misleading when talking about a continuous valued target variable RT @jrecursive: PostgreSQL 9.6: Parallel Sequential Scan - https://t.co/RL7MdhlHkv @amuellerml @GaelVaroquaux @jakevdp assuming some vague recollections of high school calculus and the canonical y = f(x) formula. @amuellerml @GaelVaroquaux @jakevdp I think the math concepts of funcs &amp; vars are close enough to programming funcs &amp; vars to be useful @GaelVaroquaux @amuellerml @jakevdp +1. And even if you decide to not use then it's still good to explain them as they are very often used. @amuellerml @GaelVaroquaux @jakevdp if you go for X &amp; y I think the origin should be explained: given X &amp; y can we learn f such  as y = f(X) @amuellerml @GaelVaroquaux @jakevdp I would use target instead of labels at least in a regression context. RT @newsycombinator: Release of IPython 5.0 https://t.co/7G7dzjLKKB RT @kubernetesio: Updates to Performance and Scalability in #Kubernetes 1.3 -- 2000 node / 60000 pod cluster https://t.co/UDlZ6drSfb #k8s @syhw have the author tried to do a multitask fastText + CBoW or skipgram loss to benefit from unlabeled text on top of the labeled docs? fastText by Joulin et al: supervised word2vec, hashing trick on ngrams and hsoftmax for scalable text classification https://t.co/BmMiC7omCe RT @kubernetesio: Kubernetes 1.3 is here! Introducing cross-cluster services, stateful workloads and Minikube for your laptop https://t.co/‚Ä¶ @pchapuis @deltheil @dfbrule congrats! RT @pchapuis: What Vision is: when I joined Moodstocks in 2010, @deltheil and @dfbrule told me exactly this. https://t.co/UKC2ZNdWsQ RT @TorchML: AMD's funded port of Torch and NN to AMD's fast HCC framework. Use high performance Torch seamlessly on AMD GPUs https://t.co/‚Ä¶ RT @PaulMineiro: Machined Learnings: ICML 2016 Thoughts https://t.co/IXBxdnmWob RT @beenwrekt: Learning to learn to love random search (another joint post with @kgjamieson). 

https://t.co/j7yTCYP6Xc RT @beenwrekt: Bayesian Optimization and other bad ideas for tuning hyperparameters.  https://t.co/0nr0TQgtdh RT @DeepSpiker: Our work on learning 3D Structure from (2/3)D Images https://t.co/KG20rjlUqx https://t.co/dj2dTMhV4z
#generative #3D https:‚Ä¶ RT @tqchenml: XGBoost now comes with dropout support(DART) by @marugari2  https://t.co/J8A9ED4Rmo @soumithchintala I don't know how cffi and ctypes deal with multiple dispatch. @soumithchintala if the args are numpy arrays with float64 / 32 dtypes I would use cython fused types. @soumithchintala cffi and ctypes are great when the wrapped function is cpu intensive / called out of any high cardinality loop. RT @DeepMindAI: PixelCNN 2.0: New state of the art generative model for conditional natural image synthesis https://t.co/2agVAzdsbB https:/‚Ä¶ RT @TorchML: cltorch: a Hardware-Agnostic Backend for the Torch Deep Neural Network Library, Based on OpenCL
https://t.co/b7Q0Ttfp77 RT @rgaidot: Interesting! OpenAI fist research results: Generative Models https://t.co/ps8mCdBdXt #OpenAI RT @amuellerml: Introduction to Machine Learning with Python with @sarah_guido is out as early (unedited) release! https://t.co/g8YEa9fZzp‚Ä¶ @ReScienceEds good idea: I forwarded your tweet to the "replicator" by email. RT @shakir_za: The third way (ala Lengyel and Dayan) for control realised; from DeepMind colleagues: Model-Free Episodic Control https://t.‚Ä¶ The ResNet and Wide-ResNet results on CIFAR-10 have been independently replicated with theano and lasagne: https://t.co/zXzClrKf1x RT @demishassabis: We're putting the final touches to a new version of #Labyrinth which we'll be open sourcing soon for everyone to use htt‚Ä¶ @HerbieLewis integer coding is empirically much faster when the cardinality of the categorical feature is large. RT @samim: New @DeepMindAI Videos: Asynchronous Methods for Reinforcement Learning: TORCS: https://t.co/BZQ2D2Zq4b &amp; Labyrinth: https://t.c‚Ä¶ RT @hackernewsbot: Second Gravitational Wave Detected at LIGO... https://t.co/xVMFcuD7Zd Here is the notebook for the quick practical / introductory talk on Gradient Boosted Trees I gave at #pydataparis16: https://t.co/DeC3TzAxqI RT @pyconfr: @pyconfr 2016 aura lieu √† Rennes du 13 au 16 Octobre ! Toutes les infos sur https://t.co/TFZgKDO3Aq @esc___ pip install pyqt5 should work as well and is an optional dependency for matplotlib. @esc___ It's available for the beta version of the upcoming 2.0 release, you can test it with: pip install -U --pre matplotlib RT @alexandreabadie: Here are the slides I presented about Joblib at @PyDataParis: https://t.co/IojNUnaYEe
cc @GaelVaroquaux @ogrisel @SEDS‚Ä¶ RT @xhochy: Slides for my #PyDataParis talk about @ApacheArrow and @ApacheParquet https://t.co/6yJsTnnxrx @fchollet @smdiehl I agree with both of you at the same time. Watch the https://t.co/ZvYJNTbBl3 python odbc driver: it should soon feature direct columnar / numpy access to resultsets. RT @OriolVinyalsML: Excited to share my first DeepMind paper! Seq2Seq + Learning2Learn for One Shot Learning on ImageNet, Omniglot, &amp; LM ht‚Ä¶ RT @Smerity: In deep learning, architecture engineering is the new feature engineering
https://t.co/JrCxQB9qS6 RT @mrocklin: Dask 0.10.0 released

https://t.co/tKwA3wUIJ9 https://t.co/2x196048VS @esc___ answered. @esc___ https://t.co/B1c9Hb8CxP can catch many typos. @esc___ check the files listing on pypi. Mattew Brett maintains scripts to build manylinux wheels for many projects: https://t.co/GYg7XNQfFE @joanfihu @fchollet automation is capital intensive. The majority of people control very little capital. I am not sure how this will evolve. @fchollet productivity gains will not be redistributed by high &amp; regular wages and will go to automation owners instead. RT @mblondel_ml: Our #icml2016 paper on polynomial networks and factorization machines is now online https://t.co/N8ztuRMWvn RT @PyDataParis: 4 days to the D-Day #PyDataParis16 : attend to enjoy the program!  https://t.co/GHMM6QePxY https://t.co/T6KT6DjXoH RT @chrisemoody: "Adverserially Learned Inference" (https://t.co/S3R4QYmDLq) is wonderful &amp; has bizarre short story as appendix B! https://‚Ä¶ RT @OpenAI: We‚Äôve just released OpenAI Requests for Research, a living collection of deep learning research problems. https://t.co/zryaeiKf‚Ä¶ Next week I will talk about distributed analytics on Tue. and gradient boosting on Wed. at PyData Paris: https://t.co/0SAPDFvuTF RT @mrocklin: Dask is considering dropping Python 2.6 support.  If anyone actively needs this then they should speak up here: https://t.co/‚Ä¶ @hugo_larochelle @kastnerkyle @fchollet @karpathy thanks I missed that. RT @shakir_za: Getting better at Montezuma's Revenge, now at 15 rooms: 'Unifying Count-Based Exploration and Intrinsic Motivation' https://‚Ä¶ @hugo_larochelle @kastnerkyle @fchollet @karpathy Curious if sampling test preds from implicit ensemble + voting / averaging would help. @hugo_larochelle @kastnerkyle @fchollet @karpathy it's not clear what they do at test time. I assume they fix the t-1 masks to zeros. RT @karpathy: Zoneout for regularizing RNNs https://t.co/GVQ1PGHJGE nice idea and fun title! except ~1 line of code difference and 11 autho‚Ä¶ RT @deliprao: #Tensorflow 9 RC is coming along well. With iOS build support and improvement for fp16 ops. https://t.co/6c27tLJ6hZ https://t‚Ä¶ RT @PyDataParis: One week left before #PyDataParis16 - discover the detailed schedule starting with Day 1 - https://t.co/goXt6w3T0A https:/‚Ä¶ RT @hugo_larochelle: My (short) notes on Swapout: Learning an ensemble of deep architectures: https://t.co/CizqExyUsa . Dropout for ResNets‚Ä¶ RT @dstufft: Try uploading to PyPI via Warehouse for a less broken experience - https://t.co/yvFyYE4vnG RT @alexip: This title is a stupid click bait but the article on reinforcement learning is interesting  https://t.co/4uBgfyeTUN @egrefen @lmthang @elonmusk @Tom_Stears I really have no idea how the solomonoff prior would infom us on this hypothesis ;) RT @alexey_r: Where does the Sigmoid in Logistic Regression come from? https://t.co/YDiE3mbHGw RT @StuartMumford: @ogrisel Along the same lines: https://t.co/dMlWE2vCQn A bit more scientific focused (dev versions of Numpy etc) Example CI configuration to run tests on Linux, OSX and Windows on a single github repo: https://t.co/y8AYcvxh5K RT @samim: "Convolution by Evolution": DeepMind paper on mixing DL and evolutionary algorithms: 
https://t.co/a35Ho34TbD https://t.co/IlBQA‚Ä¶ Congrats on the beta release @IPythonDev! The multi-line editing and progressive syntax highlighting are very nice features! RT @Mbussonn: @Mbussonn Note, it works great with matplotlib 2.0b1 ! you should try it as well ! cc @tacaswell RT @Mbussonn: Hey #IPython users, 5.0b1 is out , please give us feedback. More info: https://t.co/EiMg6kPxLJ `pip install ipython --upgrade‚Ä¶ Tombone's Computer Vision Blog: Deep Learning Trends @ ICLR 2016 https://t.co/ypb0wAZeMn RT @randal_olson: .@gvanrossum announced (optional) static typing in #Python. #PyCon2016 #programming

Slides: https://t.co/wQv2e69ITB http‚Ä¶ RT @chris_n_vinegar: Uncertainty-GBM: Sklearn implementation of GBM to predict mu(X) and std(X) on heteroscedastic data via /r/MachineL‚Ä¶ ht‚Ä¶ @minrk too bad I rewrote the git history that led to the first working version of that run_with_env.cmd script. Pure blind trial and error. RT @J_: The slides of my talk at Strata London 2016:
"The future of column oriented data processing with Arrow and Parquet"
https://t.co/PT‚Ä¶ RT @deliprao: One trick to bump accuracies in #deeplearning -- make your gradients more stochastic. https://t.co/wdW9raa7WO RT @wesmckinn: Example git log, before and after changing to linear history, no merge commits, and atomic change sets https://t.co/vG44fHXZ‚Ä¶ RT @octonion: AMD's RX 480 does 5 TFLOPS to 6 TFLOPS (overclocked) for $200. Downside for deep learning is only 4 GB of RAM, but an 8 GB ve‚Ä¶ RT @quantopian: "Bayesian Deep Learning" blog post out by @twiecki: https://t.co/7udzC13KU1 https://t.co/XENDMHwUro RT @petewarden: Impressive synthesized images in this new paper: https://t.co/t5qfJVCDb3 https://t.co/cOAP5xekaA @esc___ @twentybn looking foward to seeing what you will build ;) @SpectralFilter some mesos config for distributed would be great too. RT @karpathy: New blog post: "Deep Reinforcement Learning: Pong from Pixels" https://t.co/4WIHOGwr3Z on policy gradients https://t.co/mFsiK‚Ä¶ RT @pydataberlin: Keynotes by @ogrisel @b0rk and @wesmckinn from #pydatabln are online at https://t.co/dKa7ZN84WQ. Other videos from conf s‚Ä¶ RT @jakevdp: Most important matplotlib release in years: color-maps and style defaults are completely revamped!

https://t.co/OkPt0AyuBn @bortzmeyer which in particular? I barely ever encounter python 2 only libs those days. @bortzmeyer and the fact that supporting python 2.7 prevent us to use python 3 syntax support for asyncio, type hints, @ for matrix mul... @bortzmeyer you also have to consider the cumulated effort library maintainers spend to support compat for python 2.7 @bortzmeyer a lot of the work can be automated with https://t.co/MjlEG0uA6R RT @martinenserink: The Dutch EU presidency got what it wanted: A 2020 target for full Open Access to scientific papers.  https://t.co/3Kd8‚Ä¶ RT @cangermueller: #DeepCpG: accurate prediction of DNA methylation using #deeplearning: https://t.co/UOv55HqC6F https://t.co/rSinWeYXL1 RT @sedielem: Looks like GTX 1080 does not have fast fp16 :( disappointed, but not surprised... I guess I won't upgrade just yet! https://t‚Ä¶ RT @DeepSpiker: Could a neuroscientist understand a microprocessor? https://t.co/o09MyZLdEI RT @soumithchintala: "Discovering Causal Signals in Images". Given an image, can you tell which objects cause others? Our new paper. wdyt?h‚Ä¶ RT @PyDataParis: We have a program! https://t.co/goXt6w3T0A (we‚Äôll send individual notices to the speakers later today or tomorrow). RT @hugo_larochelle: The #ICLR2016 talks are now on https://t.co/uSIEIbRM1u: https://t.co/hftgdlwxK7
Enjoy! :-) RT @Maciej_Kula: LightFM 1.9 is out, with an evaluation module, built-in example datasets, and fancy new docs at https://t.co/lNi07d4Vs8. G‚Ä¶ RT @abursuc: Terrapattern visual search engine for satellite imagery https://t.co/bbGzIMAmMD Nice &amp; powerful: ResNet trained w/ OSM images‚Ä¶ RT @kastnerkyle: I am soliciting submissions for https://t.co/M19Pk78XxH - hilarious but not so bad samples also supported RT @kubernetesio: Hypernetes: bringing security and multi-tenancy to Kubernetes https://t.co/TNyCuSMHjK https://t.co/cuwgtBIgk2 RT @fulhack: Deep Learning without Poor Local Minima https://t.co/2gQ6OAXQ5p RT @amuellerml: Ml/stats question: would you call leaking test set information into your model "contamination"? Are there better words (or‚Ä¶ RT @hugo_larochelle: Switching to https://t.co/eNZGGnK9Bv for reading notes. It's collaborative, can upvote notes and supports latex. Hope‚Ä¶ RT @hugo_larochelle: My notes on Attend, Infer, Repeat: Fast Scene Understanding with Generative Models: https://t.co/2M2Lf5OTZ5
A really n‚Ä¶ RT @TorchML: Training code for "Wider Residual Nets", which establish new state of the art results on CIFAR-10/100 dataset: https://t.co/rU‚Ä¶ RT @navyatechnology: ¬´ What ARMA has accomplished is quite a feat. ¬ª
Discover the article about #NAVYAARMA of @cleantechnica 
#driverless h‚Ä¶ @honnibal yes, I just wanted to let you know that this exists in case your users face the issue because it really hard to diagnose. RT @johnplattml: Residual networks aren't deep, but are exponentially large ensembles https://t.co/S86O64DqYS from @SergeBelongie group #De‚Ä¶ Numpy script and bench results to highlight memory-bottlenecked workloads with Intel CPUs: https://t.co/j2Axe2HcwE @honnibal here is a script that reproduces the OpenMP + multiprocessing crash I mentioned yesterday: https://t.co/6E89O7vj2P RT @EGouillart: The CFP for @Euroscipy 2016 is open, and we're welcoming tutorials proposals too! https://t.co/hR6oEmBSiF 
also see https:/‚Ä¶ RT @octonion: The net result is that the resulting models will all tend to be very similar. Thus in practice logit vs probit link doesn't r‚Ä¶ RT @octonion: Really all you need is any reasonably well-behaved strictly increasing bijection from (0,1) to (-infinity,+infinity). RT @octonion: There are some suggested motivations, but these are post-facto rationalizations. Logit and probit really aren't that special. @yarikoptic @pydataberlin it was video recorded. It should get online soon. @aterrel also maybe power 8/9 will save scale up ;) @aterrel this script makes it possible to highlight the pbm: https://t.co/j2Axe2HcwE @aterrel I agree it depends on the algorithmic intensity of the workload / roofline analysis. But did not have the time to intro concepts. RT @pydataberlin: #pydatabln finishes in style with craft ales at Kulturbrauerei. Thank you everyone for a great conference! https://t.co/h‚Ä¶ RT @hynek: Conditional Python Dependencies: https://t.co/xWgKn3gLAE

How to encode wheel dependencies based on system markers w/o going up‚Ä¶ RT @BlueYonderTech: . @ApacheParquet Support for #python is coming. Thanks to a joint effort of @xhochy (@BYAnalytics_en) and @wesmckinn ht‚Ä¶ This is bad... https://t.co/F8jMfrUwKp @DataKyle thanks :) @Swayson @ianozsvald @mrocklin probably yes. @beaucronin I cannot predict the future but containers and kubernetes are definitely an improvement. @jseabold thanks for your input ;) RT @GaelVaroquaux: New low-overhead persistence for Python big data in joblib:
benchmarks and implementation, with @alexandreabadie: 
https‚Ä¶ @esvhd @ianozsvald here they are https://t.co/GqrTvfvyNQ sorry for the delay. @ogrisel the companion repo also has sample docker-compose and kubernetes conf to deploy a dask/distributed cluster: https://t.co/OAmjUvA8XO Slides for my presentation on predictive modeling ecosystem at @pydataberlin: https://t.co/sclUvg3U7w / notebook: https://t.co/OAmjUvA8XO @pydatalondon sure  2mins, I need to configure github pages on that repo :) RT @heinrichhartman: Flame graphs arrive in the python https://t.co/WTR6HEdf9C: Learning from the masters (@brendangregg). https://t.co/9YD‚Ä¶ @soodoku @wesmckinn @IbisData @cpcloudy no, I tweeted from my phone... @wesmckinn @IbisData @cpcloudy unintended typo in redshift, sorry... @wesmckinn @IbisData @cpcloudy great news, does ibis work with redshit, greenplum and citusdb then? RT @wesmckinn: Just released version 0.8 of @IbisData, with initial PostgreSQL support from @cpcloudy. https://t.co/NSBbHEuZL5 #pydata RT @fchollet: Google has started using Tensor Processing Units (TPUs) for Deep Learning models, faster &amp; more efficient than GPUs: https://‚Ä¶ @petewarden will you publish more details on its specs / design? RT @hadleywickham: And feather is now on CRAN: https://t.co/eYk6QOoByz üéâ #rstats RT @fchollet: An implementation of Reinforcement Learning agents in Keras with @OpenAI Gym: https://t.co/UP6S3kEPMF by @oshtim https://t.co‚Ä¶ @beaucronin I am working on containerizing cluster deployments with docker compose or kubernetes: https://t.co/OAmjUvA8XO @Springcoil @beaucronin I like the design. @Springcoil @beaucronin I am playing with it but not a direct user myself. RT @PyDataParis: The CFP for PyData Paris has been extended by 5 days. https://t.co/5Cl6DLATBX RT @confshare: We are excited to launch the ConferenceShare service for #ICML2016, #ACL2016Berlin, COLT 2016, and #IJCAI2016: https://t.co/‚Ä¶ RT @bigdata: .@ogrisel @mikiobraun @mounialalmas @IraIracohen @datoinc &amp; more üîú #StrataHadoop Londonüá¨üáß  https://t.co/TqmAQWSPkC https://t.c‚Ä¶ RT @TerryTangYuan: #HDF5 and #DASK are both supported in #skflow #TensorFlow! See examples in https://t.co/Pue4jHnnEH RT @fchollet: "we study the cognitive plausibility of LSTM.." At least when when SVMs were everywhere, people didn't obsess so much about b‚Ä¶ RT @brinkar: Google releases super accurate #TensorFlow POS tagger as open source, names English version... Parsey McParseface:
https://t.c‚Ä¶ RT @jakevdp: (Also a hard-to-ignore benefit of pytest over nose: nose no longer has active maintainers: https://t.co/bIkCspwdP9)

https://t‚Ä¶ RT @alexip: Advanced course in Reinforcement Learning, PDFs, videos
by David Silver - Google DeepMind
https://t.co/SMJ2OwA5nN RT @PyDataParis: Tickets are now on sale for PyData Paris https://t.co/MXusRQaWWT RT @mrocklin: Dask 0.9.0 released: https://t.co/HUEQk3AMSC RT @tdhopper: Love data science https://t.co/RZ18jaOJRQ RT @GaelVaroquaux: Compressed, low-overhead, pickle for big data‚Ñ¢ persistence in Python just merged in joblib:
https://t.co/N8nDUsuIJx
Feed‚Ä¶ The CFP for PyData Paris in June is still open for a couple of days: https://t.co/47OoXUKuzU RT @ICIJorg: #PanamaPapers #OffshoreLeaks data licensed under Open Database License &amp; CC-BY-SA. Here's how to download the data: https://t.‚Ä¶ RT @svial: L'universit√© de Montr√©al annule son abonnement √† 2116 revues publi√©es chez Springer. https://t.co/VvKZC8qg1H Late 2015 / 2016 is really looking bad. It looks like a phase transition. https://t.co/ZFwdUCcCFU RT @glouppe: In today's arXiv gems: Visual RL-based agent for Doom https://t.co/meZ7luODsg RT @balazskegl: Drug classification from Raman spectra, @SaclayCDS, May 11 (tomorrow) @PROTO204
https://t.co/wphh59vKm9 https://t.co/JzNCFs‚Ä¶ RT @jakevdp: I just discovered the new rolling window API in Pandas 0.18. Very nice improvement! https://t.co/cAPxfhtZdx https://t.co/6VvfL‚Ä¶ RT @mxlearn: Deep Learning in the Cloud with NVIDIA DIGITS and Titan-X GPUs starting at $0.49 per hour https://t.co/zpbDtzRd0Q Large Margin Micro Blogging: https://t.co/gKrO53YDbG "This is clearly the future of machine learning." -- Ben Recht #twittersrazor RT @minrk: My JupyterHub tutorial at #PyDataLondon https://t.co/fKzLWMY7kx https://t.co/RgKKOBU5qO RT @tristanzajonc: Edward - Python fusion of Bayesian statistics and machine learning, deep learning, and probabilistic programming. https:‚Ä¶ RT @petewarden: SqueezeNet fits AlexNet-level CNN accuracy on image recognition into 470KB, over 500x smaller than the original! https://t.‚Ä¶ RT @hugo_larochelle: Our paper on Domain Adversarial Neural Nets (DANN) now officially published in JMLR: https://t.co/YhBgI0oejk @DRMacIver @westurner @jleedev @__mharrison__ I agree. I wrote a bug in the past that was caused by this reuse of id onrecently gc'ed vars. RT @jleedev: @__mharrison__ @DRMacIver Wow, it‚Äôs as simple as
&gt;&gt;&gt; id(object()) == id(object())
True
&gt;&gt;&gt; object() is object()
False RT @DRMacIver: Reminder: In Python id(a) == id(b) only implies that a is b if both a and b are still live. Otherwise ids may be reused. RT @petewarden: How to quantize neural networks in TensorFlow: https://t.co/qfn72LEdnm RT @YhatHQ: uvloop: Blazing fast Python networking | https://t.co/6WyodJYtlO https://t.co/UoANt2ynJb RT @VictorStinner: Python 3.6 gets a builtin memory debugger https://t.co/VEsYDwLxHg Detect buffer under/over-flow, fill memory with a byte‚Ä¶ RT @arthurmensch: Accepted paper at ICML : leveraging random subsampling and online learning for large scale matrix factorization https://t‚Ä¶ Deep Language Modeling for Question Answering using Keras https://t.co/WkJZNjvpAF covers Attentional RNNs and various similarity metrics. RT @matthieunapoli: When we open a pull request, we are also asking the maintainer to commit to maintaining our code. It's never just about‚Ä¶ RT @karpathy: I regret to inform that we were forced to take down CS231n videos due to legal concerns. Only 1/4 million views of society be‚Ä¶ RT @karpathy: Terrain-Adaptive Locomotion Skills Using Deep Reinforcement Learning https://t.co/Nnpc3z4VOX v nice learned motions for dogs/‚Ä¶ RT @Maciej_Kula: Put up a new example of a hybrid recommender system for the CrossValidated Q&amp;A site, uses a dev version of LightFM: https:‚Ä¶ RT @betatim: Freelance data science training, contracting and consulting. Day one. @wildtreetech RT @pydataberlin: Very happy with the #pydatabln schedule, chk it out. Keynotes by @wesmckinn @b0rk &amp; @ogrisel https://t.co/2wZdcL2sKk http‚Ä¶ RT @adrientetar: PyQt5 can now be installed with pip on all platforms! üëè @cosimolupo https://t.co/pJUiwwade4 @dnouri @id_wildflowers indeed @id_wildflowers https://t.co/5HmqRrRtJ9 @id_wildflowers https://t.co/P2LLZquysM @id_wildflowers https://t.co/PEDHOvnUJQ @id_wildflowers https://t.co/PkocZdvzG4 @id_wildflowers https://t.co/wOTWiUmUWo RT @kaggle: OpenAI Gym: a toolkit for standardizing reinforcement learning environment and evaluation from @open_ai https://t.co/VRCiNJap3B RT @jedisct1: Inside Capacitor, BigQuery‚Äôs next-generation columnar storage format https://t.co/hbZDMVFD00 @mfiguiere @samklr congrats indeed. RT @matsiyatzy: That's some crazy impressive convnet class visualizations from forthcoming work from @anh_ng8 and @jasonyo. https://t.co/XF‚Ä¶ RT @le_roux_nicolas: Great news for @open_ai . https://t.co/FNgPfEpKa0 RT @fpedregosa: My paper "Hyperparameter optimization with approximate gradient" is accepted for #icml2016 https://t.co/ifYmbPPeTX https://‚Ä¶ RT @DeepSpiker: Important step towards standardizing RL: Benchmarking Deep Reinforcement Learning for Continuous Control https://t.co/SIcEt‚Ä¶ @communicating why not just reuse one of eigen, dynd or armadillo? RT @alexjc: Second HD texture render (after many test runs); seems to fix artifacts from previous iteration. Slow but promising! https://t.‚Ä¶ RT @fchollet: I added a tutorial on using Keras as part of a TensorFlow workflow: https://t.co/reoniEndJl https://t.co/NbBAec4VEZ RT @npinto: GPUCC - An Open-Source GPGPU Compiler -- Seriously exciting! https://t.co/7IrR34jkfs RT @HNTweets: Google Open Sources GPUCC, LLVM-Based NVCC Competitor, Better Tensorflow Perf: https://t.co/6nVKxLqek7 Comments: https://t.co‚Ä¶ @glouppe have you tried 2 tanh units, one for cos, one for sin, then average: (arccos unit1 + arcsin unit2) / 2 to smooth boundary effect? RT @alexjc: First HD texture with new code. Currently needs 2x resolution then downsample to match photo quality. #NeuralDoodle https://t.c‚Ä¶ RT @xamat: Efficient implementation of matrix factorization for implicit data open sourced @Quora https://t.co/DFSIWVobZm #recsys #machinel‚Ä¶ @communicating for the bindings, have a look at cffi or Cython. I would recommend cython if you plan to use numpy arrays in the API. @communicating see also: https://t.co/HfusXkohCQ @communicating I wish I could use: advanced unpacking, raise from, type hints for better doc &amp; linting / completion in code editor... @communicating the python 2 &amp; 3 syntax is ver close to python 2 only syntax but prevent use of python 3 only cool new stuff RT @sdouche: PostgreSQL 9.6 with Parallel Query vs. TPC-H #PostgreSQL - https://t.co/yANYroGhJY RT @jakevdp: .@thefreemanlab: cloud services have the advantage over institutional clusters for democratization &amp; portability of science RT @hackernewsbot: CERN has released 300 terabytes of research data from LHC... https://t.co/Xy2SBj8Gtu RT @hdf5: Major new release HDF5 1.10.0 - Solutions to data challenges https://t.co/xa5xiyE34u https://t.co/yy4fQTHhmT @JoanieLemercier ca devrait te plaire : https://t.co/lMCKqHb8TI @wesmckinn @asmeurer @esc___ also the manylinux1 toolchain is too old for c++ 14 I think (G++ 4.8). @wesmckinn @asmeurer @esc___ I would not say fragile but problematic for security sensitive deps like openssl. True. @wesmckinn @asmeurer @esc___ at the moment yes. But auditwheel repair will be updated to support windows and delocate does the job for osx. @asmeurer @esc___ @wesmckinn python package dependencies can be embedded in wheels. @esc___ @wesmckinn @asmeurer no, not yet. But travis and appveyor work. The conda forge layer is nice but conda specific at the moment. @esc___ @wesmckinn @asmeurer but hopefully mingw will be updated to build binaries compatible with vs 2015 built python at some point. @esc___ @wesmckinn @asmeurer building scipy on windows for python 3.5 is still an open issue because of lack of fortran compiler though. @esc___ @wesmckinn @asmeurer pypi already exists and now accept binary wheels for linux, windows and osx. RT @karpathy: The code uses Torch/Lua. The release includes pretrained models, lot of docs, eval code, live webcam demo etc. Worked on rele‚Ä¶ RT @karpathy: Justin and I just released code for our CVPR2016 paper DenseCap: joint detection+captioning https://t.co/yZAemhubfx https://t‚Ä¶ @yoavgo @LeonDerczynski @soegaarducph @barbara_plank what is the prediction speed of your biLSTM tagger relative to the CRF or tnt taggers? RT @ianozsvald: Ubuntu 16.04 released with Python 3.5 as default: https://t.co/9EAExbN66Y @communicating not yet. But I would start a new personal project today I would probably do it Python 3 only. Ad-hoc computations and distributed random forests with Dask by @mrocklin: https://t.co/BlJGLnYxF4 blog post: https://t.co/RhAomjz2MB RT @rgaidot: Self-driving cars with rubber duckies
https://t.co/PviWh5h1K1 RT @x0rz: Create fully static, including rootfs embedded, binaries from container, pretty cool https://t.co/YEijFbcJDb https://t.co/qEB1fe6‚Ä¶ RT @fchollet: DQN with intrinsic motivation: https://t.co/6gQMwlZtXF v. important topic RT @yoavgo: .@LeonDerczynski @soegaarducph @barbara_plank one nice result in this work imo is that the biLSTM model *does not need* more da‚Ä¶ RT @LeonDerczynski: New SotA PoS tagger for many langs: https://t.co/AlZLfCWWHX #nlproc Great work @barbara_plank @soegaarducph @yoavgo --w‚Ä¶ @communicating I use python 3 but still write code that is also compatible with python 2.7. RT @amiconfusediam: Unsupervised learning by solving jigsaw puzzles. VERY cool results, 38% on Imagenet-1000. 
https://t.co/Ulkp6x1yxi http‚Ä¶ RT @amiconfusediam: @amiconfusediam The key seems to be to give spacing between the puzzle pieces -- otherwise the neuralnet can cheat a bi‚Ä¶ RT @benjguin: #Azure #Container Service is now generally available with Swarm CC: @docker https://t.co/HbHLm9Q7wt https://t.co/UJeQBk3nyD @drfeifei congrats @karpathy! RT @alexjc: These textures were generated overnight by a neural network with same parameters, from example images. #NeuralDoodle https://t.‚Ä¶ RT @kastnerkyle: After a (too long) blog hiatus, I just posted on "Bad Speech Synthesis" https://t.co/WvNDpQcZrf There are still seats available for the pre-dotscale @DataParis meetup: https://t.co/eNmwxq2WS9 (you can submit a lightning talk too). Practical Riemannian Neural Networks https://t.co/DT3bt0obX4 like adagrad / RMSprop but with affine invariance with medium scale benchmarks. Very intuitive video series by J. Gordon to discover Machine Learning w/ sklearn and tensorflow: https://t.co/Yb1Hfl7HAG (2 episodes so far) RT @hugo_larochelle: @hugo_larochelle Congratulations to Scott Reed and @NandoDF for their ICLR Best Paper Award for: https://t.co/gzYOLNb0‚Ä¶ RT @hugo_larochelle: @hugo_larochelle Congratulations also to Song Han, Huizi Mao and Bill Dally, for their ICLR Best Paper Award for: http‚Ä¶ @yoavgo gluten free RT @yoavgo: They market it as a programming book but its actually the missing Deep Learning guide. https://t.co/oakxhVd1jl @john_lam yes probably. @ogrisel the inline API viewer does not scale to the average docstring size of numpy / scikit-learn though. Our doc is too comprehensive :) VS Code 1.0 with the Python extension is a snappier alternative to Atom. It's quite easy to get the debugger working https://t.co/RnJxnEuPY7 RT @albertcardona: Our paper in @biorxivpreprint has &gt;1000 downloads and a public review by expert @pollyp1. Future of journals dubious
htt‚Ä¶ RT @abursuc: MS Sequential Image Narrative Dataset: a novel dataset for visual storytelling going beyond simple image captioning https://t.‚Ä¶ RT @alexjc: BetaGo: not quite as good as AlphaGo (yet) but open source! https://t.co/4jjPjBAd1q (Written with Python and Keras.) https://t.‚Ä¶ RT @amiconfusediam: My proposal for DeepMark, a new set of complete benchmarks for deep learning, join in! https://t.co/91rXrdtoo2 @tdhopper @GaelVaroquaux that does not look good though, you have a lot of iowait and your cpus are under-used. RT @almarklein: I wrote about an approach to wite Python 3 and still support @RipLegacyPython https://t.co/ZQH8lcFcVM RT @bigdata: LIME, an algorithm that can explain the predictions
of any classifier or regressor in a faithful way by @guestrin https://t.co‚Ä¶ Linux users: pip install --upgrade pip &amp;&amp; pip install numpy scipy scikit-learn  #manylinux @ogrisel related blog post by @shakir_za on the score function / log derivative trick:  https://t.co/FJqhmqOyqd Gradient Estimation Using Stochatic Computation Graphs by Schulman et al https://t.co/kJOK1m8bS8 nice wrapup on many probabilistic ML tricks RT @shakir_za: Thinking about Data-efficient Deep Learning? Contribute to the #ICML2016 Workshop on Data-efficient Machine Learning https:/‚Ä¶ RT @planetpython: Python Engineering at Microsoft: How to deal with the pain of ‚Äúunable to find vcvarsall.bat‚Äù https://t.co/W1TwaXdH1w Announcing TensorFlow 0.8 ‚Äì now with distributed computing support! https://t.co/zIiQhcKFyZ via @googleresearch OpenBLAS dev branch now under CI to run the numpy/scipy/sklearn tests on 7 Intel/AMD arch for Linux + Intel on OS X: https://t.co/lBZvRJTiA7 RT @shakir_za: List of #ICML2016 Tutorials  now online. https://t.co/hgUvTHO11i Do I need to use deep learning to build a product recommendation engine in e-commerce? answer by @smolix https://t.co/XbbPrtKZBN RT @maxpumperla: Elephas has distributed hyperparameter #optimization &amp; ensemble models now: https://t.co/bMpiQgb6lX #DeepLearning #keras RT @lukede0: #CNN + #GRU + list comprehension in #Python with the new #Keras #API! Raving mad stuff @fchollet, this is great https://t.co/1‚Ä¶ RT @rasbt: "Introducing Keras 1.0 --  It has made tremendous progress since, both on the development front, and as a community" https://t.c‚Ä¶ RT @fchollet: Keras 1.0 is now released. Announcement: https://t.co/XQsC0TBiG4 
Code: https://t.co/qp8oTlPNpo 
Get started: https://t.co/JF‚Ä¶ RT @petewarden: @b0rk you're right, and data is the most common source of problems! I tried to cover some common image issues here: https:/‚Ä¶ RT @heuritechdata: A report of the @heuritechdata #deeplearning #meetup 6 with @ylecun @nvidia at @TelecomPTech https://t.co/cdSmoYQwQr htt‚Ä¶ RT @glouppe: Meet me at the next Zurich ML meetup for "Robust and calibrated estimators with Scikit-Learn" https://t.co/sMEo8KIaF6 Material‚Ä¶ @communicating you should have a look at: https://t.co/P15N3wfDtg RT @libcloud: We just released 1.0.0-rc2! 8 new drivers, big changes to the EC2 driver and improvements to GCE and Dimension Data https://t‚Ä¶ RT @stanfordnlp: scan comes to @TensorFlo! Progress for sophisticated recurrent nets. #dlearn  https://t.co/6OtRRPvw6Q RT @sedielem: I've uploaded my PhD thesis "Learning feature hierarchies for musical audio signals", which I defended in January: https://t.‚Ä¶ RT @hugo_larochelle: My notes on Adaptive Computation Time for Recurrent Neural Networks (by Alex Graves on a neat neural architecture): 
h‚Ä¶ RT @sedielem: Two G+ discussions on weight initialization in NNs from a while back, still a great read! https://t.co/ouxzIKaFgD https://t.c‚Ä¶ RT @samim: Deep Networks with Stochastic Depth - now with #Keras implementation (see links): https://t.co/Qm4zFQrgng #GitXiv https://t.co/U‚Ä¶ @iskander learning from label proportions? Might be related to the work by https://t.co/bd7qb3cd5Y RT @le_roux_nicolas: That is absolutely phenomenal control. https://t.co/e2WDpfIngb RT @petewarden: I'm amazed by this new bipedal robot from Schaft - https://t.co/sdlYqRqKSH - I want to go hiking with it! RT @randal_olson: What's new in #Python 3.6. #programming

I'm excited about the new string formatting!

https://t.co/w1K5vH1vhu https://t.‚Ä¶ @aCraigPfeifer @yoavgo @yaringal @gchrupala @kadarakos and someone else is working on a keras version as we speak: https://t.co/yZ3z0iIkDY @aCraigPfeifer @yoavgo @yaringal @gchrupala @kadarakos original code is available at: https://t.co/F0GWQ7dwC4 @aCraigPfeifer @yoavgo @yaringal @gchrupala @kadarakos the model is very simple. I am confident that people will reproduce the results soon. RT @ylecun: In his keynote at GTC, NVIDIA CEO Jensen Huang showed a video of BB8, a self-driving car that uses "DaveNet" a... https://t.co/‚Ä¶ RT @koverholt: Using Dask with YARN for distributed computations natively from Python via the knit library, by @quasiben: https://t.co/OeAK‚Ä¶ @edersantana @recurseparadox @deliprao @gchrupala resnet is a stronger inductive bias because the linear combination has fixed weights. v2 of theory paper "Train faster, generalize better: Stability of SGD" by Hardt, Recht &amp; Singer comes w experiments: https://t.co/08Xgu87Erc @yoavgo @le_roux_nicolas @gchrupala @kadarakos https://t.co/wpT3Z3kIkM @yoavgo @le_roux_nicolas @gchrupala @kadarakos e.g. @ylecun's EBM framework tastes like Bayesian formalism without intractable integrals @yoavgo @le_roux_nicolas @gchrupala @kadarakos dl is like graphical model without caring about the partition function / normalized proba @yoavgo @gchrupala @kadarakos by continuity it is to me ;) @yoavgo @gchrupala @kadarakos @yaringal might be generalized to stochastic depth as well. @yoavgo @gchrupala @kadarakos the variational Bayes interpretation of dropout by @yaringal is interesting though. @yoavgo @gchrupala @kadarakos see https://t.co/08Xgu87Erc @yoavgo @gchrupala @kadarakos and SGD provides Bousquet-style stability / generalization. It might be more important than commonly accepted. @yoavgo @gchrupala @kadarakos stochatic optimizers allow for stochastic regularizers / architectures: dropout, batchnorm, stochastic depths @yoavgo @gchrupala @kadarakos the common point is they are compositions of parameterized differientable modules. @gchrupala @kadarakos @yoavgo what is important is modularity/composabilty + differentiability wrt inputs &amp; params + loss minimized w/ sgd @gchrupala @kadarakos @yoavgo e.g. the DL family is as rich as the Bayesian graphical model family. @gchrupala @kadarakos @yoavgo my point is that restricting Deep Learning to MLPs is too restrictive. DL is a rich family of models 1/n @kadarakos @gchrupala @yoavgo does not cover architectures like categorical embedding, convnets, RNNs, soft attn mechanisms, NTM, stack RNNs @gchrupala next step is stochastic recurrent unrolling? @gchrupala this is what I thought but I suspected important hidden tweaks (eg init of gate params). @yoavgo learning with compositions of parameterized differentiable functions. @nikete @chrishwiggins @hannawallach @NipsConference would love a practical tutorial on causality by Leon Bottou. @gchrupala is the code online? Or a blog post describing the details? RT @gchrupala: Stacks of residual GRUs learn much faster than plain stacked GRUs. https://t.co/DqQsMzyFe0 RT @dribnet: More neural puppetry.
(ML note: mostly unsupervised, turn vector determined from a few dozen examples) https://t.co/68UyG2GBHc Interesting twitter discussion on the state of the art of deep nets for speech recognition: https://t.co/gO1MGQK7ZI RT @dribnet: neural puppetry experiment https://t.co/ZFr2cn9rwW RT @cournape: my last project: import profiler, a small too to find slow imports in your CLI python tools https://t.co/2Gy8y7dyMo RT @betatim: Neutrinos are like üëª but you can classify them with CNNs!Nice work by NOvA on trying out new ideas: https://t.co/xK8mEoYF6k #m‚Ä¶ RT @leonpalafox: The deep learning book by Goodfellow and Bengio is ready. https://t.co/9SbfB0vyh2 @viodek hum presentation vs compute... There is quite a bunch of JS machine learning tools. RT @wesmckinn: Slides: "Next-generation Python Big Data Tools, powered by @ApacheArrow", from last night's meetup @yelpsf https://t.co/i9Fo‚Ä¶ RT @twiecki: #PyMC3 paper published at @PeerJCompSci https://t.co/s0TKDeJBlW https://t.co/dHGK3jfNZ4 RT @mrtz: Asynchronous SGD is so 2015. We're back to synchronous: https://t.co/iUihxHmRMt @jackclarkSF @clmt @npinto a cup holder to keep coffee warm. RT @agramfort: You're a master student with #datascience skills somewhere on the planet? Participate to https://t.co/i9jHOMBGx3 and show wh‚Ä¶ @mrocklin @intelswfeed I wonder how we could fix oversubscription issues directly in the python standard lib threadpool. RT @jamesgrime: How good are you at guessing correlation coefficients from scatter plots? Test your skills! https://t.co/wrDw6CrEOv by @oma‚Ä¶ RT @lmthang: Excited abt our translation model! +7.9 BLEU. SOTA English-Czech. See Fig3, Table4! https://t.co/gA4H7Zf8aT @chrmanning @stanf‚Ä¶ RT @naivebayesian: @ylecun Page not found. However this works: https://t.co/6ubYiP46Xt RT @ylecun: Facebook image captioning for the visually impaired is going live today.

This engineering blog post tells the... https://t.co/‚Ä¶ RT @mrocklin: Happy to see Intel use #Dask to test their new TBB ThreadPool #PyData https://t.co/CiqFMjY3TG @RichardSocher @MetaMindIO congrats! @vchollati those are only for linux. Windows will follow but slightly more complex. Please help us test manylinux1 wheels for cython, numpy, scipy, pandas, numexpr, scikit-learn &amp; statsmodels: https://t.co/qkaheyESX8 #pydata RT @mikeloukides: Locally generated and traded energy on Brooklyn Microgrid. Using Ethereum/Blockchain.  https://t.co/h2zEBVQ4kb RT @culurciello: https://t.co/gHnCz9QIHT
IBM truenorth first cool result @deliprao @AlecRad I hope that the pthreads build is still robust. I have not tested it extensively in the latest versions. April is treating is well so far: new @ModeratOfficial and @3TrappedTigers albums! #music @deliprao @AlecRad or you can also rebuild OpenBLAS to use a robustly managed pthreads pool instead of the openmp managed threadpool. @deliprao @AlecRad if this is the cause of your pbm you can use the forkserver startmethod of multiprocessing in python 3.4+ @deliprao @AlecRad if you built it using openmp and gcc and use python multiprocessing with the fork startmethod  it can freeze. RT @amuellerml: My PyData Amsterdam talk on advanced scikit-learn is online: https://t.co/p3lxUE2ef9 (in case that's what floats your boat)‚Ä¶ RT @jeremyphoward: 8-Bit Approximations for Parallelism in Deep Learning https://t.co/D1x6UWHTBn @Maciej_Kula @alexjc stochastic: it does not try to model a posterior distribution explicitly but training is randomized like sgd &amp; dropout RT @alexjc: Machine Learning has no April Fools: drop out entire layers during training... and it works! https://t.co/jzF9FhqCSc https://t.‚Ä¶ RT @abursuc: Torch code for Deep Networks with Stochastic Depth is already out https://t.co/4DLJ5oLZv0 RT @karpathy: That's odd. ResNets and papers like this https://t.co/abUPKIGEHw (stochastic depth!) challenge the conventional view of how C‚Ä¶ RT @brettsky: Video showing @ubuntu on @Windows using gcc, SSH, Ruby, etc. https://t.co/1IT9xgMV3r /cc @1st1 RT @twiecki: Variational Inference with mini-batches lands in #PyMC3 https://t.co/IH4kJtxqUb Thanks to @mcmc_stan and all others. https://t‚Ä¶ RT @open_ai: The OpenAI team is growing: https://t.co/xFrpYSBhGR. Welcome, everyone! RT @hroncok: #Python tests on virtual #MacOS X with #Vagrant and #pytest https://t.co/iwZ8aTHfa4 #selfblog RT @evolvingstuff: Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles
https://t.co/rWG2tWCdfc https://t.co/LK4CFFnDaL RT @kellabyte: "We are bringing Bash to windows" no that's not the story. The story is you're bringing Ubuntu native binaries to Windows. RT @utopiah: Machine learning commodification player 2 enters the game https://t.co/aPX1N9JNCy after @Google @Microsoft RT @FrancescAlted: C-Blosc 1.8.0 is out! MS VS2008 and VS2010 compatibility (important for Python ecosystem) is restored: https://t.co/YZ92‚Ä¶ RT @gchrupala: Finally someone did it: learning to trade off accuracy against speed.  https://t.co/UYPlIM0x9H RT @mblondel_ml: py-earth is the second project to join scikit-learn-contrib https://t.co/keqvYDdy4D More projects welcome! RT @rsalakhu: New paper on Revisiting Semi-Supervised Learning with Graph Embeddings
https://t.co/uB1YIkr61o
with Zhilin Yang and William C‚Ä¶ RT @kneufeld: @dabeaz alias pip='python -m pip' RT @chrisemoody: Not only does batch normalization work with LSTMs, it works dramatically better! I need to implement immediately! https://‚Ä¶ @wesmckinn @hadleywickham do you plan support for memmaped feather buffers? @wesmckinn @hadleywickham how does it compare to parquet? Less mallocs/memcopies because lack of chunking? RT @seanjtaylor: IMO killer feature for Feather isn't speed, it's storing column type information: https://t.co/lDv9wP0CBF Thanks @wesmckin‚Ä¶ @AlisonBLowndes @BaiduResearch @adampaulcoates very interesting work. Will you opensource some code to implement Persistent RNNs? RT @karpathy: Attend, Infer, Repeat: Fast Scene Understanding with Generative Models https://t.co/9f9IC8OOCl + nice video https://t.co/QrrR‚Ä¶ RT @shakir_za: Deep generative models with discrete and continuous latents! Insights from DeepMind colleagues .@theophaneweber   https://t.‚Ä¶ RT @glouppe: Keras-powered neural networks are used at @CERN by @ATLASexperiment to process pixel detector data https://t.co/s61LI579QK CC‚Ä¶ RT @johnmyleswhite: Glad to see @wesmckinn and @hadleywickham announce the release of feather: https://t.co/3DJF9Tqa6O RT @mrocklin: @BokehPlots More in-depth screencast of Dask Bokeh web UI:  https://t.co/bYJUg3uB00 RT @mrocklin: Really been enjoying building a Dask.distributed web UI with the @BokehPlots server.  This just landed in master. https://t.c‚Ä¶ RT @ylecun: Marat Dukhan has released a super-fast CPU back-end for convolutional nets. Marat started this project while... https://t.co/ZM‚Ä¶ RT @rgbkrk: üéâ New Hydrogen release for @atomeditor! Complete with inline plots, tables, and images. https://t.co/Y7Wm8HcFUs üéâ https://t.co/‚Ä¶ RT @xamat: False alarm: Deep Learning doesn't (easily) win #NetflixPrize. Read https://t.co/xMPBZbbB66 for a practical example of bad time ‚Ä¶ @EGouillart salut, t'as vu https://t.co/BNtzUgP2jC ? Est-ce que tu connais d'autres mat√©riaux qui aurait les m√™me caract√©ristiques ? @qlamerand yes j'ai vu ca :) RT @kubernetesio: Scaling neural network image classification using Kubernetes with TensorFlow Serving. https://t.co/Fw7XnBKlw6 https://t.c‚Ä¶ @wesmckinn too bad they do not use the postgresql license :( RT @wesmckinn: CitusDB source released (AGPL + CLA) https://t.co/DvJYexMOM6 RT @Maciej_Kula: @seanmylaw @ogrisel Keras example: https://t.co/2GXnXDICie LightFM example: https://t.co/oyo2lhFvac @Maciej_Kula @seanmylaw +1 for a keras example. @mrocklin @jseabold @martin_durant_ I prefer vendor agnostic tools eg libcloud. It supports aws, google cloud, azure, rackspace, openstack.. @albietz @__genji__ @agramfort Indeed @__genji__ @agramfort this was my feeling as well. EM convergence is easier to assess. No learning rate to tune. @__genji__ @agramfort this something I wanted to explore but did not get to it yet. @mblondel_ml https://t.co/YomDxixAnG @mblondel_ml this is the kind of architecture i had in mind when we were talking about DL alternatives to factorization machines. RT @fchollet: Beating the winners of the 2009 Netflix prize ($1M) with a super simple Deep Learning solution, 15 lines of Keras: https://t.‚Ä¶ RT @freefrancisco: How to make any protein you want for $360 from the comfort of your jupyter python notebook. https://t.co/wXMOkdCJ19 RT @aterrel: God bless progress bars, and especially tqdm https://t.co/oG76CjLOIo RT @stefanvdwalt: Python wheels now support binary installs on Linux through the manylinux1 platform! https://t.co/FQdoORBUrF #python #pypi‚Ä¶ RT @mikeloukides: Interesting new battery chemistries: liquid calcium, salt. 
https://t.co/W06v87ljdz RT @tejasdkulkarni: Impressive results! Confirms intuition that "alignment" will greatly improve deep gen models
https://t.co/Grkkji1VQD ht‚Ä¶ @jakevdp @dhalperi but why a new Python based product in 2016 that does not support Python 3? It could benefit so from inline type hints... RT @jakevdp: Google just released Python support for DataFlow. Could be worth exploring: https://t.co/B0GA8Bkwew HT @dhalperi RT @gchrupala: Words are doomed
A Character-level Decoder without Explicit Segmentation for Neural Machine Translation, Chung et al https:/‚Ä¶ @asmeurer better it to start new python 3 only projects leveraging new features: eg inline type hints &amp; async io syntax. @lothiraldan it's going to happen. I am working on improving the tooling. gevent is the first project to publish manylinux1 wheels on the official PyPI: https://t.co/AzSf7SzanJ RT @betatim: Great roundup of all the things powered by @ProjectJupyter https://t.co/8xXoT7fxaT written by @parente PostgreSQL 9.6 will support parallel aggregation! TPC-H Q1 @ 100GB benchmark shows linear scaling up to 30 workers https://t.co/Lw1AKzkoXe RT @lemire: ‚ÄúWhen a computer defeats a Go champion, it will be a sign that AI is beginning to become the real thing.‚Äù (NYT 1997) https://t.‚Ä¶ RT @stephtdouglas: WOW - in Py3, can specify required unit for function arguments -@adrianprw  #PyAstro16 https://t.co/7Y7GfZIpAZ @clmt @IgorCarron a fablab in the 1740's :) @eshvk @karpathy i try to have around 90% of vegeterian meals on a weekly basis to reduce environmental impact as well. RT @johnplattml: Redoing Ba&amp;Caruana: non-convolutional nets cannot mimic high-quality convnets on CIFAR-10 https://t.co/wvxbwj5ZAk #DeepLea‚Ä¶ RT @samim: "Neural Variational Inference for Text Processing", by @carpedm20: https://t.co/xy9tdnM0Hj #GitXiv https://t.co/qawGfDa3lV RT @cccalum: South Korea is going AI-crazy.  Its govt has announced $850m to boost AI industry.
No doubt partly cos of AlphaGo. 
https://t.‚Ä¶ RT @amiconfusediam: Face2Face: Real-time face to face reenactment! holy sh** this is awesome :)
https://t.co/K9PaI47X78 https://t.co/QX14PS‚Ä¶ RT @coreoslinux: #Clair, the container image security analyzer by CoreOS, is 1.0! https://t.co/nJMR2rwhY1 https://t.co/c1eok2CvI3 RT @amuellerml: Really need to read the paper on auto-sklearn from last nips in detail https://t.co/6L0PKg2CGY by @__mfeurer__ RT @karpathy: Google Puts Boston Dynamics Up for Sale in Robotics Retreat https://t.co/ZG56dZe1HZ interesting glimpse into internal turmoil‚Ä¶ @amuellerml and navigation to source code from any tab. And a source code heat map with %CPU time and max memory usage per line. @amuellerml would be great to have snakeviz via in a vprof tab and a call graph tab. Also a timeline with both CPU and memory info. RT @wesmckinn: Google's Flatbuffers: lightweight and incredibly useful (and easier to learn than you might think) https://t.co/SmuU04Kor0 RT @sedielem: New ResNet results from He et al.: put ReLU/batchnorm before weight layers instead of after! https://t.co/Pz69rAw8Yv https://‚Ä¶ @xhochy @amuellerml hey I used to maintain that pyprof2calltree project! amazed that people still use it :) RT @TomAugspurger: @amuellerml cProfile + https://t.co/1RuAePwvQn is nice @amuellerml and https://t.co/PCbqdccFWM @amuellerml i use line_profiler and cProfile using the I python magic commands. Recently discovered https://t.co/4gOEKDIIHz RT @shakir_za: See our latest paper. Stress-testing the generalisation properties of deep generative models, with @DeepSpiker https://t.co/‚Ä¶ @wesmckinn @IbisData any plans to change that? Citusdb will be open sourced soon and docker run much lighter weight. @wesmckinn @IbisData is it possible to install impala + kudu without a JVM yet? via brew install or conda install maybe? RT @wesmckinn: Released @IbisData 0.7 today! @ApacheKudu integration, bug fixes, and SQL compiler improvements https://t.co/fYRBAcMIMd @rlacombe fait, merci ! RT @samim: Cool looking TensorFlow implementation of DRAW: "a RNN For Image Generation": https://t.co/ZjdqrB9ZM5 https://t.co/KNqXXC0Tpq @rlacombe j'habite pas loin de Porte de Bagnolet, l'une des intersections les plus pollu√©es de Paris. Je suis volontaire pour beta-tester. @wesmckinn I think this is a greatest strength of Rust vs C++: cargo and crates.io @adnothing congrats, you are on Slashdot: https://t.co/LyztvfqHtc RT @mrocklin: Try #Dask on EC2 with a simple startup script.  Blogpost: https://t.co/xUYY2Z9TGa RT @adnothing: Computer games are now so realistic they can teach AI algorithms about the real world https://t.co/uMREOmYHwq On my upcoming‚Ä¶ RT @marin_dim: TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems https://t.co/XWkJ1xuuBh
#GPU #MachineLearning ‚Ä¶ RT @googlecloud: Find out how switching to the Avro file format helped us speed up BigQuery ingestion 10x: https://t.co/X4KRSrpIvi https://‚Ä¶ RT @edersantana: Mufasa teaching deep learning 101 #sorry https://t.co/QuY1uO9L2M RT @ktbartholomew: Learn how to back up and restore your data on Carina: https://t.co/TmOHQ2Gft5 RT @AndrewYNg: How to make self-driving cars a reality soon: Change our attitudes about them! https://t.co/ft0kHRQ1EL @betatim if interpreter latency is not an issue you could just embed a python interpreter in your app. RT @AlecRad: Quick comparison of LR, DNN, CNN with @genekogan visual confusion matrix! https://t.co/H9UflpwjQk RT @AlecRad: Awesome twist on a confusion matrix! Great for intuition/qualitative understanding of a model. https://t.co/kPHIaaUjPy RT @cra: "@ApacheMesos will Support Multiple Container Formats with Unified Containerizer" https://t.co/sgiG9Rh6Is RT @fulhack: Everyone's favorite library for approximate nearest neighbors is celebrating 1‚É£0‚É£0‚É£0‚É£‚≠ê  today! https://t.co/7sRLG054tT üéâüéäüé∑ @eugeneteo nope but I accept maintenance PRs RT @dakami: Theory:  #AlphaGo's advantage is that it can learn from a billion games.  A human's advantage is that it can learn from three. RT @samim: Deep-Q learning Pong with Tensorflow and PyGame: https://t.co/8mEShVsqzb nice tutorial + code RT @CharlesOllion: New post on multilingual text representation learning: FastSent and Correlational Network by @heuritechdata R&amp;D team htt‚Ä¶ RT @karpathy: Having some fun training an Generative Adversarial Network on mixture of 2 gaussians. In raw Javascript, of course. https://t‚Ä¶ RT @TorchML: Texture Networks: Fast networks for styling images like others. From folks at Yandex. https://t.co/OF0yDrP9TQ https://t.co/VVw‚Ä¶ RT @yoavgo: Good stuff. (interpretation: the creative move in game 2 *required* the distributed system, as heavy search was key) https://t.‚Ä¶ @haldaume3 also "solving language" probably requires experience from the real world which might not be seen in the language data only. RT @kentbye: Amazing AI analysis of #AlphaGo's victory. Argues that human commentators inaccurately saw games as even or balanced
https://t‚Ä¶ RT @sedielem: Texture networks: feed-forward style transfer, avoiding costly iterative optimization! https://t.co/sMYNfLQdSq https://t.co/4‚Ä¶ @beaucronin @Sam_L_Shead I feel they should have invited David Silver on stage at the press conference. RT @beaucronin: The fixation on Hassabis is standard media MO, but really runs me the wrong way. Doesn't reflect reality of progress https:‚Ä¶ RT @hugo_larochelle: My notes on Variational inference for Monte Carlo objectives: https://t.co/nMF7WIpPPm
Great job Andriy Mnih and @DeepS‚Ä¶ RT @dontusethiscode: At my @pydata @pydataamsterdam talk today, I added the print statement back to Python 3 as a joke. `from __past__ impo‚Ä¶ RT @pydataamsterdam: Room is so packed for @amuellerml that the boat he‚Äôs on is fluctuating like crazy!! https://t.co/ks8t1ECA4J RT @mtrc: Go players begin to suspect AlphaGo may not have had classical training nor have had a subscription to Go Monthly https://t.co/hT‚Ä¶ RT @jreback: Pandas v0.18.0 released! Lots of new goodies. See the whatsnew here: https://t.co/rQ1GCjasxt #pandas #python @jpatanooga and the value net a great analysis/ debug / educational tool to better understand recorded and possible games configurations @jpatanooga better pruning matters a lot. Also just the policy net is probably an already excellent pro level player now. @datao there is the caffe model zoo for computer vision and google published the weights of their inception net linked in tensorflow tut. @datao and the torch or tensorflow code to run them as part of an MCTS hybrid program. I really hope that @DeepMindAI will release the code and pretrained weights of the winning alphago model after the games end. RT @yoavgo: Yes! Someone plotted the thinking times of man vs. machine! (this is 2nd game) https://t.co/nHw5dH1mWZ @zooba Matthew Brett did it. RT @zooba: No idea who snuck this out without anyone noticing, but "pip install numpy" now works on Windows! https://t.co/M8U8uUivta #pytho‚Ä¶ RT @DanielIGolden: Thanks @ogrisel! The code isn't merged yet, here's an updated link: https://t.co/RcOuwgR91Z RT @jakevdp: Interesting notebook showing a solution of GMM/MoG with (convex!) manifold optimization rather than E-M https://t.co/Bv7rAitc8P 1kWh/kg batteries will have a big impact for (autonomous) cars, robotics, drones &amp; solar/wind energy storage: https://t.co/pYakfDzeWz RT @graphific: Deploying a trained #TensorFlow net to production using only #numpy with #tfdeploy: https://t.co/FQqs9WTp73 https://t.co/fDs‚Ä¶ RT @NandoDF: Google‚Äôs robotic arms are teaching themselves to do things https://t.co/vzWAvwMDvF RT @cfbolz: PyPy 5.0 released: https://t.co/iacPoG3T0f better warmup, much faster C extension emulation, VMProf profiling, CFFI embedding s‚Ä¶ RT @jakevdp: Just released ‚Äì megaman v0.1: Manifold Learning for Millions of Points
code: https://t.co/VRLsd3iDvR
paper: https://t.co/h9Gfk‚Ä¶ RT @karpathy: Curious to what extent is there a necessary trade off between something working very well and something being interpretable b‚Ä¶ RT @karpathy: AlphaGo is an interesting glimpse of a likely future - of AIs working extremely well in various tasks and us mostly confused ‚Ä¶ @Miles_Brundage @yoavgo it might be possible to distill the knowledge of the hybrid mcts system into a single cheap to run policy network RT @F_Vaggi: To our robot overlords.  I'm on your side, I will betray the puny humans whenever you give the order. https://t.co/npp8KTAcZr @ogrisel @glouppe it could be the case that they know the new true value of komi and it's higher than 7.5. @glouppe I wonder if DM knew they had "solved go" before the first game: self-playing had converged to 50% win-rate against younger self. RT @glouppe: Not sure when in the game #AlphaGo knew it would win, but still amazed by Lee Sedol pushing it to the overtime period pip 8.1 has landed in Ubuntu Xenial 16.04 LTS to be released next month: https://t.co/b7tds3ysEm \o/ RT @johnplattml: Details of #XGboost (winning algorithm in many @kaggle competitions) described in https://t.co/tgcrLkLl4V #machinelearning‚Ä¶ @nfaggian @mrocklin if you need a single machine you can use docker directly without swarm. Remove the overlay network from the compose file RT @internetofshit: fuck your rules https://t.co/hq2FFJuPaN tensorflow-slim to remove boilerplate from tensorflow model definitions: https://t.co/vf9stHdkIa RT @petewarden: We have released a full Inception training example in TensorFlow: https://t.co/mSk35mzfAJ RT @PyData: CFP for @pydataberlin has been extended until the 11:59 March 20th! @CallbackWomen @PyLadiesBer #python #datascience RT @shakir_za: The Go Files: AI computer wins first match against master Go player. https://t.co/H0grDltiv1 @Mbussonn @minrk @takluyver I used twine to upload sklearn releases in the past and I did not experience any 500. RT @jpetazzo: Swarm vs Kubernetes, the benchmark: https://t.co/GZN1Hff4UG #SwarmWeek RT @brettsky: @dabeaz And @VictorStinner is doing more for Python 3.6 https://t.co/w7INWr1jd8 @brettsky @dabeaz @VictorStinner and faulthandler to debug deadlocks! RT @dabeaz: Hmmm.  The new Python tracemalloc module--just one more reason to not use anything older than Python 3.4. Yes. RT @ylecun: Pretty big news. https://t.co/yGzPZezxLo RT @stefanvdwalt: üöÄ New @IPythonDev console has proper multi-line editing! https://t.co/zvGq4dVJOA @mrocklin @nfaggian and the kubernetes config works very well on google container engine. There is no security what so ever though. @mrocklin @nfaggian it's not documented yet but the docker compose config works on swarm such as rackspace's Carina (in free beta) RT @larsonite: OK "Deep Mind", see how well you fare on THIS challenge: puppy or bagel?
https://t.co/2ykKPIZ5Wa https://t.co/ULyzFDVstQ @alexjc I don't know how CPU convs are implemented. @alexjc maybe your openblas is compiled with pthreads instead of openmp. @ramez what are your thoughts on nanoflowcell? https://t.co/M2lwLMX7fH sounds like a pretty big deal to me. @mustafasuleymn @glouppe will the curve of the estimation of the value network at each step during the game be published? RT @egrefen: Well done #AlphaGo!! Fantastic game from Lee Sedol. Four more games, but indubitably a new milestone has been reached in AI re‚Ä¶ RT @shakir_za: First Deepmind AlphaGo vs Lee Sedol match in just a few hours. Link to the livestream: https://t.co/9wJOWTNb21 @squeaky_pl are there other musl based distributions besides alpine? RT @letsencrypt: We've issued over 1 million certs for more than 2.4 million domains! https://t.co/0PmXhdzo9a RT @twiecki: Our paper "Probabilistic programming in Python using #PyMC3" was accepted at @thePeerJ! @fonnesbeck @johnsalvatier https://t.c‚Ä¶ RT @amuellerml: The scikit-learn team just created a contrib organization for algorithms that are sklearn compatible: https://t.co/21ACL1ft‚Ä¶ RT @amiconfusediam: Can you predict if a tower of blocks fall? I was worse than a convnet at my attempts. Fun paper from my colleagues http‚Ä¶ RT @TorchML: Facebook releases UETorch, a deeply integrated Torch interface to Unreal Engine 4. https://t.co/QPXQkcRliC RT @EGouillart: scikit-image 0.12 is out! More goodies (featuring inpainting, 3D skeletonization), doc... &amp; hopefully less bugs!
https://t.‚Ä¶ RT @DataParis: Join us for the Pre @dotScale Party at @DeezerFrance HQ for a special meetup on April 23.  https://t.co/jXcrv90Iio RT @jiffyclub: Wow three PyCon tutorial schedule is like 50% data/science related. https://t.co/c24PQXOmy9 RT @alexjc: Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artwork [PDF] https://t.co/4MqQ4Zo5EE New! /cc @nuclai https://t.‚Ä¶ RT @cfregly: #ApacheSpark 2.0 with whole-stage, fused operator codegen as well as vectorized, columnar  parquet I/O.  Glorious! https://t.c‚Ä¶ RT @natefoo: These wheels install on Debian 7, 8, Ubuntu 12.04, 14.04, Fedora 21, CentOS 6, 7, and openSUSE13. Next stop: PyPI! #python #ma‚Ä¶ RT @natefoo: psycopg2 wheels that will work on nearly any Linux w/ pip 8.1.0, no external dependencies: https://t.co/mBpIJuTexL #python #ma‚Ä¶ RT @mappingbabel: We live in a world where Geoff Hinton can talk about 'thought vectors' on a major Canadian TV show https://t.co/LKLO6QFzLk RT @ylecun: I created a community page for my course at Coll√®ge de France.

Visit the page for up-to-date information about... https://t.co‚Ä¶ @rmcgibbo @natefoo you are welcome, thank you for designing the manylinux1 PEP &amp; docker tools RT @natefoo: pip install -vvv --only-binary :all: --no-index -f https://t.co/k6oTH8XprE psycopg2 --&gt; manylinux1 wheel installed! =D =D #pyt‚Ä¶ pip 8.1 is out and can install manylinux1 binary wheels on all major Linux distros: https://t.co/AothoUF7kK https://t.co/ZpKsmNifC0 #python RT @kastnerkyle: Whoa. https://t.co/ocJM5uoYLz https://t.co/1I0sfxxbE9 RT @alexjc: Left: me trying to paint a new Renoir piece late at night. Right: semantic style transfer fixing it up for me! https://t.co/T3q‚Ä¶ RT @amiconfusediam: I've independently benchmarked Nervana's Neon kernels here. Almost 2x on VGG-style nets: https://t.co/G9Y5w4bDXC RT @jbeda: Great read that lays out the relationship and journey from Borg to Omega to #Kubernetes.  https://t.co/1EZ8EPJfDb @michalillich London has not closed when NYC opens? RT @humanfromearth: Wrote about running flask &amp; celery with kubernetes: https://t.co/xVWiSjIdpX RT @pydataberlin: Come join us and help organise a great pydata conference (May 20-21) https://t.co/17V9vVXTVK, or submit a talk at https:/‚Ä¶ @michalillich @corrieelston why the teasing? What is the leakage? @ctitusbrown to implement long running async tasks started from notebook, you can use distributed: https://t.co/CF5PKsUDNa @dabeaz @betatim @khinsen @ctitusbrown you can use joblib.Memory to implement checkpoint-enabled functions: https://t.co/kvbwqsBCRK RT @jedisct1: HighwayHash is significantly faster than SipHash for all measured input sizes https://t.co/6lzpKmg4vq TensorFlow machine learning w/ financial data on Google Cloud with datalab notebooks and BigQuery https://t.co/K2LJanRiFl via @googlecloud @mikedewar sorry, the funny casing can be attributed to android autocorrect. RT @hugo_larochelle: My notes on Pixel Recurrent Neural Networks: https://t.co/LqnccoFq4t
Great work from Aaron van den Oord, @nlstm and @k‚Ä¶ @mikedewar I think research in causality and RL has recently attracted attention precisely because of this PBM. Eg: https://t.co/QQRXUGB7g3 RT @CMastication: Big ram is growing faster than big data... In some cases. Good points.  https://t.co/7Pe523WsAP RT @petewarden: TensorFlow for Poets: https://t.co/dwzqxUiS6w @nikete @samim @Miles_Brundage ai is still pretty bad at real time strategy games with partial observation of the game state, eg starcraft RT @mrocklin: Interactive distributed #Dask arrays on NetCDF data, new blogpost

https://t.co/rPQvBtjNSI https://t.co/GFOXinJAUE @ahmadia @tacaswell @gvwilson maybe try to debug and fix the issue they have with the system python? RT @mat_kelcey: "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks" https://t.co/R7Zn42NqwD RT @SymPy: Please download and test the SymPy 1.0 release candidate https://t.co/8sGj7OhDvw Distributed Tensorflow landed in the master branch: https://t.co/djvKExt5B9 RT @ylecun: Lecture on deep learning at Coll√®ge de France, as popular as ever.

This week, Yann Ollivier is giving a seminar... https://t.c‚Ä¶ @kastnerkyle have you tried identity init for the recurrent matrix? @seanmylaw not but lightfm looks nice. RT @rgaidot: Swarm v. Fleet v. Kubernetes v. Mesos https://t.co/ZHUhFIPzWX #docker RT @TristanDeleu: @fchollet Researchers from IBM Watson have recently benchmarked NTM &amp; NMT for QA: https://t.co/9Xs5JIkh1b. It's a start! RT @fchollet: We'll have to benchmark on bAbI ;-) the MemNN performs as advertised by FB. https://t.co/oQuTQs5VfR RT @pypyproject: A branch with faster and more stable C-API support in PyPy was merged: https://t.co/qSnDhJOD8l RT @driainmurray: Symbolic and algorithmic ways to differentiate the Cholesky. Note on arXiv and Python code:  https://t.co/2pKk6Sf8h7 http‚Ä¶ RT @TristanDeleu: I'm talking at the Paris Deep Learning Meetup tonight at @heuritechdata about Neural Turing Machines https://t.co/gFxNyBc‚Ä¶ RT @mblondel_ml: SAGA is now in lightning https://t.co/srkoflaU4K RT @wfwhitney: Our latest paper learns concepts of objects and movement with zero supervision. Description, demos, and code: https://t.co/s‚Ä¶ @rgbkrk I am playing with carina, docker-compose, distributed and jupyter: https://t.co/QqmJdsBBX1 is it possible to get &gt; 4GB RAM flavors? RT @TristanDeleu: With @snips, we published our Neural Turing Machine library in Lasagne https://t.co/fLeHNnxrga RT @heuritechdata: Registration are open for @heuritechdata #DeepLearning #Meetup #5! With Matthieu Cord TDeleu VGire CCharbuillet https://‚Ä¶ RT @StatMLPapers: Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression. (arXiv:1602.05419v1 [math.OC]) https://t‚Ä¶ Lazytweet: has anybody tried to reproduce the results of the Evolutional Dropout paper with an open source toolkit? https://t.co/rVwtSgXly9 RT @karpathy: Wow, it looks like Boston Dynamics' Atlas robot is making a lot of progress https://t.co/oiYWR34g7r RT @karpathy: Also Inception + ResNet combo now gives 3.08% top 5 error on ImageNet https://t.co/RxCUqwHpTJ just to put this in context, th‚Ä¶ RT @jakevdp: Vega-lite 1.0 just released by @uwdata! Looks really nice: https://t.co/9SEtJoRc4b RT @ianozsvald: Reminder - @pydatalondon Call for Proposals closes first week of March: https://t.co/QOvz1AuBvc (&lt;2 wks), please get your t‚Ä¶ RT @yaringal: I was recently asked about a peculiar behaviour of dropout's uncertainty estimates... [blog] https://t.co/tnS2m9QSCT https://‚Ä¶ RT @syhw: Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples https://t.co/AlKx2fkNKK RT @RadimRehurek: Swivel: Google's successor to outperform #word2vec &amp; #GloVe https://t.co/OUjZlS4Xf6 | HT @JorisPelemans | @stanfordnlp RT @wesmckinn: New post: "Why pandas users should be excited about @ApacheArrow" https://t.co/NTUpWvRwEZ #pydata RT @wesmckinn: New post: "Python and Hadoop: A State of the Union" https://t.co/h15zNe4JpF @tlipcon @przemur @ApacheKudu would love to brew install it (with impala) on dev laptop. RT @tlipcon: Nice slide deck on #Kudu from @przemur, an early @ApacheKudu adopter: https://t.co/wqZJrWusec RT @mrocklin: Pandas on an HDFS cluster with dask dataframes.  https://t.co/AMYeBpiFZX https://t.co/EKJtqvgtxS RT @J_: https://t.co/sleLVhn6HK RT @J_: I ever wished you could read @ApacheParquet from C++ or have native bindings to your favorite language, now is the time to watch pa‚Ä¶ RT @ManAHLTech: Pandas on HDFS with Dask Dataframes by @mrocklin https://t.co/DcyPXiEdK4 RT @deliprao: The results look great here. Gains by incorporating context at multiple levels. Very cool. https://t.co/xM2ysbtvqm RT @hugo_larochelle: My thoughts on "A note on the evaluation of generative models":
https://t.co/drwPBkJutP
Mandatory reading to do genera‚Ä¶ @pasku1 @ryan_sb @marknca you mean .pyc files? RT @ylecun: Training set augmentation improves human learning, not just machine learning?

Distort your tennis swing or a... https://t.co/t‚Ä¶ RT @ChristianHeimes: CPython @CoverityScan is down to 0 defects (3.6 dev + a couple of patches I'll submit soon). https://t.co/GNAjHgZdoO #‚Ä¶ Using Scikit-Learn in AWS Lambda via @ryan_sb https://t.co/T6NV7OdpjF RT @gvanrossum: Mypy 0.3 is out! See https://t.co/bRNzUharjg -- or just "python3 -m pip install -U mypy-lang". Repo is now https://t.co/wTe‚Ä¶ RT @fulhack: Annoy 1.8.0 is fresh and it's 2x faster than before! Could be the fastest library for approximate nearest neighbors. https://t‚Ä¶ @ryan_sb @pasku1 @marknca excellent. Looking forward to reading it. @iskander thanks. Looking forward to more trial results. @iskander would love to hear your take on https://t.co/61bI9Wccz6 RT @petewarden: We've just opened up the Google Cloud Vision API to everyone! https://t.co/WUNIbSjb4z RT @NeuroDebian: Super-fresh #scikit-learn (0.17.1 with few post-release fixes) is up for grabbers! Don't forget to upgrade #joblib as well Really enjoying Rackspace's https://t.co/mp1iJ6YpMc to quickly provision docker clusters from the command line. RT @wesmckinn: Slides from @NYCPythonMeetup: "Enabling Python to be a Better Big Data Citizen [with @ApacheArrow]" https://t.co/ELuVWLuTLi RT @vintermann: Ian Osband blogged a great explanation of Thompson sampling https://t.co/JKc3jYxdpp. Now they're applying it in RL https://‚Ä¶ RT @sfermigier: EuroPython 2016: Call for Proposals https://t.co/V6PIBHD412 scikit-learn 0.17.1 is out. This is a bugfix release: https://t.co/MPBaJN7wmA RT @graphific: #DeepDream Single Unit acivations on GoogLeNet (ILSVRC2012) first 100 of 256 at #flickr (1024p) https://t.co/EpMtUDo3W7 RT @mrocklin: Distributed computing with Python. Blog using #Dask to analyze GitHub data on a cluster.
https://t.co/HyqFYrkuFD
https://t.co‚Ä¶ @ryan_sb @marknca @pasku1 or numpy scipy from conda. @ryan_sb @marknca @pasku1 you should use experimental manylinux wheels for numpy / scipy to get the openblas dll embedded in site-packages. RT @ryan_sb: @ogrisel @pasku1 @marknca That's awesome! Scripted: https://t.co/0LuRJV7p4v
original size 198M
venv after stripping 100M
final‚Ä¶ @ryan_sb @pasku1 @marknca probably I havenot tried yet. Cc me if you can get it to work. @ogrisel @pasku1 @ryan_sb @marknca and then: find venv -type f -exec zip -9 https://t.co/R31aNIr6DL {} + @ogrisel @pasku1 @ryan_sb @marknca find venv/lib/pythonX.X/site-packages/ -name "*.so" | xargs strip @pasku1 @ryan_sb @marknca try to strip the compiled extensions in your venv before zipping @ryan_sb @pasku1 @marknca what is the size limit for AWS Lambda? RT @shakir_za: Fantastic! State-of-the-art semi-supervised learning in deep generative models using auxiliary variables.  https://t.co/fq81‚Ä¶ RT @amuellerml: Next call for NYU Center for Data Science Fellows is up https://t.co/j1xLwKiL6x Interdisciplinary post-doctoral position at‚Ä¶ RT @wesmckinn: More technical details and motivation for @ApacheArrow from the @dremiohq team https://t.co/YfzgFYDAxs RT @pydataberlin: We are happy to announce #PyData-#Berlin @ #kosmosberlin on May 20-21 with @wesmckinn, @b0rk and @ogrisel. RT @stonebigdotdot: @YhatHQ @ogrisel and Sframe beta on Python3 and on Windows (and Mac) https://t.co/skZs9YX2v6 RT @YhatHQ: TensorFlow 0.7.0 Release Notes | https://t.co/wJnYx5jD0c RT @samim: Generating Images with Recurrent Adversarial Networks: https://t.co/7RS9j2UYgg via @kastnerkyle &amp; @graphific https://t.co/rBkuk6‚Ä¶ @wesmckinn @tom_e_white I missed the established :) @tom_e_white @wesmckinn TIL: pandas is an 'emerging' open source project :) RT @tom_e_white: "Apache Arrow will enable Python and R to become first-class languages across the entire Big Data stack" @wesmckinn https:‚Ä¶ RT @NVIDIAGeForce: .@thekhronosgroup launches @VulkanAPI! GeForce drivers for Windows &amp; Linux available now: https://t.co/fUJRqvxHvP https:‚Ä¶ @beaucronin it's not the model that is shitty, it's the statistical problem itself. RT @nvie: So if you ever wanted to ‚Äústash away‚Äù only one chunk, or just one file, use git stash -p next time. RT @fperez_org: Kudos to @thefreemanlab for amazing https://t.co/GhzgQJ9CaV that makes this possible! (docker + jupyter on demand) https://‚Ä¶ RT @jakevdp: LIGO scientists on a Reddit IAmA thread talk about how #Python is used in science: https://t.co/ef69eSBOcV RT @tdhopper: I like the Pandas 0.16 .assign method for creating temporary columns https://t.co/AknZmJmzAC via @jreback https://t.co/7Szmti‚Ä¶ RT @glouppe: I'll be giving a Scikit-Learn tutorial  as part of the Heavy Flavour workshop https://t.co/ddo425ml3z Materials at https://t.c‚Ä¶ RT @samim: "How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks": https://t.co/57CIOgM14D https://t.co/lDP3h25wyL RT @StatMLPapers: Improved Dropout for Shallow and Deep Learning. (arXiv:1602.02220v1 [cs.LG]) https://t.co/SRaCVKu6o8 RT @dancohen: Very notable: landmark paper about gravitational waves has just been published with a *CC-BY license* https://t.co/GTN6V1Zywl @betatim @matplotlib and they did not use the jet colormap for the heatmap! We live in the future :) RT @mxlearn: [1602.03609] Attentive Pooling Networks (IBM Watson, " it has been applied to both convolutional neural networ... https://t.co‚Ä¶ RT @quasiben: Nascent support for deploying PyData applications on YARN: https://t.co/JsbGwyczw7 -- built with support from @ContinuumIO RT @PyDataTokyo: Excited to announce @PyDataTokyo is now on @Meetup supported by @NumFOCUS ! Join us and check for upcoming meetups! https:‚Ä¶ RT @ParisMLgroup: Tonight #MLParis, we'll have @ogrisel @quantopian @heloisenonne thanks to @quantmetry https://t.co/4Xy9ixg1I2 RT @codinghorror: the CPU speed party is basically over at this point https://t.co/piosfaYCsU RT @abursuc: New dataset of high quality 3D scans of ~10k real objects https://t.co/uomLaE1z3Y RT @alxndrkalinin: #Python code for BinaryNet: Training Deep NNs w/ Weights &amp; Activations Constrained to +1/-1 https://t.co/2mBwgmfnyd http‚Ä¶ RT @deliprao: Converting dot products to bitwise operations in BinaryNet. https://t.co/7mqdyanVhF RT @sdouche: vprof 0.2 released (Visual profiler for Python) #python - https://t.co/NlWik1xiRx @chrisemoody @edersantana this is the way it's introduced in the origainal kingma et al paper right? RT @mxlearn: [1602.02830] BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 https://t.co/kJhOIj‚Ä¶ RT @sedielem: Finally wrote a paper about convnets with cyclic pooling / rolling (used to win last year's NDSB competition): https://t.co/z‚Ä¶ @edersantana Interesting, I had not seen vae like that :) RT @tallinzen: Turns out a 2-layer LSTM with 8192 units per layer, trained for 3 weeks on 32 GPUs, makes a great language model https://t.c‚Ä¶ RT @StatMLPapers: Compressed Online Dictionary Learning for Fast fMRI Decomposition. (arXiv:1602.02701v1 [stat.ML]) https://t.co/pw9FpMJ0vD RT @jakevdp: I just discovered ``numpy.core.umath_tests.inner1d()``. I can't understand why such a useful function is so hidden! https://t.‚Ä¶ RT @NandoDF: ICLR 2016 accepted papers on learning representations  https://t.co/dmdo5idlbG RT @amuellerml: PyData Amsterdam is March 12-13. Looks like I'm going :) Still time to submit your talks. It'll be great! https://t.co/AQ9u‚Ä¶ RT @mtyka: Neural net class visualization with bilateral filters: https://t.co/pHNBH73BnZ https://t.co/WdhXDpNi9R RT @zephoria: 20 years ago @JPBarlow wrote Declaration of Independence for Cyberspace. I went to Davos w/ that in mind: https://t.co/9gKyy8‚Ä¶ RT @dlowd: @hintikka The key to learning NNs is to read the papers in small batches and randomly blank out half the words. RT @shakir_za: Agents having fun navigating and finding rewards in our DeepMind Labyrinth 3D environment. https://t.co/5MTyvQp7LQ RT @kestelyn: 300+ RSVPs for @wesmckinn at SF Python Users meetup next Weds 2/10 ‚Äî 100 seats left https://t.co/2GdrZE9NNr RT @karpathy: Also a very nice writeup on Torch blog about various ResNet experiments &amp; training protocol implementation details https://t.‚Ä¶ RT @karpathy: Very nice code release from Facebook re-implementing Kaiming He et al. Residual Networks (+pretrained models) https://t.co/64‚Ä¶ RT @koush: Html5 is amazing. High end hair rendering. https://t.co/sjX7Q4V8Zq @thibaut_barrere overview paper by googlers on the topic: https://t.co/AIzmjNKLM0 RT @cshperspectives: Reproducibility reminder: bioRxiv has sections for Contradictory &amp; Confirmatory Results https://t.co/xfNeiSjIEV https:‚Ä¶ RT @alexjc: Reasons not to publish on @Nature: Deep Mind takes down its copy of AlphaGo PDF, under pressure? Mirror: https://t.co/G1uSBmfmjo RT @clmt: Nice summary of #reworkDL conference https://t.co/YmWKwzCe66 RT @egrefen: Happy to announce that our @DeepMindAI paper with @_rockt on reasoning with neural networks got in to ICLR 2016! https://t.co/‚Ä¶ @jeromeporier typo dans https://t.co/vyoXHgbb2q : programmeurs (informatique) au lieu de programmateurs (radio ou appareils √©lectriques) RT @fulhack: Very cool ‚Äì @Teejosaur took my font data set and applied a generative adversarial network https://t.co/tDYzEtb0vi RT @mfiguiere: Google Dataflow has been approved as Apache Beam incubator project. https://t.co/WQVKDWADKP RT @DataJunkie: Nice! This can be a great corpus for natural language processing! https://t.co/G3x05bBO4V RT @gensim_py: #Gensim 0.12.4 released! Faster #word2vec,  fixes in #doc2vec, Windows wheels &amp; other improvements: https://t.co/WjNhMIBZzC RT @algoritmic: Neural Enquirer: Learning to Query Tables with Natural Language #NLProc #machinelearning 

https://t.co/0a7Yji6yW5 https://‚Ä¶ RT @amuellerml: xray is now xarray. Labeled n-dimensional arrays for python! https://t.co/54XafsbHVW @cournape oui! @cournape non j'ai pas pr√©vu finalement... @mrocklin @kastnerkyle indeed, but as soon as you need locks, complexity quickly explodes. @amiconfusediam @clmt thanks for the feedback! @clmt what's your opinion on EAMSGD vs one bit quantized SGD as implemented in     https://t.co/bw9v0n0964 or cntk? @kastnerkyle actually this also a pattern we want to implement in joblib at some point. RT @alexjc: Hope Lee Sedol doesn't realize he can beat AlphaGo by drawing panda to fool neural network! https://t.co/DFW6ioOXcz https://t.c‚Ä¶ @kastnerkyle I am not 100% sure it does what you want though. @kastnerkyle multiprocessing.ThreadPool.imap should do what you want. @kastnerkyle thread pool executor is already built upon threading and queue modules. @kastnerkyle in that case manual thread management is probably better. @kastnerkyle and you can easily switch to ProcessPoolExecutor in case of GIL contention. @kastnerkyle the api is much higher level &amp; intuitive. @kastnerkyle you should use concurrent.futures ThreadPoolExecutor and futures instead of manual threads. @seanmylaw nothing wrong but don't expect it to find the same cluster as dbscan on randomized pca in 30d for instance. RT @egrefen: Proud of my DeepMind colleagues' achievements in producing what is probably the best Go player in the world! https://t.co/POJb‚Ä¶ RT @egrefen: Nature interview with @demishassabis and David Silver about DeepMind's tremendous Computer Go success https://t.co/S6jFAkf9AP RT @clmt: We‚Äôre releasing new libraries for parallelizing the training of deep neural nets, in @TorchML https://t.co/nH0yZUTzCN RT @pchapuis: The Society of Mind, by late Marvin Minsky, can be read online for free at https://t.co/UycWjj8iKE RT @DeepSpiker: Checkout the latest DeepMind generative model paper by Aaron and Nal:
https://t.co/c6QaKg0XpW @dabeaz indeed I have not seen an ad in a while. I had not realized it could be tied to the number of followers. @minrk maybe @syhw or @kastnerkyle could make a suggestion. @minrk maybe find a tool to add the missing punctuation first, eg as part of a speech to text library. @luispedrocoelho @rasbt I agree. @rasbt @luispedrocoelho I meant you can put the args at the beginning of the next line with an indent. @luispedrocoelho @rasbt teaching jupyter notebook for live tutorials: increase font size, get horizontal scroll. To me 80 is a good habit. @luispedrocoelho @rasbt I read code on my phone. RT @chrisemoody: Visualizing topics now works in lda2vec. code demo &amp; (minimal) docs: https://t.co/PSZsaJz121 https://t.co/ypy89f1GuJ @shoyer congrats! Will you be able to continue developing xray and other python tools? @rasbt you could just put the line break after the opening paren of the function call. @luispedrocoelho @rasbt and mobile devices and video projectors from 2016 without horizontal scrolling. @pmarca @Sumanpatel96 in absolute value or as a rate of working age people? RT @Maciej_Kula: RPForest 1.3 is out, with much improved memory efficiency (through reusing the projection hyperplanes) https://t.co/Tf5v8o‚Ä¶ RT @mtyka: A new style transfer paper out. This method is even more impressive. https://t.co/Qz0pswaMeR via @samim https://t.co/iRC2ZKK0jr RT @PyDataMad: &gt;&gt;&gt; from pydata import Madrid
&gt;&gt;&gt; mad16 = Madrid()
&gt;&gt;&gt; print(mad16.is_open())
True
&gt;&gt;&gt; print(mad16.url)
https://t.co/fHj186I‚Ä¶ RT @alexjc: Pretty convincing Text-To-Speech results from DNN: https://t.co/8uKyxXpsW6 Paper: https://t.co/8pn7xkhGTB via @samim https://t.‚Ä¶ @dpg I think the specific workflow is very business dependent (e.g. frequency of re-training the model on fresh data). @dpg store model training env metadata (e.g. pip freeze) + training data snapshot id for reproducibility, then https://t.co/QLfsjvPzY2 @La_Chaine_Meteo @larrouturou j'approuve le message mais c'est malhonn√™te de publier un tel graphique. Les ratios des barres sont faux. @dpg if you have a specific question in mind please go on. RT @rgaidot: Free SSL certificates by AWS (like let's encrypt) https://t.co/LgVb8Ps3lH RT @PyDataParis: PyData Paris will tentatively take place during the second week of May. Still waiting for venue availability details to co‚Ä¶ RT @samim: Generative Typography will be big, watch the space. https://t.co/Nf2Ew7Blzu https://t.co/fh9fmUzJ2G RT @jtaylor108: my first crappy systemtap script to find #python GIL holding functions: https://t.co/Dieoz0liU4 tips for improvement welcome @jtaylor108 I think it's a good opportunity to subscribe. Doing it now. @jtaylor108 thanks RT @jtaylor108: @ogrisel persistent memory, non volatile memory (NVM), lwn category: https://t.co/vdFcsbGVzS @jtaylor108 It could reduce power consumption significantly if well supported by data analytics tools, e.g. an alternative to np.memmap. @jtaylor108 thanks! what are the keywords to google those articles? @F_Vaggi actually once you install msys2 to get bash and pacman to install packages such as git from the CLI, I think it's a very usable OS. @fcodvpt I installed successfully without any activation key on a newly created virtualbox virtual machine. It works perfectly so far. #lazytweet Is there a way to write programs that load data from PCIe SSD directly to CPU cache SRAM without going through DRAM first? RT @DataParis: Registrations for the next meetup opened this morning. Grab one while it's hot :-) https://t.co/UlHulULlHJ @luispedrocoelho indeed, that's my primary use case. @datao indeed! And you can resize the window height and width without having to increment the rows and columns counter in the settings. Don't worry, I still run POSIX systems on my computers but the windows console is getting much better in Win10 :) Recently noticed that it's free to download the official Windows 10 ISO from https://t.co/XPpvdCM7zM for instance to use with Virtualbox. Related: Carl is making good progress on https://t.co/pKIlJ8jdxN a C/C++/fortran toolchain for building MSVS binary compatible Python wheels Draft PEP and implementation of the manylinux1 platform tag to build binary wheels for major linux distros at once: https://t.co/ONDsgrQhJy RT @BlackiLi: Visualizing K-Means equilibria w/ different parameters
https://t.co/x6Jntezi8l https://t.co/j4qZvqMDj2 RT @benhamner: Phenomenal Q&amp;A session about deep learning by Yoshua Bengio on @Quora https://t.co/mYWuLExtbv Short blog post that gives an intuitive introduction to semi-supervised learning with ladder networks: https://t.co/D0vkp9o5Ox RT @ChainerOfficial: We released Chainer v1.6.0! Many functions and links are added. See the release note for details: https://t.co/Q3yeANg‚Ä¶ RT @CharlesOllion: New post on attention mechanism by @leonardblier and I, beginning of a series on attention and QA. Comments welcome! htt‚Ä¶ RT @meickenberg: Towards a better understanding of what deep convnets could be doing https://t.co/jENwYfQdnr by St√©phane Mallat RT @brandondamos: I'm excited to announce OpenFace 0.2.0! Check out the details in my blog at https://t.co/1R9MX8u0zy #Python #Torch #DeepL‚Ä¶ @Seboss @PenelopeB pourquoi √ó2 en c√©r√©ales ? On peut manger les l√©gumineuses et c√©r√©ales directement plut√¥t que la vache qui les mange. RT @amuellerml: great resources on NLP with neural nets: @yoavgo's primer https://t.co/uYmzyQ06nw and Kyunghyun Cho's lecture notes https:/‚Ä¶ @franckcuny haha j'ai h√©sit√© √† t'envoyer un texto de bonne ann√©e mais je me suis dit que c'√©tait pas dr√¥le. Bonne ann√©e qd m√™me ! @deliprao recursive functions? You can often unroll recursion to an arbitrary horizon in practice though. @chrisemoody thanks, lda2vec was indeed the part I am most interested in ;) @chrisemoody has your talk been recorded? joblib 0.9.4 is out with several important bugs fixed: https://t.co/n1eGf9RyHP RT @sedielem: Lots of useful insights from the winners of the Right Whale Recognition @Kaggle comp: https://t.co/9qmsPwwpiM Great use of au‚Ä¶ RT @chrisemoody: My slides on #word2vec #lda &amp; #lda2vec  https://t.co/JBM93IWwLM and with presenter notes https://t.co/07k6Lmouzr https://t‚Ä¶ @tarek_ziade bienvenue Freya ! RT @teoliphant: Another excellent release of Numba:  https://t.co/sqLOyq3JDi  jitclasses, ‚Äò@‚Äò support, and multi-threaded ufuncs.   #pydata‚Ä¶ RT @karpathy: IPython notebook tutorial: "Adversarial image generation in Tensorflow" https://t.co/YEkuaQxjNR Looks nice/short/sweet. Funny‚Ä¶ RT @johnmyleswhite: So glad to see this request for changes from GitHub to support large OSS projects: https://t.co/MVGlYdMpdQ RT @ContinuumIO: #Dask + Distributed for #cluster via @mrocklin  https://t.co/2zhqxb1UtC RT @haldaume3: giant (13tb compressed) user/news interaction data just released by yahoo: https://t.co/eAocwdvK5w RT @Maciej_Kula: My long-neglected GloVe word embeddings package is now on pypi, with fewer dependencies and speed improvements: https://t.‚Ä¶ @Maciej_Kula @MakingLyst @gglanzani see also: https://t.co/QzMJ6P0DUd &amp; https://t.co/vVY4JUWIqG @Maciej_Kula @MakingLyst @gglanzani you can use appveyor to generate wheels for windows: https://t.co/THNLhQIVQz RT @Maciej_Kula: Version 1.8 of lightfm is out, now builds on Windows with MSVC! https://t.co/0Zg7g0Q11R @MakingLyst @gglanzani RT @yoavgo: Irrationality of EU grants: when 147 teams apply and 2 are awarded, more money is spent preparing grants than is won https://t.‚Ä¶ @alvations @yoavgo it's possible to put public comments on CMT during the ICLR 2016 reiview process: https://t.co/mul0FvwDdi RT @jakevdp: I added a section to my Locality Preserving Projection example, showing improvements over PCA for noisy data https://t.co/zMbd‚Ä¶ @yoavgo @gchrupala @sameasiteverwaz I agree this is no silver bullet (yet?). Investigating which tasks work well together is hard. @yoavgo @sameasiteverwaz pre-training https://t.co/DpqyGaFEMi and / or multi-task learning: https://t.co/PzXbjj1baK @mlblanchette @qz but I agree that discussing the issue publicly helps a lot. @mlblanchette @qz actually I think it's important to invite underrepresented groups to speak at cons too @yoavgo @sameasiteverwaz also Ryan Kiros' Skip-Thought vectors look like a very powerful auxiliary task in that context. RT @mlblanchette: There‚Äôs a way to get girls to stick with science‚Äìand no, it‚Äôs not more female role models https://t.co/WgDIwDV6aD @yoavgo @sameasiteverwaz by pooling training data of different tasks (NLM, MT, QA, image captioning) in a multi task model. @yoavgo @fchollet it's hard to work with approximate meaning without using a continuous representation in the first place. @yoavgo @sameasiteverwaz maybe because of the lack of labels to train models to solve those tasks individually. @fchollet @yoavgo both approximate mapping and partially shared cultural references might be enough for many tasks. @yoavgo it exists to some extent otherwise we would not be able to have this discussion (I am not a native English speaker). @yoavgo a sentence is language dependent. An 'ideal' representation of its meaning might not be. @yoavgo @karpathy @mdreid I agree but then writing the partial derivatives using the nested notation is ugly esp. w/ expanded bias terms. @yoavgo @karpathy @mdreid using the  function composition notation œÉ2 ‚àò l2 ‚àò œÉ1 ‚àò l1 instead of œÉ2(l2(œÉ1(l1(x)))) makes it more readable @yoavgo @karpathy @mdreid what I find confusing is trying to write the predict function of an MLP as nested function calls. Python 3's faulthandler.dump_traceback_later is a great tool to debug deadlocks: https://t.co/TGzuUtBYp2 RT @jakevdp: I just released a lightweight @scikit_learn-compatible implementation of Locality Preserving Projections https://t.co/VAwLYJiC‚Ä¶ RT @janschulz: Make your python code future proof: only check for PY2, never for PY3! https://t.co/qEumx5Sxfo RT @kaggle: Understanding neural network performance bottlenecks through the NOAA right whale challenge https://t.co/e7eOMhH6Cs https://t.c‚Ä¶ RT @hackernewsbot: Stop writing code that will break on Python 4... https://t.co/Xmv2NyuLOv RT @MIT_CSAIL: Why teaching robots to wash dishes would be a game-changer: https://t.co/6LtgwjCmAn @WIRED https://t.co/1SUqUvzenh @yoavgo @karpathy @mdreid indeed that might be an issue. @yoavgo @karpathy @mdreid it's required but only once the BP alg has been shown in action on a specific case (1 hidden layer MLP) as a graph @yoavgo @karpathy @mdreid presenting a graph of ops with activations as forward msg and errors as backward msg is clearer. @yoavgo @karpathy @mdreid presenting a FF net as composition of functions is too verbose from a math notation point of view to indroduce BP. @karpathy also for your slides: https://t.co/cJJatdkV3V @karpathy &amp; highlight the fact that it's necessary to keep the batch of intermediate activations in RAM between the forward &amp; backward pass @karpathy and datastructures involved in a typical backprop implementations for typical network architectures 2/ @karpathy something that is usually not covered in teaching backprop is the complexity in CPU seconds &amp; MB of RAM for individual ops and 1/ @yoavgo @karpathy @mdreid I think going from concrete / informal / specific to abstract / formalized / general is better for teaching. @yoavgo @karpathy @mdreid oops I meant to end that tweet with 2/2 @yoavgo @karpathy @mdreid to prefer to start from the formal abstract pure general case first and then derive specific concrete examples 1/2 @yoavgo @karpathy @mdreid I thing going from informal to formalized is easier for the student although the (French) math teachers tend 1/2 @yoavgo @karpathy @mdreid I think it's good to present the 2 sides: first the error flow on a comp graph and then the formal chain rule RT @mrtz: Deep learning: Adding dropout 1% change, weight decay 1%, momentum 1%, change some initial biases from 0 to -7.2: 10% improvement. RT @t3kcit: .@ylecun on the challenges of moving from task specific algorithms to general artificial intelligence #futureai https://t.co/MT‚Ä¶ RT @CharlesOllion: Looking forward to this workshop and the one on Evaluation of representation. https://t.co/F8kVODgGEa RT @twiecki: Jupyter Notebook 4.1 released https://t.co/SKRZ1EOmLp Multi-cell selection, Restart &amp; RunAll, Find &amp; Replace and Command Palle‚Ä¶ RT @betatim: https://t.co/9aaPJrBq9i interactive jupyter notebook based blog posts on @github pages! Now on the same page, even better than‚Ä¶ @EdwardRaffML @kastnerkyle numpy devs quickly released bugfix releases in 1.10.2. @EdwardRaffML @kastnerkyle why not reporting the issues you observe and maybe help fix them instead? RT @davidcelis: THIS CHANGES EVERYTHING https://t.co/1BxqSfqIhX @marcchantreux PyCon FR? Je sais pas ... RT @mxlearn: Nvidia GPU + CoreOS + Docker + TensorFlow = A Fast, Flexible, Deep Learning Platform https://t.co/uenQ2ka9yn RT @petewarden: Colorizing black and white photos using neural networks - https://t.co/voGhp96bzx - once you can understand image content, ‚Ä¶ RT @sergecell: @sergecell Slides for gpucc
https://t.co/MvcEwFGMl4 @chris_brockett works for me also on chrome for Android... @chris_brockett works for me (firefox). RT @b0rk: Winning the bias-variance tradeoff (the first piece of machine learning math that I think can help me build models!) https://t.co‚Ä¶ RT @hugo_larochelle: My notes on Net2Net: Accelerating Learning via Knowledge Transfer: https://t.co/xD6plHZKdx #deeplearning #iclr2016 RT @fbOpenSource: Introducing MazeBase, a sandbox for learning from games. https://t.co/8EHqmHLH5L RT @AlexSteffen: Roy Batty will be incepted Friday.

#oldfutures https://t.co/OT5136Qakq Video intro of GPUCC: open source clang-based re-implementation of CUDA nvcc + cuFFT / cuBLAS / cuDNN by Google https://t.co/8rKc0GMX6m RT @BokehPlots: Bokeh 0.11 is loose! https://t.co/A5cqzvsW6o Check out demo apps with the new server at https://t.co/gdhbWeO6FV  #datavis #‚Ä¶ RT @grodola: #gevent finally ported to #python3, it was one of the biggest stoppers for python 3 adoption RT @genome_gov: CRISPR's Most Exciting Uses Have Nothing to Do With Gene-Editing https://t.co/7dIhIbZpHk https://t.co/SfV1fnEUf0 RT @desertnaut: New free book by Hastie &amp; Tibshirani: Statistical Learning with Sparsity https://t.co/xWQhaZ4Prl #machinelearning cc @IgorC‚Ä¶ RT @deeplearningldn: Andrej @karpathy explains OpenAI's plans, vision, and future of #AI in an interview. https://t.co/vJtGShizrW https://t‚Ä¶ @sedielem the small improvement over the baseline benchmark is disappointing though. Do you know which method was used for the bench? @F_Vaggi @IgorCarron I don't know any but the architecture is simple enough so that I would not be surprise to see a couple soon. @t3kcit on a power spectrum (eg stft for instrumental music) you use it to decompose in additive components. RT @hugo_larochelle: My notes on Gated Graph Sequence Neural Networks (by @numbercrunching  and colleagues): https://t.co/z5YjSR7xXm . @michoo_42 merci ! J'etais clair ? RT @edgarmeij: Wikipedia makes pageview counts easily accessible, including the ability to differentiate between access methods ‚Äì https://t‚Ä¶ RT @samim: Generating G.W.Bush with moustache, big nose &amp; glasses: https://t.co/Ju3LipvfEd via @graphific https://t.co/uKv9ppiZqM RT @AlecRad: Learning to cluster with adversarial autoencoders. https://t.co/WxAs06MmCy RT @Inria: [Alerte] @ogrisel dans @LaTacfi √† 14h sur @franceinter : Nos vies √† l'heure des #bigdata https://t.co/xtDM1yxckt cc @Inria_Saclay Adversarial Autoencoders by Makhzani et al.: https://t.co/d50LCYra8Z TFD manifold walk at: https://t.co/BvPgv3Leuq https://t.co/q5XCQoOYJR @samim I wonder if it's not overfitting too much. The paper looks interesting. RT @samim: "Autoencoding beyond pixels using similarity metric": https://t.co/Qxpf8loPr7 code soon at: https://t.co/vPjNcuMz6h https://t.co‚Ä¶ RT @__DataTau__: Generating Synthetic Data with Random Forests: https://t.co/05zMsgEFTV RT @samim: DCGAN (Deep Convolutional GAN) in #TensorFlow by @carpedm20: https://t.co/KnnkH6ur9K https://t.co/mXvq7CPyP2 @kastnerkyle working on RL? RT @msurd: Why differentiable functions are important: https://t.co/7QG0VplAnN RT @egrefen: David Dalrymple on Differentiable Programming as the next frontier for Machine Learning Research: https://t.co/GSYi1017VQ RT @sujann143: Brett Cannon: Stop using Python 2.6 please https://t.co/L0xxn99h6g #Python #Django https://t.co/EOZGs3xOKD Very insightful talk by @redshiftzero at #32c3: Prediction and Control - Watching Algorithms https://t.co/cmwXhtu0BB #algorithmicfairness RT @peteskomoroch: Good example of why providing code + raw data is much better than passing along "cleaned" or "standardized" data https:/‚Ä¶ RT @debian: Debian mourns the passing of Ian Murdock: 
With a heavy heart Debian mourns the passing of Ian Murdock, stalwa... https://t.co/‚Ä¶ RT @ematsen: Sane #Jupyter notebook git diffs: https://t.co/twzmGqSe1k
 A simple text-only solution via nbconvert. @ivoflipse5 but this is another issue. @ivoflipse5 &amp; we can't generate windows wheels / conda packages for numpy / scipy because we don't have a fortran compiler under windows yet @ivoflipse5 also note that this is just for CI, not for end-users consumption. @ivoflipse5 both numpy and scipy were already generated wheels (for dev), all I added is the upload step. RT @DeepSpiker: Why ML needs Statistics by prof. M Welling
https://t.co/pmy7R7nLuw @rgbkrk if cloudfare object storage were supported by Apache Libcloud it would have been easier to try... @rgbkrk ok I might have a look at it later then. @rgbkrk arff. Is cloudfare expensive? I decided to use rackspace because of the nice complimentary account we got for scipy related projects @rgbkrk at least not with a CNAME: https://t.co/z9qEBW9MDg (bottom of the page) @rgbkrk It's a rackspace CDN container and HTTPS is not supported AFAIK. Both numpy and scipy now upload binary package for their master branch to https://t.co/yGE5tQjX4o (for CI purpose only). sklearn is next :) RT @nervanasys: Part 2 of our guest #blog on #deeplearning has been posted! Check out "Deep Reinforcement Learning with #Neon" here: https:‚Ä¶ RT @wesmckinn: New post: Interactive Analytics on Dynamic Big Data in Python using @ApacheKudu, @ApacheImpala, and @IbisData https://t.co/X‚Ä¶ @deliprao it might be very useful for some specific use cases though e.g. giving directions to a nav system / self driving car @aria42 @yoavgo out of curiosity which models do you have in mind? @yoavgo @aria42 maybe forget gate init? https://t.co/rnHjCbPAp7 RT @cartazio: huh, theres a libm maintained by the julia language folks, thats kinda neat, looks readable https://t.co/L7xb6SS2AC RT @dabeaz: Python's best feature is "conversational typing."  You sit at the REPL and type out a conversation with the problem you're tryi‚Ä¶ @fchollet I agree perfect mimicry is probably not possible without experience / embodiment but I don't think 'mimicry' it-self is the issue. @JorisPelemans @fchollet there was a workshop at nips 2015. Expect more in the coming year. Not true 'embodiment' but still. @lawrennd imagenet &amp; visual genome datasets labeling was probably costly but I think it does not require Google / Facebook infrastructure. @lawrennd not all AI relevant data is private and @karpathy has an excellent track record of producing labeled vision data at Stanford @fchollet I would replace 'none' by 'few', at least for multimodal embeddings (text + image + speech). @yaringal @chrisemoody vs current practices which include early stopping on validation set when useful. @yaringal @chrisemoody OK, I think it's still worth making it explicit. Otherwise one might think that BLSTM can improve the test error 1/2 @chrisemoody @yaringal interesting read but apparently dropout LSTM + early stopping performs better than BLSTM, right? RT @chrisemoody: Disciplined, Bayesian Dropout in RNNs. https://t.co/9nQrVjDP1a by @yaringal Better perf and comes w/ Theano snippet! https‚Ä¶ RT @shakir_za: A Year of Approximate Inference: Review of the #NIPS2015 Workshop. With @dustinvtran @mcinerneyj @lawrennd @matthoff https:/‚Ä¶ RT @nicolastorzec: Google quietly launched a Knowledge Graph Search API to search for entities matching constraints and get a few facts htt‚Ä¶ RT @haldaume3: arxiv paper from FB folks comparing language modeling techniques. on 1b words, Kneser-Ney ngrams still win. https://t.co/c0R‚Ä¶ @sergecell well it's the first time people use it on deep nets trained on imagenet to such an extent. RT @amiconfusediam: @sedielem @syhw @alexjc @ogrisel we asked Kaiming. It is before the addition. But it makes sense more to be after the a‚Ä¶ @syhw @alexjc i agree. Also resnets might also work for deep MLPs but this has not been investigated in the paper. How long before we can download pre-trained models from the model zoo? RT @PaulMineiro: Machined Learnings: NIPS 2015 Review https://t.co/kkgaKEFTek Residual Networks (ImageNet &amp; MSCOCO 2015 winners) seem to solve the underfitting problems of deep convnets: https://t.co/yrIqF2yvym #dlearn RT @fchollet: The Power of Depth for Feedforward Neural Networks - some clues as to why deeper is almost always better https://t.co/xzOqS5K‚Ä¶ RT @drfeifei: Visual Genome: a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language.  https://t.co‚Ä¶ @PierreTran attentionnel. D√©sol√© pour la confusion. On peut imaginer des m√©canismes intentionnels tels que : https://t.co/eUKqMDzWHn RT @hackernewsbot: Implementing a CNN for Text Classification in Tensorflow... https://t.co/TzjMch7Sfy RT @mappingbabel: OpenAI: "This collection of people is stunning" https://t.co/GAzzOcNfRC Ilya Sutskever, @karpathy , etc. Potential for hu‚Ä¶ RT @NandoDF: Rich Sutton locked out of the RL workshop at #nips2015 because it's too full :) https://t.co/VutJwIAYW0 RT @karpathy: ImageNet2015 results https://t.co/JWhWEvBsnS big congrats to @MSFTResearch team with 3.6% top5 error. 150 layers! looking for‚Ä¶ RT @graphific: PrettyTensor, Apache-licensed wrapper for #TensorFlow looks pretty nice and intuitive! https://t.co/0BZJTkrr0Y https://t.co/‚Ä¶ RT @johnmyleswhite: This made my day: https://t.co/uYPlHDOEEf RT @johnplattml: Cool learning of robot motion policies with physics trajectories and #neuralnetworks. Nice results. https://t.co/xMixhMSD1‚Ä¶ RT @egrefen: Here's the poster for our #NIPS2015 paper on Learning to Transduce with Unbounded Memory (https://t.co/ryrkFSoVBe): https://t.‚Ä¶ @economistress it can happen if you have very imbalanced classes. In that case accuracy is useless: prefer precision, recall, f1 or ROC. @karpathy it's unfortunately too big to upload the weights to the model zoo. @karpathy 100 trillion? https://t.co/sYaqjTcm74 RT @samim: Google¬¥s "Inception-v3" model released: https://t.co/NU6IUdpsZy https://t.co/xXjnd4l0XJ https://t.co/SNStc1fGKN RT @dribnet: chair math: CNN "Generate Chairs" paper updated, includes new section on feature arithmetic. https://t.co/xIEole5Ecg https://t‚Ä¶ RT @raymondh: #python news:  CPython 2.7.11 has been released.  Thanks to computed-gotos, you should get a nice performance boost.
https://‚Ä¶ RT @deeplearning4j: Better Computer Go Player with Neural Network and Long-term Prediction
https://t.co/fB32CQwfcA #deeplearning RT @samim: Attribute2Image: Conditional Image Generation from Visual Attributes: https://t.co/Suz9i0SWr8 https://t.co/d95SNmymYe @glouppe I am going RT @karpathy: Squeezed in an hour to create this year's NIPS 2015 papers in nice LDA format https://t.co/MVQRhtnOwq excited for the confere‚Ä¶ @asmeurer @pmhobson maybe this can help: https://t.co/UINZAa8sCw RT @sdouche: Agg: Parallel aggregations for PostgreSQL #PostgreSQL - https://t.co/MKvualmfSh RT @fchollet: Just released Keras for TensorFlow. Release post and comments: https://t.co/SoOKGlfwGw --Get the new version here: https://t.‚Ä¶ Nice overview by @heuritechdata on learning multimodal embeddings: https://t.co/JHoXbW6icE @CharlesOllion @karpathy you would have to retrain or fine tune the STV model because of the specific technical vocabulary though. RT @t3kcit: If you're at #strataconf Singapore, join me for my interactive #scikit_learn tutorial in room 331 at 9. Material at https://t.c‚Ä¶ RT @AlecRad: https://t.co/oYGRRDjkVI Nice and simple regularizer for RNNs with some promising experimental results. @deliprao @haldaume3 the mode? RT @adriancolyer: Adding async communication to Apache Spark to support a new generation of machine learning algorithm implementations http‚Ä¶ RT @yoavgo: I'm not quite clear on the connection between title and content, but nice slides about RNNs+Stack+Attention+Lang.
https://t.co/‚Ä¶ @fchollet and maybe mxnet as well? @bortzmeyer mais bon ok tant mieux si la d√©riv√©e second est n√©gative constamment sur une longue p√©riode. @bortzmeyer elle compare les d√©riv√©es premieres √† 1an d'intervalle Je sais pas si la courbe est assez r√©guli√®re pour en d√©duire la curvature @bortzmeyer @rubin d√©riv√©e premi√®re dans ce cas. RT @CharlesOllion: #DeepLearning Workshop 3 on attention models on Saturday 28/11 sponsored by @heuritechdata. Expect Theano, Keras, Torch,‚Ä¶ RT @fchollet: All of Keras including RNNs is now running both on TensorFlow and Theano. The rest is polishing. I expect to release in a few‚Ä¶ RT @fastml_extra: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs):
https://t.co/ogld7dX0ef

Reddit:
https://t.co‚Ä¶ RT @AlecRad: Another really good extension of GANs - https://t.co/slozzEkMP1 - Will be interesting to combine and see how they do together! RT @carlystrasser: @ProjectJupyter is hiring two postdoctoral fellows! https://t.co/cvWXjjbScn #MooreData RT @syhw: @AlecRad @amiconfusediam every paper should have a TL;DR written like that https://t.co/yGmt7nBn2X ;-) RT @moritz_stefaner: "NeuralTalk and Walk" by @kcimc w/ code from @karpathy shows us a glimpse of the near-future machine-readable world ht‚Ä¶ RT @shakir_za: New post. Machine Learning Trick of the Day (5): Log Derivative Trick. Score fns, stoch grad estimators, connections https:/‚Ä¶ @pavelnaiya @neilkod it's a very old video and the API in scikit-learn has changed. You can find more recent tutorials on YouTube. @stonebigdotdot @benhamner it's fine to be compatible with python 2.7, the point is starting a new package that python 2.7 only. RT @StatsPapers: Reducing Overfitting in Deep Networks by Decorrelating Representations. https://t.co/2IP2pcTSvW RT @benhamner: Every time I see software that only supports Python 2.7, I'm reminded of this Monty Python scene https://t.co/aTgHInwVbo RT @jasonbaldridge: New paper! "Character-based Neural Machine Translation" by Ling, Trancoso, @redpony &amp; Black. https://t.co/6tqP5mmn4Y #N‚Ä¶ RT @TorchML: [blog] Generating faces via Adversarial Networks. Comparison with variational approaches.... https://t.co/JzX33Aw3xA https://t‚Ä¶ @mjambon but it's still hard / impossible to predict ahead of time. @mjambon not necessarily better fitness but expected ratio is still non zero in the long run if fitness neutral for the species. @mjambon gmo is an irreversible change to the environment. If it has a bad consequences, there is no way to remove the bad genes. RT @fchollet: All of Keras except RNNs is now running seamlessly on TensorFlow and Theano (more tests needed, though). I expect to release ‚Ä¶ RT @RideImpala: Impala 2.3 is out! Includes support for nested types on @ApacheParquet, col-level security, and other goodness. https://t.c‚Ä¶ RT @moustaki: https://t.co/pGzudgRcXb Stacked Attention Networks for Image Question Answering (from Alex Smola's talk at #mlconfsf) @atelierhide it was a pleasure to meet you too! RT @haldaume3: Chris Manning on the "deep learning" tsunami that will hit NLP; has actual linguistic examples! https://t.co/FKhcCavUr1 RT @petewarden: "A yellow school bus is walking across a green grass field" Generating images from captions: https://t.co/h1pcxaH6gz https:‚Ä¶ RT @petewarden: How Android can use deep learning to continuously listen for "OK Google", while using almost no battery power: https://t.co‚Ä¶ @karpathy very interesting and entertaining story that reminds me of tweets of @petewarden IIRC. @karpathy typo: ablation studies RT @karpathy: New blog post: Shaking things up a bit with a short story on AI, "A Cognitive Discontinuity" https://t.co/tiStoOBCi8 RT @AlecRad: Vector arithmetic for faces - all unsupervised :) https://t.co/emaM63bdDn @alexjc @AlecRad i would also vote for gen as the eyes colors and shapes do not seem to be matching although heterochromia is quite common. @zaxtax @IgorCarron @GaelVaroquaux yes RT @robertmhaas: Parallel Sequential Scan is Committed! https://t.co/SxA2T7kFvI RT @AlecRad: Samples after one epoch on 2.7M LSUN bedroom samples suggests high visual quality is not just overfitting. https://t.co/XekfbK‚Ä¶ RT @beaucronin: OK, my hot take on why TensorFlow matters. What do you think? https://t.co/6J209TmB30 @egrefen @dwf @edersantana @hugo_larochelle https://t.co/Pry9OzCmY5 RT @syhw: "Autograd for Torch" has arrived https://t.co/vHpktj9HyM Thanks to the people from Twitter Cortex! @t3kcit almost ;) @t3kcit thanks for making it happen! RT @t3kcit: Scikit-learn 0.17 is out via pip and conda! changelog: https://t.co/G1JFjUMIOP LDA topic models, faster NMF, faster trees, fast‚Ä¶ RT @tarek_ziade: Now I am going to have this GIF in mind when running Python apps :) https://t.co/dao6mLaibD RT @teoliphant: The new Numba release is awesome!  parallel vectorize plus ahead of time compilation, object-code caching and more:  https:‚Ä¶ RT @karpathy: Google will begin rolling out automated replies in Gmail, based on encoder decoder LSTMs https://t.co/mDhWEDXNrn "Gmail autop‚Ä¶ RT @ContinuumIO: We're pleased to announce #Anaconda v 2.4 - with #Python 3.5 installers! See details here: https://t.co/G6NXqgZjcs https:/‚Ä¶ @vnfrombucharest or Adam / adadelta. @vnfrombucharest no pbm i was just curious about the poisson loss. @vnfrombucharest which paper did you use as reference for the implementation? RT @vnfrombucharest: Fast factorization machines with Poisson loss using Numba in surprisingly few lines of code. https://t.co/9RYo0ZWPmX RT @kdnuggets: Getting Uncomfortable with #Data: #Algorithms can hurt #humans - it is our job to mitigate the harm @dtunkelang https://t.co‚Ä¶ RT @alexip: My latest post: RIding on Large Data with scikit-learn https://t.co/pO0it2hvQ1 on the @OpenDataSci blog #OutOfCore https://t.co‚Ä¶ RT @tacaswell: #matplotlib 1.5.0 is out!

 See whats new: https://t.co/WB9az4y3Tr  

Wheels for mac+win+source on pypi, official conda comi‚Ä¶ RT @jseabold: Anyone try out the embedded Python/NumPy stuff in MonetDB? Looks super interesting. https://t.co/glDE7RQsjq @CharlesOllion I think you will like that paper (previous tweet). Traversing Knowledge Graphs in Vector Space by Kelvin Guu, John Miller, Percy Liang https://t.co/LnL3VvpR3p #metriclearning #embedding @pwang @rabernat @mrocklin but yeah going for cluster-first is premature 'optimization' in many cases. @pwang @rabernat @mrocklin you might still be bandwidth limited compared to a cluster for emb parallel workloads on partitioned data RT @ylecun: Research cannot be done in secret. 

Perhaps technology integration and product development can (at least for a... https://t.co‚Ä¶ @quasiben @mrocklin great! Hope the read-only FDs trick will help with data and memory locality then! @quasiben @mrocklin are you working on better tooling for deploying python code on mesos and yarn? @quasiben @mrocklin no idea. @mrocklin I agree, but on real corporate clusters with kerberos and ACL you won't be able to do so. RT @thefreemanlab: visualization of the notebooks running via binder -- literally watching reproducibility in action! https://t.co/n27dpHR7‚Ä¶ @mrocklin s/Dickerson/sockets/ (fat fingers + auto correct) @mrocklin check REQUEST_SHORT_CIRCUIT_FDS and https://t.co/zh4bC8ALLY @mrocklin I think it uses UNIX Dickerson the data nodes. if you collocate the workers there launched by same user via yarn that might work @mrocklin to get the data while respecting the ACLs it should be possible to use the hdfs cache that can pass memmapable open fds RT @mrocklin: #PyData on HDFS without Java

https://t.co/ylDnB8Xo1Z https://t.co/J8Fz17Mixr RT @mrocklin: Ad-hoc distributed computation: https://t.co/qksL94NtSQ
New blogpost on distributed prototype. RT @frenchdata: How @heuritechdata won the @SaclayCDS image classification challenge with #DeepLearning  https://t.co/wWec1zBrkq https://t.‚Ä¶ RT @iskander: ICLR 2016 submission is open: https://t.co/U76s5iOkfN RT @bigdata: #pydata at #stratahadoop Singapore: @wesmckinn of Ibis &amp; #pandas https://t.co/zByN5I50FW + @t3kcit of @scikit_learn https://t.‚Ä¶ RT @adampaulcoates: Check out this compendium of optimization tricks for Recurrent neural networks from @erich_elsen  http://t.co/obWD7CHj53 @mblondel_ml @fulmicoton @cournape je suis √† Kyoto du lundi 2 au vendredi 6. Si t'es dans le coin on peut boire un verre :) RT @fulmicoton: @ogrisel is doing a talk about scikit-learn at Indeed Tokyo https://t.co/vDzAfmS8pz  ! @rgbkrk @cournape yeah maybe it's not worth the trouble. Would have been fun though ;) @rgbkrk @cournape we will arrive in Kyoto afternoon... RT @jshrsn: spark-redshift 0.5.2 is now available and includes some important bugfixes and documentation improvements: https://t.co/e35jGjZ‚Ä¶ RT @ncoghlan_dev: Will be interesting to see where this goes. Lots of projects where even a single paid dev can make a big difference. http‚Ä¶ End of the sklearn sprint. Many improvements to the codebase. Met new nice contributors. CI is exhausted. Thanks @criteo for hosting us! @GroupeRATP ma rame ligne 13 en direction de chatillon secoue √©norm√©ment (oscillations horizontales) au niveau de Malakoff. RT @karpathy: My dream finally came true: Yelp has discovered ConvNets https://t.co/NoZoNhKY5A So much value for so little cost. Strangely ‚Ä¶ RT @jseabold: smart_open: Utils for streaming large files (S3, HDFS, gzip, bz2...) from @RadimRehurek https://t.co/KpJiIZ6WPY @mblondel_ml @swagKumar yes @swagKumar https://t.co/6YsP4GUtlo Nice: unusual git pull stats when cleaning up some old deprecated code in sklearn master at the Paris sprint. https://t.co/7uFVCx6LMx RT @t3kcit: The Gaussian Processes in scikit-learn got a complete rewrite in the current dev version: https://t.co/8xkWE310IC Feedback welc‚Ä¶ RT @mrocklin: #dask + #sklearn experiment. Reuse intermediate results from Pipelines in parameter sweeps.

https://t.co/mXnZfe1WLm https://‚Ä¶ RT @balazskegl: Open Software @SaclayCDS, oct 26 with @fperez_org @t3kcit @agramfort. Full program and registration: http://t.co/ID3tlSm1aU Neural Networks with Few Multiplications http://t.co/czzfjrcMTx RT @nvaroqua: Want to meet #scikit-learn devs ? Go to the social event, Wednesday  21st, at Roy's pub, 7pm: https://t.co/HZHv6tiHdl @GaelVaroquaux ah OK sorry, it was truncated in the twitter ui. @GaelVaroquaux we should not point to SF but to pypi for downloads. SF has a bad track record and I would rather like to stop using it RT @t3kcit: Scikit-learn 0.17b pre-release was just pushed to pypi. Install with "pip install --upgrade --pre scikit-learn" try it out and ‚Ä¶ @ccomb vous √™tes o√π ? @cmdevienne je viens de voir ton message :) @DeepSpiker @shakir_za @hugo_larochelle I see thanks. @DeepSpiker @shakir_za @hugo_larochelle nuke everything that moves sounds like a good strategy towards that end, no? @shakir_za @hugo_larochelle @DeepSpiker correct me if I'm wrong: is do actions to destroy env until nothing moves an optimum of the obj fun? RT @hugo_larochelle: My notes on Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning by @shakir_za http‚Ä¶ @FFeth @pyconfr je cede le mien aussi. RT @ylecun: Awesome blog post by Nicolas Leonard from Element about a Torch implementation of a recognizer with an attention... http://t.co‚Ä¶ RT @GaelVaroquaux: joblib 0.9.2: Python dataflow jobs
- Parallel context manager
https://t.co/2MBMLpBO5a
- Automatic batching of short task‚Ä¶ RT @pypelson: ANN: conda-execute - allows you to define dependencies for your scripts and then run them in temporary conda envs https://t.c‚Ä¶ RT @kastnerkyle: Code release for VRNN (Chung et. al, including me!) https://t.co/ldCLxbNu2I paper is here http://t.co/zG4PKiKWFm RT @YhatHQ: Deep-Learning Robot Takes 10 Days to Teach Itself to Grasp | http://t.co/6Q3mqEjuh3 | #babyvsbot #DeepLearning http://t.co/VzXW‚Ä¶ RT @ch402: Ever wonder what information entropy or mutual information is? I've written a visual explanation! http://t.co/VdwPvqr5kV @benjguin @abolibibelot mais il faut attendre la video pour appr√©cier les slides √† leur juste valeur :) @DRMacIver &amp; a standard way to document the expected types of the arguments for human readers. @DRMacIver it's just syntax extension to allow for external type checkers to analyze the source (IDE, QA linters on CI, compilers...) RT @mxlearn: Yhat releases Rodeo 1.0: Free native Python IDE http://t.co/EBloeQYSj3 RT @oceankidbilly: Wow: @googlecloud announced Datalab, supporting data exploration on their platform using @ProjectJupyter! https://t.co/u‚Ä¶ RT @amiconfusediam: OpenFace: A full face recognition pipeline based on Google's system. https://t.co/aeSpFVS0mb RT @shakir_za: Slides from a talk I gave recently: Bayesian reasoning and Deep Learning. On the imp convergence of Bayes and DL. http://t.c‚Ä¶ RT @snazy: User-defined-functions in Apache #Cassandra 3.0 and beyond: http://t.co/WMEDlLE6rR RT @haldaume3: Last week, this ML system beat winningest Jeopardy champion Ken Jennings 300-160 in quizbowl. For 20 pts, name it! https://t‚Ä¶ RT @hugo_larochelle: Our ICCV paper, Describing Videos by Exploiting Temporal Structure, is out: http://t.co/uREc76Ufkj #DeepLearning RT @VictorStinner: I'm happy, I optimized ASCII, Latin1 and UTF-8 codecs with error handlers in the future Python 3.6: https://t.co/mk9lSMa‚Ä¶ RT @mrocklin: Critical feedback requested on prototype distributed computing project.  #PyData 

http://t.co/1H2YxJswiX
https://t.co/hmiu4a‚Ä¶ RT @mat_kelcey: "Character-Aware Neural Language Models" http://t.co/qZepSj19KZ h/t @nova77t @jeffbarr why python 2.7 for a new product in 2015? Why not python 3 first? RT @oceankidbilly: New Amazon EC2 X1 instance type has 2TB of memory: a whole new class of data problems is now accessible by in-memory too‚Ä¶ RT @TomAugspurger: We should type it jUpyter, just to annoy the iPython devs. @swagKumar we also need to finish the release of joblib 0.9.0 and sync it in sklearn. This is under way. @swagKumar we try to do as many as possible from: https://t.co/VQuvAucnvF RT @t3kcit: A text-book introduction to data science http://t.co/A2bAEKnXbj with Jupyter notebooks from @BerkeleyData. Uses simple table da‚Ä¶ RT @t3kcit: Pre-alpha patsy-sklearn adaptor https://t.co/WEfYK1zCaz  for  a formula interface (for feature engineering) with scikit-learn. ‚Ä¶ RT @rasbt: NumPy 1.10 is here with efficient and prettier matrix multiplications:  np.linalg.multi_dot &amp; the new "@" operator  https://t.co‚Ä¶ RT @amiconfusediam: faster convolutions on the way.... this time thanks to Terry....errr.. Schmuel Winograd! http://t.co/aKLRyNxZbO RT @GaelVaroquaux: The @EuroSciPy videos are online https://t.co/JbuLd28kP3 
Thanks to @enthought RT @Mbussonn: Dear tech conference organizers, I have too many-shirts, but limited supply of socks. Please help. #swag. RT @neil_conway: Another provocative statement in Google's Dataflow paper (http://t.co/aQeGvzyO7x); def a widespread design pattern http://‚Ä¶ RT @michael_erasmus: Cool post on anomaly detection for airbnb's payment platform http://t.co/J18ejuurP3 RT @deliprao: This is probably the most definitive (and only?) survey of DNNs for NLP. Well done @yoavgo. https://t.co/kvFr7RizS2 @Collinsjo12 @twiecki @t3kcit Never used InTrees myself. Would love to know if practioners find it useful on realistic tasks. @twiecki @strataconf @quantopian it's probably an issue on my version of twitter or drive for Android. It works if I manually open in chrome RT @awiltsch: This is interesting, large file storage at GitHub. Could do cool science data sharing with this! https://t.co/0bFRlPhiXh. cc ‚Ä¶ @twiecki I cannot open the slide deck: permission issue. RT @twiecki: Thanks to everyone who attended my @strataconf talk on Probabilistic Programming! Slides: https://t.co/ujfTec9PwQ @quantopian ‚Ä¶ RT @wesmckinn: My slides from #StrataHadoop NYC: "Ibis: Scaling Python Analytics on Hadoop and Impala" http://t.co/YHd3NXySQe #pydata RT @Theremina: The Google DeepDream Makeup Tutorial That Nobody Asked For http://t.co/vfhFl8BH7v http://t.co/iE0iIeYS7h RT @ds_ldn: @ogrisel great! thanks for sharing, check out MS Research WikiQA Question &amp; Answer Dataset http://t.co/ylk5BhbRAf Script to rebuild the Daily Mail &amp; CNN QA datasets from the internet archive by Google Deepmind: https://t.co/DzZUwHfs9H RT @ylecun: The MemNN code is available on Github.

The original MemNN code is in Matlab, while the newer end-to-end MemNN... http://t.co/d‚Ä¶ RT @abursuc: CelebA: a large-scale dataset of celebrity faces with attributes annotations -  10k celebs, 200k imgs, 40 attr/img http://t.co‚Ä¶ @teoliphant that's great news! Do you also plan to release the source of the conda recipes for XP builds of numpy / scipy / sklearn? RT @teoliphant: I‚Äôm also thrilled to announce that #Anaconda is also now 3-clause BSD licensed for everyone: http://t.co/hUM9SfGJrb RT @being_bayesian: Welcome the move to MIT license! RT spaCy #NLProc now available under MIT license! (formerly AGPL) http://t.co/RX1uHd4X‚Ä¶ RT @GaelVaroquaux: scipy lectures http://t.co/nEXvvCmRbR
New edition: mobile-friendly, Python 2 &amp; 3
Learn numerics, science, + data with Py‚Ä¶ RT @matei_zaharia: Caffe on Spark for deep learning from Yahoo!: http://t.co/C0CVVpRjAy RT @iskander: The layout of multi-dimensional arrays (what I spent like 1/2 of grad school thinking about): http://t.co/AXkEro42ZS @tarek_ziade congrats! RT @karpathy: Several ppl asked for my gaming/deep learning rig build. Here's the part picker spec I used: https://t.co/6s5T4VlVnY RT @t3kcit: Here's an early release of my @OReillyMedia video series "Advanced Machine Learning with scikit-learn" http://t.co/NEPeV5J2Tc F‚Ä¶ RT @cangermueller: Efficient Stochastic Neural Attention Models: http://t.co/Y8jSMJSsBC RT @TalkPython: Today I recorded an excellent episode on #scikitlearn with @agramfort. We'll have some sweet machine learning coming on #31 @fchollet @tttthomasssss still it would be useful to quantify the computational cost more often. RT @drewr: The reign of TF/IDF ends in Lucene 6.0 https://t.co/segv0LolHV @FFeth j'ai appris la nouvelle sur lemonde.fr @haldaume3 @egrefen but @yoavgo did it for word2vec vs factorized context distributions. He might do it again for DL w/ AM vs others ;) @yoavgo @haldaume3 my point is that not all latent variable models are classically directly trained via SGD (e.g. EM, variational inference) @yoavgo @kastnerkyle @haldaume3 @brendan642 Probably because it's easier to train with SGD derivatives. Non-differentiable need complex RL. @kastnerkyle @yoavgo @haldaume3 @brendan642 yet another interesting paper on entailment: http://t.co/wqUhAQvCxO (I have not read it yet) RT @chrisaballard: Looks like pandas will support threading on groupby operations in next release http://t.co/iZp7d0uWtL RT @ellisonbg: @ProjectJupyter is going to be organizing JupyterDay hackathons in NYC in October and Chicago in ~Feb. Please ping us if you‚Ä¶ @yoavgo @haldaume3 I think it can be made reusable. Though not part of generic end-user frameworks such as keras and lasagne yet. @yoavgo @haldaume3 actually I am mistaken: neural reasoner implements something different from the usual soft attn structure. @yoavgo @haldaume3 from a coding POV it's a paramterized module you plug in the architecture's graph. Then do backprop on the usual cost. @yoavgo @haldaume3 also sometimes the attn is conditioned on the context (e.g. question for QA) otherwise on the last sample of the decoder @yoavgo @haldaume3 also there is attn on rnn embeddings of words for MT, of sentences for QA and CNN spatial regions for captioning. @yoavgo @haldaume3 there is a common structure in soft attn models but some are deeper, eg: http://t.co/CuIb7XxNuh http://t.co/N2IVhDSQo7 @haldaume3 @dwf hard attention mechanisms don't make this assumption but are seemingly harder or more expensive to train. @haldaume3 @brendan642 AM is a broader prior than POS tagging, chunking &amp; parsing preproc and does not require fine grained supervision. @brendan642 @haldaume3 even more so if it allows training end to end with weaker supervision. @dwf @haldaume3 interesting question. Which experiment could be devised to confirm / infirm this hypothesis? @haldaume3 @dwf not sure if it would be a win. Depends on the resulting model size / fit &amp; inference speed. @haldaume3 @dwf deep lstm seems to have troubles on bAbI. Though I wonder if this can be fixed with a question reconstruction auxiliary obj. @dwf @haldaume3 to me it's more like an inductive biais like 'you are encouraged to not memorize everything and you should ignore clutter' @dwf @haldaume3 which BPTT woes can attmech shortcircuit? Vanishing / exploding gradients? RT @ylecun: Excellent post by Nicolas Leonard from Element-Research about visual attention mechanisms and the REINFORCE... http://t.co/22cW‚Ä¶ RT @tinysubversions: Proud to announce The Ethical Ad Blocker! It solves all the messy issues around ad blocking: http://t.co/tCKX1L5oZC RT @jakevdp: Wow! Over the weekend my "Statistics for Hackers" slides had 45,000 views... maybe I should give the talk again! https://t.co/‚Ä¶ RT @mrocklin: @ogrisel @t3kcit note that this rough draft has moved to https://t.co/SjL87JnGYk .  Critical feedback welcome. RT @balazskegl: Deep learning TS (Oct 6) and insect classification RAMP (Oct 8) @SaclayCDS @PROTO204 http://t.co/Lh8rM1yYTe http://t.co/Ol9‚Ä¶ RT @jedisct1: Introducing Brotli: a new compression algorithm for the internet http://t.co/PKrWCENgTv @F_Vaggi @t3kcit and it's fast because it can reuse intermediate computation naturally. @mrocklin should blog it soon I think. Prototype dask-based implementation of scikit-learn pipeline and parameter search by @mrocklin and @t3kcit: https://t.co/VZJT51iPY6 @VolodymyrK @meickenberg I too would like an offline / mobile video player. RT @oceankidbilly: Columnar Compression: slides and quick thoughts from my very lightning-ish talk at #ds4ds last week: https://t.co/9Qdxpr‚Ä¶ @AlecRad what is the difference with previous attempts? Larger model? Different optimizer? RT @AlecRad: Samples from conditional convolutional GAN on CIFAR-10.
*Might* be overfitting - but that's a good problem to have? http://t.c‚Ä¶ RT @wesmckinn: Huzzah! @datoinc C++ data structures released under BSD license: https://t.co/ygQ2UZ5XPV RT @mdreid: Ben-David &amp; Shalev-Shwartz's book "Understanding Machine Learning: From Theory to Algorithms" is now free to download http://t.‚Ä¶ RT @wesmckinn: Slides from BIDS today: "Data Science Languages and Industry Analytics" http://t.co/gZccjDcHez #pydata #rstats #julialang http://t.co/r44DakNGRC can turn any github repo with jupyter notebooks as dockerized hosted service (for free ;) by @thefreemanlab et al. RT @meickenberg: Videos up of Deep Learning Summer School Montreal 2015 http://t.co/P7edAdBCNe RT @wesmckinn: Wish you could quit writing SQL? Me too. Presenting "Ibis for SQL Programmers", a comprehensive guide http://t.co/3Ap0TvRk0d‚Ä¶ RT @indicoData: Great read by Robert Chang on doing Data Science at @twitter #goodread #DataScience http://t.co/A1P47vwAOS http://t.co/6i7R‚Ä¶ RT @Maciej_Kula: We've open-sourced LightFM, a Python hybrid recommender for item and user cold-start https://t.co/o47Ajf9JhY #recsys2015 @‚Ä¶ @AlecRad you are exploring a new dimension of the uncanny valley. RT @AlecRad: THE PEOPLE ARE IN THE COMPUTER 
(128x128 generated samples from ~ 375k faces) http://t.co/Fcm58ibtdb RT @twiecki: Analyzing 1.7 Billion #Reddit Comments with #Blaze and #Impala http://t.co/4uN6lvHJTa RT @wesmckinn: I started assembling open data sets in binary formats (like @ApacheParquet): http://t.co/54mziLsKUs  . Feel free to contribu‚Ä¶ RT @ahoy_jon: And the slides for tonight for @DataParis : http://t.co/vbCRMeXikF @J_ @ahoy_jon Jon correct me if I got it wrong ;) @J_ @ahoy_jon the use of pig cogroup to nest data early, store to parquet and then avoid shuffling when querying downstream in the pipeline Rereading @J_'s post on Parquet / Dremel nested columnar data structures in light of @ahoy_jon's talk at @DataParis: https://t.co/aFYbPfii5z @ahoy_jon ping me if you do? @ahoy_jon very interesting talk yesterday. Do you plan a blog / google+ post in English to summarize your message to a wider audience? @CharlesOllion and @meickenberg :) RT @CharlesOllion: Deep Learning Paris Meetup #4 registrations open http://t.co/Os8bz8FhXt with speakers @ogrisel @Moodstocks and @heuritec‚Ä¶ RT @rsalakhu: Yura Burda is releasing the code for training Importance Weighted Autoencoders (pronounced i-ways). Enjoy!
https://t.co/G2njo‚Ä¶ RT @rsalakhu: Ryan Kiros is releasing the code for training skip-thought vectors for learning useful sentence representations:
https://t.co‚Ä¶ RT @fastml_extra: Scalable Distributed DNN Training Using Commodity GPU Cloud Computing - PDF:
https://t.co/KfjUj2Cjtp @mblondel_ml any code available? RT @mblondel_ml: Slides of my talk at University of Cambridge on convex factorization machines http://t.co/lvNiUBOqyz and paper http://t.co‚Ä¶ RT @minrk: `conda create -n py35 python=3.5 notebook` @ContinuumIO is quick! RT @hugo_larochelle: My notes on Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks:
https://t.co/8JLJSo6HJD
 #DeepL‚Ä¶ RT @amiconfusediam: google releases their imagenet inception model pre-trained weights: https://t.co/ClhD5WERC5 RT @shakir_za: New post: Machine Learning Trick of the Day (3): Hutchinson's Trick. 
On scalable ML with randomised linear algebra
http://t‚Ä¶ @Collinsjo12 @oceankidbilly but postgresql does not seem to have support for associative UDAF that would allow for generic MPP workloads @oceankidbilly they do not mention associative aggregate functions at all (yet). RT @oceankidbilly: (Scalar) Python UDFs in Amazon Redshift: http://t.co/exqSHQ64XC @oceankidbilly @wesmckinn that would make me very happy too. Hope postresql &amp; @citusdata will follow too. RT @maxogden: announcing `npm install linux -g` - installs tiny linux on Yosemite in under a minute https://t.co/rjVMyjzePs http://t.co/wns‚Ä¶ RT @minrk: Surprised to see new projects list 'python 3 support' as an early goal, rather than step 0. Py2 should be the second-class case ‚Ä¶ @cangermueller the second link is dead. RT @cangermueller: Curriculum learning improves sequence prediction with #RNN: http://t.co/oBimfjaUah, https://t.co/wQ4Mhi6RnK, #deeplearni‚Ä¶ RT @fbOpenSource: Releasing two extensions to the Torch scientific computing library: ZTorch and LuaFFI-FB. https://t.co/FuKG8bXi9M &amp; https‚Ä¶ RT @t3kcit: Only 5 days left to submit your talk proposal to PyData NYC: http://t.co/VCnRQKpTEh RT @wesmckinn: New post: Analyze the Crunchbase dataset in the next 5 minutes with Ibis and SQLite http://t.co/Oq9mH0i4Mr #pydata RT @wesmckinn: Ibis 0.5 is released! SQLite support, Python 3 compatibility, and much more http://t.co/vdH464vRvg #pydata RT @t3kcit: Spark 1.5.0 adds CountVectorizer, MinMaxScaler (yes these are the names) http://t.co/xTub6CisMi :D RT @ylecun: Our paper on character-level ConvNets for text classification was accepted to NIPS. The paper is available on... http://t.co/wp‚Ä¶ RT @abursuc: MegaFace: a new 1M CC Flickr faces dataset and challenge for recognition at scale http://t.co/ZnivjtFktB RT @karpathy: Nervana updated their neon Deep Learning framework https://t.co/alLUJiANo7 , also includes Image Captioning demo 200x faster ‚Ä¶ RT @TorchML: @Moodstocks write about spatial transformer networks, 1 STN beats a committee of 25 CNNs ... http://t.co/OfvY7sb9sf http://t.c‚Ä¶ RT @glouppe: Great team work going on in scikit-learn to make trees and boosting faster! https://t.co/9MrUoVOsui and https://t.co/X0kTeiwM6‚Ä¶ @AlecRad this one was censored by YouTube apparently. Overfitting == copyright violation? RT @egrefen: Good work from Yao et al: passing LSTM-like properties along depth in deep RNNs as well as along temporal dimension
http://t.c‚Ä¶ @stefanvdwalt I am sure it did :) @stefanvdwalt you are no longer on stage to look beautiful? RT @danilobzdok: Our NIPS-paper on #semisupervision and #representationlearning in #fMRI http://t.co/byPWlfy6Iy @meickenberg @ogrisel @Gael‚Ä¶ 82.90 NLL on MNIST! https://t.co/3GkoXvjY5i @Cmrn_DP shall python core dev try to get rid of the GIL ;) RT @marcua: Also, I wrote up a blog post summarizing our VLDB paper on Argonaut: Processing Complex Work with the Crowd" http://t.co/CXCVLC‚Ä¶ RT @kastnerkyle: @kcimc great writeup https://t.co/b6BdZoGWNy . Thanks for summing it all up succictly. Who would've thought L-BFGS was the‚Ä¶ RT @ClouderaEng: Dynamic Progress Reports in the Impala Shell http://t.co/OJXGxV661W (via @grundprinzip) LLVM 3.7 is out with builtin OpenMP 3.1 (&amp; partial OpenMP 4) support: http://t.co/P2jQAAWoeZ @hugo_larochelle your notes summarize exactly my feelings about that paper. Thanks for sharing them. RT @hugo_larochelle: Notes on Towards Neural Network-based Reasoning https://t.co/Q5NZFt4OXB . 
A take on reasoning different from Memory N‚Ä¶ RT @scaleway: We‚Äôre slashing the C1 price by 70%, get your C1 for ‚Ç¨2.99/mo or ‚Ç¨0.006/h! This applies right now. https://t.co/eg44nuS7vv @CharlesOllion @syhw did you install cudnn to get a speedup? RT @CharlesOllion: playing around with Neural Artistic Style http://t.co/ueLuPtZKXK and obama on our EC2 instance @ogrisel @syhw http://t.c‚Ä¶ @teoliphant @wesmckinn @sqlalchemy the will happen before the end of september I think ;) RT @wesmckinn: Just merged the initial @sqlalchemy backend into Ibis! Support for more SQL engines getting closer https://t.co/rbelysp9pf #‚Ä¶ RT @karpathy: Gill Pratt (PM @ DAPRA) on "Is a Cambrian Explosion Coming for 
Robotics?" http://t.co/K9k1LJsDTJ [pdf] about trends in robot‚Ä¶ RT @amiconfusediam: the latest round of convnet benchmarks: https://t.co/j0hHo2igFr @AlecRad @karpathy @Luke_Metz I am really interested in understanding the training dynamics of attentional models better. RT @hugo_larochelle: My notes on Semi-Supervised Learning with Ladder Network: 
https://t.co/MumFvKC66g
Impressive results of unsup learnin‚Ä¶ @thefreemanlab @mrocklin why? Is it because of the complexity of the format? RT @EGouillart: ReScience: a new journal to publish implementations of algorithms already published. https://t.co/8IQZB2tpoM @ReScienceEds ‚Ä¶ @mrocklin it looks nice BTW, this the kind of tech pydata needs. @mrocklin how is it different from existing formats like Parquet and ORC files? Why introduce a new persistency scheme? RT @benhamner: Efficient tabular storage: nice post by @mrocklin. Love the work that @ContinuumIO's supporting for the Python &amp; data http:/‚Ä¶ RT @evolvingstuff: Inferring algorithmic patterns with a stack augmented recurrent network http://t.co/KZz5EwAASy http://t.co/1H1y1NBn6a RT @kdnuggets: How to create #Prediction intervals for #RandomForests #rstats #DataScience http://t.co/jeBRdOpbo7 http://t.co/SoUr8kNEVo RT @t3kcit: Duecredict: interesting project to derive the right citations for all methods from your code: https://t.co/YHEUSuWdVM nifty ide‚Ä¶ RT @yaringal: Nicest ML generated art I've seen for quite some time! http://t.co/dnBYlKf9QF http://t.co/zmU9Yw9u5x RT @surendra_sedhai: Understanding LSTM Networks http://t.co/vRSMUyIVSx RT @samim: New Instrument Captures the Secret Lives of Cells: http://t.co/ZwUh4wsutL incredible video! RT @jedisct1: For all your APIs, provide simple, self-documenting code examples for common use cases. Then, document the gory details. RT @jedisct1: As highlighted in Google‚Äôs study on code search, developers are looking for code examples. These are more important than text‚Ä¶ RT @tqchenml: A self-contained and comprehensive introduction to boosted trees  http://t.co/k15UGC8iOd   #xgboost RT @AlecRad: Moar pixels! Samples from 128x128 generative model of ~700K album covers. http://t.co/ttsaDd979n RT @ylecun: Project M: AI assisting humans training AI.

M is a digital assistant service rolled out to a small number of... http://t.co/Mo‚Ä¶ @petewarden I am also looking forward to seeing how https://t.co/NggztpuGPv and https://t.co/IvPRLCA7Y5 get adopted. @petewarden hopefully nvidia and AMD with also pack more 8 and 16 bit integer SIMD operations for deep learning in their next gen chips. @DataScienceLA @benhamner @kaggle if you really want to use one-hot-encoding, maybe a conversion to scipy.sparse.csc_matrix will help. @DataScienceLA @benhamner @kaggle for sklearn you should definitely use integer coding for categorical variable as the default RT @GaelVaroquaux: Material for my "Statistics in Python" tutorial @EuroSciPy http://t.co/MzeQ9Z9dZL

Big picture + simple things, hopefull‚Ä¶ @GaelVaroquaux why oh why Python 2 only?... in 2015... :) @glouppe congrats! @petewarden any idea if https://t.co/nkKr6f5G8b will be ported leverage the AVX-512 8bit ints of future Xeon Phi architectures? RT @hugo_larochelle: My first shared notes, on Accelerating Stochastic Gradient Descent via Online Learning to Sample: https://t.co/h3HPx4w‚Ä¶ RT @karpathy: Worth checking out new Deep Learning library for Python "CGT replicates Theano's API but has very short compile time" http://‚Ä¶ CGT: a new contender for theano (and torch &amp; caffe): http://t.co/QvMDNTOIOv CGT End-2-end trained deep architecture Neural Reasoner can solve path finding and positional reasoning babi tasks: http://t.co/N2IVhDSQo7 RT @AnimaAnandkumar: I am part of organizing committee for NIPS workshop on non-convex optimization. Welcome submissions/participation http‚Ä¶ Intuitive intro to logistic regression: http://t.co/B6TQxd44um #machinelearning @fchollet congrats! RT @garybernhardt: Just-in-Time Summoning of Unikernels http://t.co/axrzJK5xAE RT @sdouche: A New Linux File-System Aims For Speed While Having ZFS/Btrfs-Like Features #linux - http://t.co/3BxMLp1xIp @wesmckinn Ctrl-4 if you started the program with bash. @Tim_Dettmers the aws spot prices for g2.2xlarge have increased a lot in mean and variance since end of July for some reason though... @fperez_org pickling is fine. Unpickling is frightening ;) RT @twiecki: #PyData NYC 2015 announced http://t.co/M3E3UIu6yp PMML exporter for scikit-learn: https://t.co/D2bRVYOy1f (MIT licensed) #machinelearning Reasoning, Attention, Memory (RAM) NIPS Workshop 2015 http://t.co/yProZ4l3k2 organized by FAIR with talks by Brain, DeepMind, UMontreal, NYU WER are we: https://t.co/kKgQ71lhG1 @syhw tracking the state of the art of speech recognition systems #dlearn RT @dnouri: A Spatial Transformer Network made with Lasagne:
https://t.co/bmlFipHPzh @bat__go il faut choisir, soit abonnement, soit pub. Je me suis abonn√©. @CMastication thanks for the feedback, glad you liked it :) Non-smooth, non-finite &amp; non-convex optimization. Lecture slides by Mark Schmidt at Deep Learning Summer School 2015 http://t.co/kRM3AIfnEQ @swagKumar thanks! RT @aria42: Really fantastic paper "Compositional Character Models for Open Vocabulary Word Representation" http://t.co/NnEqaQkIBG h/t @red‚Ä¶ RT @kaiblin: Who creates CSV files with some fields in utf-8 and some in iso8859-1? WHO??? RT @kdnuggets: Baidu explains how it‚Äôs mastering #Mandarin with #deeplearning  http://t.co/SjJMCkEzAR http://t.co/PdKNZW2jZj RT @ylecun: Announcement for the Deep Learning Symposium at NIPS. http://t.co/MnWRuKrNqp RT @wesmckinn: New post: Leveraging SQL window functions in Ibis http://t.co/cnwpLoeu98 #sql #pydata RT @ssdd3: Great drawing of neural networks done by @kastnerkyle http://t.co/lIvQPdU3cC RT @t3kcit: Variational Latent Dirichlet Allocation was just merged into scikit-learn master: http://t.co/xNBpBVP06h https://t.co/rMBu1pfSy‚Ä¶ RT @jakevdp: New post: Out-of-Core Dataframes in Python: Dask and OpenStreetMap https://t.co/4X8d4JiLq2 RT @wesmckinn: Ibis 0.4 is out, read more here: http://t.co/KIlllQ5VK3 RT @_onionesque: How do hyperparameters in ConvNet (and generally NN training) interact? Some good experiments and rules of thumb here http‚Ä¶ RT @NatureNews: Scientists film neural activity across entire brain &amp; central nervous system of fruitfly larva http://t.co/aNTczDC1Y7 http:‚Ä¶ RT @sedielem: Lasagne 0.1 is released! PyPI: https://t.co/0aQk65pMGE GitHub: https://t.co/7FZx4j18FS + it is now citable! https://t.co/Lz8T‚Ä¶ Roofline analysis &amp; arithmetic intensity of deep learning related computations presentation by @adampaulcoates http://t.co/QFara3NLxS I enjoyed the Deep Learning summer school very much. Highlights: generative models, attention, differentiable memory, optimization issues. @silvercorp i have no experience with vfdt unfortunately. @jasonbaldridge modeling everything at the character level is likely to be too expensive / wasteful. @jasonbaldridge looking forward to multiscale models: e.g. character / word / sentence sequential models RT @StephenPiment: #IPython 4.0 released: the last piece of the first release of Jupyter and IPython after The Big Split. http://t.co/DZXby‚Ä¶ RT @rsalakhu: Slides from my deep learning tutorial:
http://t.co/usKoGVKtI6
http://t.co/QwAakU5wp3

Slides from all other talks:
https://t.‚Ä¶ RT @kastnerkyle: Some really great slides on optimization from Mark Schmidt http://t.co/zpFTwLQZM8 RT @sedielem: RNNs / LSTMs have finally arrived in Lasagne! http://t.co/pP3KseegG8 Excellent work by Colin Raffel, S√∏ren S√∏nderby et al. RT @dnouri: Find a good CNN architecture, plot activities and occlusion sensitivity. New nolearn-based tutorial explains how to: http://t.c‚Ä¶ RT @dnouri: U-Net: Convolutional Networks for Biomedical Image Segmentation
http://t.co/AEalVXeZC4 http://t.co/ILgvGhRSGL RT @abursuc: The neural networks behind Google Voice transcription: from GMMs to LSTM RNNs http://t.co/GJDcyD1kXK RT @EuroSciPy: Arrival info for participants posted https://t.co/gjoSj8Y5NT - Also, plenty of beginner track seats left to learn #scipy in ‚Ä¶ RT @ProjectJupyter: Give this a try: `pip install --upgrade jupyter` RT @yoavgo: What can you tell me about the equiv of RNNs and Turing Machines? Is there a proof that is not based on the "2-counter machine"‚Ä¶ RT @t3kcit: I'm happy to announce that I'm joining forces with @sarah_guido on 
Introduction to Machine Learning with Python" http://t.co/O‚Ä¶ @petewarden thanks! @petewarden the 8bit gemm is great news. Is batch normalization necessary to leverage it robustly in deep nets in your experience? RT @petewarden: Five Deep Links - http://t.co/ntFPBn0JhS - Musical RNNs, visual translation, 8-bit GEMMs, GoogLeNet visualizations, and sel‚Ä¶ RT @AlecRad: Sure it's MNIST, but wow, ladder networks can outperform old NNs on all of it with only 100 sup. training examples http://t.co‚Ä¶ @fulhack format == UX? RT @Chris_Said: Guy makes lovely classical music neural net, with blog post + code:
Top Hacker News comments: "Not as good as Bach!"
https:‚Ä¶ RT @fastml_extra: Question answering on the FB bAbi dataset using LSTM and 175 lines of Python + Keras:
http://t.co/KdKhQoZeXY

Codez:
http‚Ä¶ RT @chrisemoody: Deep Learning Compared: #Theano vs #Torch vs #Caffe vs #Chainer http://t.co/RBxgTGzI96 courtesy of @ChainerOfficial http:/‚Ä¶ RT @wesmckinn: New Post: Using other compute engines with Ibis http://t.co/BIQ50cmVTc #ibis #sql #pydata RT @glouppe: Finally! Wrap custom function as scikit-learn transformers https://t.co/mJlXTgvpWl @wesmckinn any equivalent for parquet? RT @wesmckinn: Oh, wow, Python bindings for Avro C! https://t.co/KmURL9qsuX RT @rjurney: What is it like to work hard on an academic paper, then release it without code so it will never be used? @Mbussonn @rgbkrk you mean the type system should catch all the bugs at compile time? RT @StatFact: Today's posterior becomes tomorrow's prior at midnight. RT @sedielem: Spatial Transformer Network implementation for Lasagne by Soren Sonderby https://t.co/3bbyVOWKnx just plug a TransformerLayer‚Ä¶ RT @njgoldbaum: @wesmckinn I use "with nogil" habitually for cdef functions because it gives me compile-time checking for most Python inter‚Ä¶ RT @fchollet: 1st &amp; 3rd place entries in the CrowdFlower competition were based on XGBoost, Keras, sklearn: http://t.co/jZ2ZGx7JwR http://t‚Ä¶ RT @russpoldrack: Full raw MRI dataset for #MyConnectome now available for download at http://t.co/1fj9jCIOXL RT @evolvingstuff: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift http://t.co/sdD3P8oXj2 As of joblib 0.9.0b3 it's possible to reuse a process or thread pool for several consecutive calls to parallel: https://t.co/vzliYyZSKY @fchollet @syhw @adnothing have studied the impact of the recurrent non-linearity (ReLU vs tanh)? @wesmckinn you should really host a rendered version of https://t.co/w4FKRCKpvW (with cell outputs). Or better: provide a tmpnb-based demo. @syhw @adnothing also tried it on more different kinds of tasks / datasets. @syhw @adnothing they cite it and it's slightly different: Le et al's use relu and a single uniform recurrent weight matrix. RT @adnothing: That's what we get for doing non-convex stuff and focusing on perf: init RNN with identity matrix ~ new Hinton paper https:/‚Ä¶ RT @CommonCrawl: Ready for a new #BigData experiment? The June 2015 crawl archive with 1.67 billion webpages has been released!
http://t.co‚Ä¶ RT @t3kcit: Every experiment is sacred! Store the output and code snapshots for all our experiments in mongo automatically. http://t.co/R9r‚Ä¶ RT @graphific: Great time to be a researcher: splendid to see authors putting their articles on arxiv with accompanying code on github #ope‚Ä¶ RT @mrocklin: New blogpost.  Custom parallel workflows with #dask and for loops 

http://t.co/5woLMmmKJ4 http://t.co/JNicoOjxbD @seaandsailor @syhw better wait for stimpack and shield to time the first push. New Horizons should scout their gas soon. RT @mat_kelcey: This is the best diagram I've seen of a text based RNN with attention (from http://t.co/SCJUIgNoS3) http://t.co/fEJZHIuNLh RT @fchollet: Keras now supports Graph models (arbitrary connectivity). You can now build Neural Turing Machines &amp; MemNNs in Keras. http://‚Ä¶ @syhw it's a trap, it's infested with zergs. RT @syhw: AP makes one million minutes of historical footage available on YouTube http://t.co/uxn8fRITYO RT @robstewartUK: It's the end for emacs https://t.co/eVpOz6jHFK http://t.co/MhsBKVlWRP RT @benhamner: I wrote a quick Python example using RF and XGBoost to detect simulated œÑ‚Üí3Œº decay in our latest @CERN competition https://t‚Ä¶ RT @josephreisinger: Inferring Networks of Substitutable and Complementary Products [McAuley, Pandey, and Leskovec] http://t.co/Li3dIPshEB ‚Ä¶ RT @thefreemanlab: read the docs http://t.co/Eru7k3XcJN play in the notebooks http://t.co/S2OQw1Vv5E #jupyter RT @ProjectJupyter: For anyone who has been waiting for it, notebooks are now rendered on http://t.co/KPnrdCES4e as well. @dangillmor @lbrothers @cstross then it should not be called open source any more: http://t.co/e9gKmfqsiw RT @t3kcit: auto-sklearn https://t.co/O1Egm9aysK does automatic machine learning based on SMAC http://t.co/eKs68hn4vJ looks cool :) RT @mxlearn: Neon v0.9 released today with multi-GPU support for Maxwells; VGG can now be trained with about 8 s/macrobatch... http://t.co/‚Ä¶ @kastnerkyle @karpathy @AlecRad little weird indeed @kastnerkyle @karpathy @AlecRad ok, but for pixel values? I don't see how one-hot encoding would work on that data. @kastnerkyle @karpathy @AlecRad what is BCE? RT @wesmckinn: Ibis website here: http://t.co/CCQwWVvjj7. Blog with ongoing project updates (and much more documentation) coming in the nea‚Ä¶ RT @wesmckinn: Really excited to announce Ibis: high performance Python at massive scale for the Python data community: http://t.co/Q22KWcS‚Ä¶ @bortzmeyer @GaelVaroquaux @github @t3kcit thanks! @bortzmeyer @GaelVaroquaux @github @t3kcit invalidation then :) RT @yoavgo: The expression-composition neural-net frameworks just keep on coming. This one in pure python from @davidweichiang : https://t.‚Ä¶ RT @GaelVaroquaux: @github @t3kcit @ogrisel DNS cache propagation... it's not up for everyobdy yet. RT @GaelVaroquaux: http://t.co/eSZirkHHZD is back online, we have switched to @github for hosting.

Thanks to @t3kcit and @ogrisel for impl‚Ä¶ RT @AlecRad: Meet this weeks people! A little less creepy than last weeks people? http://t.co/3L2fWLeWyf @stonebigdotdot @Mbussonn @scikit_learn @IPythonDev this is not the case. RT @Mbussonn: Apparently #IPython have a Bus/Truck factor of 3 : http://t.co/fj2j6gvhqt better than #pandas (1), worse than @scikit_learn (‚Ä¶ @samuelcharron use http://t.co/jHnrj237Bo RT @t3kcit: Totally non official https://t.co/QqgHDY83cU Adversarial examples fact check by Ian Goodfellow: http://t.co/tVc4HWN1E3 #dlearn #machinelearning @ebottabi @samklr i meant keras.io or https://t.co/vpeBAzFAhH RT @MakingLyst: First the code. Now the blog. 'Searching for Approximate Nearest Neighbours' by a @Maciej_Kula http://t.co/1kW8WWUWMj RT @benhamner: Winning solution to the @kaggle @CrowdFlower challenge https://t.co/KmU1awBXq8 https://t.co/Vg2ea8LKfT http://t.co/jHefWHkAPD RT @rsimmon: Viridis, the new, perceptually-accurate color map for matplotlib http://t.co/ZW7b0zihPc via @dougmcneall http://t.co/F8pUJ8Afpx RT @kastnerkyle: incredible! A Better Default Colormap for Matplotlib | SciPy 2015 | Nathaniel Smith and St√©fan van der Walt https://t.co/3‚Ä¶ RT @rgbkrk: Whoa, new @jakevdp article on pivot tables in Python with full interaction: https://t.co/NMGVg5rgLI @esc___ @dabeaz you can use 'pip install -v package' to get the verbose output. RT @meickenberg: "Mind the Duality Gap: Safer Rules for the Lasso" http://t.co/BhqQiwVZ5T (finally rules that seem to truly speed-up estima‚Ä¶ RT @rsalakhu: Releasing BookCorpus: 11,038 books dataset that we used to train skip-thought vectors
http://t.co/SU83EFKdoc
Encoder https://‚Ä¶ RT @PyDataConf: .@PyDataConf follower #6000 gets a free ticket to PyData Seattle, Jul 24-26 at Microsoft HQ: http://t.co/OOHfI6QqEl http://‚Ä¶ RT @amicel: Pensez √† proposer une session pour PyConFr 2015 avant le 25 juillet http://t.co/pOwiKUrUHg @nicolaspipard as far as I know, nobody has started working on it. RT @AlecRad: Generative adversarial networks playing http://t.co/fM9VP5U6N9 RT @modernscientist: Videos from #SciPy2015 are online: https://t.co/m7dtfApTwz

Thanks @enthought!

Tutorial materials here: http://t.co/B‚Ä¶ RT @shoyer: Numba has been making huge improvements recently! All these new features in the last year. #scipy2015 http://t.co/QC66OaYYQt RT @astronomatty: Advice from @jakevdp at #scipy2015 keynote http://t.co/JgIYuAbLxV @mrocklin thx RT @ST4Good: Promising Proba. Back Prop. for scalable learning Bayesian Neural Nets http://t.co/ZV7rfm3z7x #ICML2015 #deeplearning http://t‚Ä¶ @mrocklin some slides cannot be fetched correctly. RT @mrocklin: Slides for my talk on Python, Parallelism, and Dask at #icml2015 #mloss15

http://t.co/z3YTXu2SDo RT @shoyer: Slide and video for my xray #PyData talk at #SciPy2015:
http://t.co/bhrSRvEcMo
https://t.co/kvOzlOy2a8 RT @Al_levity: #ICML2015 DL3
Generative Moment Matching Networks
http://t.co/AmP96cwUzt
Code https://t.co/ZaxumYEX4X @IgorCarron @BigphinOrca if not PRs welcome but after we merge the current refactoring. @BigphinOrca we are currently refactoring sklearn's GMMs with a GSoC student. We might have a look into that as an extension if we have time @BigphinOrca this is interesting, thanks. Have you used that trick for regression your-self? or for some other task? If so which? @BigphinOrca where is this screenshot taken from? RT @balazskegl: . @agramfort is explaining his super cool fast Lasso at #icml2015. Soon in @scikit_learn. http://t.co/ABfqlxfNyi RT @randal_olson: Here's part 2 of that excellent #scipy2015 sklearn tutorial. #Python #MachineLearning

https://t.co/SPInYHs5e1 RT @t3kcit: First half of scikit-learn / ml tutorial with @kastnerkyle : https://t.co/6yII971dUc notebooks here https://t.co/HWMUzOtda4 #sc‚Ä¶ RT @balazskegl: Journey through the layers of the mind http://t.co/c7D6LKhNn7 @ynd yes RT @traviskorte: #deepdream is fun but reminds me of when artists discovered fractals. Seems to increase artistic possibility but all outpu‚Ä¶ RT @ds_ldn: Interactive Deep Neural Net Hallucinations (+ source code ) http://t.co/00HpIyeo7v via @samim #deepdream http://t.co/BuyYAqFmXl RT @mxlearn: [1507.00210] Natural Neural Networks http://t.co/GQjZ8tQXXN RT @aut0mata: Hieronymus Bosch, The Garden of Earthly Delights #deepdream http://t.co/tqPM6uMlBx RT @RadimRehurek: Check out Optunity http://t.co/9VnlqhWtrS, a neat new Python library for optimizing hyperparameters by @marc_claesen #KUL‚Ä¶ RT @dnouri: The Palladium tutorial shows how to set up training, save a trained model, and use it to serve a web API for Iris. http://t.co/‚Ä¶ RT @dnouri: We released Palladium 1.0 yesterday: a scikit-learn based framework for setting up predictive analytics services. https://t.co/‚Ä¶ RT @quantombone: Creepy #deepdream animation of my face. http://t.co/3jaczSXeTn via @imgur RT @dwf: And @ylecun appears in the comments with a much needed dose of real talk: https://t.co/0WE85jvzup RT @treycausey: The @__DataTau__ who's hiring thread doesn't get the same love as the HN one. It should! Post your data science jobs: http:‚Ä¶ RT @hashicorp: Terraform 0.6 has been released! Azure, 15 new AWS resources and plenty of bug fixes and improvements. https://t.co/SUseXauI‚Ä¶ RT @DataRobot: [How To] Write complex dask graphs for non-trivial algorithms: http://t.co/YVBGV9Ixnm #algorithms #python RT @fulhack: Annoy is finally on par with kgraph and beating flann https://t.co/40GfinE81j https://t.co/7sRLG054tT http://t.co/C8Pvt5aQEv @CharlesOllion @rsalakhu and would it be possible to make it cheaper to train with the FOFE trick http://t.co/0aiMs8D3Qt ? @ynd this could be done by studying the convergence speed of training a series of NN w/ random init closer and closer to the 0 weights init. @ynd would be interesting to compare ESGD vs RMSProp vs SGD vs LBFGS in their ability &amp; speed to escape a known saddle point area. @ynd the plt.hist in your ESGD paper would be more beautiful by aligning the bins with linspace as at the end of http://t.co/oW02kiDew6 Handy resource: where to hire Python devs in France? http://t.co/ursYKTMDn4 (in French) via @amicel Skip-Thought Vectors by Ryan Kiros, Yukun Zhu, @rsalakhu et al. http://t.co/WTqxDKwhnW weakly-supervised sentence embeddings #dlearn RT @EGouillart: want to improve the knowledge of scientific Python inside your lab/company? Send colleagues to @Euroscipy tutorials! https:‚Ä¶ RT @teoliphant: Dask 0.6.0 release is out:  https://t.co/wfciEoVx3w   DataFrame support ready for initial use and Distributed support impro‚Ä¶ RT @DeepLearningHub: Deep nets generating stuff - FastML http://t.co/WJV6lqfIep via @fastml RT @allthingshadoop: A Mesos framework for scaling a Ceph cluster https://t.co/iHoFagnVuF RT @karpathy: Visual Q&amp;A dataset http://t.co/tbByaMV7d6 good luck computer vision :p RT @PeoplePattern: AI? Not Yet. But Machine Learning is Here and Now! #MachineLearning via @jasonbaldridge http://t.co/RB2ud0OOzS http://t.‚Ä¶ RT @michaelwaskom: seaborn 0.6 is out! Take a look at the release notes to find out what's new:  http://t.co/JjvR0Ftbqv http://t.co/DIE0oWR‚Ä¶ Joblib 0.9.0b2 is out with automatic batching of very short tasks: https://t.co/z6LcucY1gh RT @twiecki: Effort to port @Cmrn_DP's great book "Bayesian methods for hackers" from pymc2 to #PyMC3: https://t.co/lTUGFSYB4F Contributors‚Ä¶ @deliprao @ChrisDiehl @agramfort but VW has a better optimizer by (AdaGrad) + online max absolute feature scaling by default @deliprao @ChrisDiehl @agramfort without quadratic / nn features, they are both linear models so accuracy should be similar. @ChrisDiehl @agramfort BTW it also maintains internal optimizer state across partial_fit calls, e.g. learning rate schedule. RT @quantombone: Deep down the rabbit hole: CVPR 2015 and beyond http://t.co/J1EWazvbnE #computervision #deeplearning #cvpr #research http:‚Ä¶ @ChrisDiehl @agramfort also current stage of the optimizer internals, eg learning rate schedule @ChrisDiehl @agramfort yes @mrperki @ramez http://t.co/UJJ7orxa0o RT @DataScienceLA: Slides from my talk on benchmarking #machinelearning tools https://t.co/XjkpId0nip  #rstats #pydata @h2o #xgboost #spark‚Ä¶ @fastml_extra and it's open source with a non-viral license. RT @fastml_extra: Intel wants to be a player in deep learning hardware, with its Iris Pro GPUs running OpenCL. Here's the software:
https:/‚Ä¶ RT @mdreid: COLT 2015 proceedings are now available: http://t.co/IvjY3wgObF RT @317070: http://t.co/QtxWHUxG4D received an update: 2x more optimization, 720p rendering and settings for bigger, screen-filling objects. Nifty implementation of the NBSVM strong baseline for text classification: NB-style features for liblinear models: https://t.co/3yiOX1ArzM @fionntan better: there are 2 short scripts to rebuild it from the base ubuntu ami: http://t.co/jHnrj237Bo RT @ContinuumIO: Discover how to use #pandas categoricals to improve performance on data with text categories: http://t.co/WEEBpptoU0 via @‚Ä¶ RT @jedisct1: Atom 1.0 is here! http://t.co/LcHctPWuzj RT @adriancolyer: Microsoft figured out a way to test less, still get the same level of quality, and save $2M... http://t.co/7sh26auTih RT @adriancolyer: "The Art of Testing Less Without Sacrificing Quality" - Herzig et al. 2015 http://t.co/7sh26auTih #themorningpaper RT @samim: Here is the blog post about the neural #LSD: https://t.co/dPmOH4vkKM code coming a few days. Art world, get ready, set... RT @317070: My work of the past week. https://t.co/qHs2yqvTDw RT @mxlearn: General Sequence Learning using Recurrent Neural Networks. Great Talk for Beginners in RNNs! http://t.co/Z9RzMLtJT0 RT @karpathy: Deep Learning singularity: when new cool papers are coming out faster than you can read them RT @bbabenko: here's an ipython notebook showing how i did the cat inceptionism thing #hax : http://t.co/Y9WOXb30F0 RT @RichardSocher: Amazing #deeplearning #NLP projects from the #CS224d class at Stanford: http://t.co/NvwicqpLjF e.g. music generation htt‚Ä¶ @kastnerkyle Nice! RT @rsalakhu: Ryan Kiros and Yukun Zhu show that one can learn surprisingly good generic sentence embeddings. On the whole... http://t.co/C‚Ä¶ RT @karpathy: Neural Nets dreaming natural images http://t.co/06R1ufn5SQ , about the recent FAIR paper. Adversarial nets sound quite tricky‚Ä¶ RT @deeplearning4j: Quoc Le and Oriol Vinyals of @Google have built an impressive little chatbot with recurrent nets: http://t.co/kZckhNxKWh RT @swcarpentry: Software Carpentry would like to hire a program coordinator. Details are at http://t.co/PDlv8iNs7w - please share. RT @kastnerkyle: VAE, take 3. Getting better each time :) https://t.co/bKqatQCC73 http://t.co/4TMfJtuh6C RT @rsalakhu: Inceptionism-Captions: Check out captions generated from Google's inception images using visual attention. 

http://t.co/kknc‚Ä¶ RT @aflyax: Prediction intervals for Random Forests:
http://t.co/txPbGla4WS #machinelearning #python #scikitlearn @treycausey recent progress on visual models with RNN-based attention mechanisms might be the way to tackle the limitations of 2014 CNNs RT @twiecki: #PyMC3 beta released: A simple and readable, yet powerful, probabilistic programming framework in #Python https://t.co/zo31hGc‚Ä¶ Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks by NYU / FAIR researchers http://t.co/S3Vo0jqyaE #dlearn @betatim @therriaultphd I found the post interesting but do you know if bumping is actually ever useful on real tasks / data? @jon1012 @FFeth il fait des capteurs qu'il portent dans son sac-a-dos pour des mesures de qualit√© de l'air dans la rue, √† pied et √† velo. @jon1012 @FFeth si vous voulez un avis scientifique sur les mesures de qualit√© de l'air vous pouvez demander a https://t.co/2ii7FN6FFn @jon1012 +1 pour un produit finit. Tu peux tenter des pre-commandes via un kisskissbankbank ou kickstarter /cc @FFeth @EGouillart RT @DataRobot: No, more data isn't always better than better algorithms in #machinelearning, explains @Quora's @xamat &gt; http://t.co/aRnu8xn‚Ä¶ @huitseeker @KrisztianSzucs agreed for perform aggregations prior to passing code to numpy programs written in Python. @huitseeker @KrisztianSzucs with tungsten it will be a different story. @huitseeker @KrisztianSzucs it needs to be benched. @huitseeker @KrisztianSzucs not sure that converting columnar df to numpy array each time will be faster than converting to ArrayRDD once RT @betatim: New blog post: Bumping http://t.co/4R2TyCDBdo Why can a decision tree not learn the chessboard pattern #ml #bootstrap http://t‚Ä¶ @mat_kelcey @chrisemoody @astrobiased @fchollet BTW you should try to seed your CNN dreams with some pics by Jean Lecointre. @ylecun recursive Jean Lecointrisme RT @KrisztianSzucs: https://t.co/9ziN7dng5g now supports Python 3 and Spark 1.4 RT @TwitterEng: We‚Äôve acquired @whetlab to accelerate Twitter‚Äôs machine learning efforts. https://t.co/wGXuHUc36a RT @kcimc: finally the full story behind that creepy squirrel-slug http://t.co/SDwf8VwoZu a bunch more: https://t.co/WiejWTLHWM http://t.co‚Ä¶ RT @googleresearch: Inceptionism - peaking under the hood of neural networks through visualizations: https://t.co/cjQnFmEnfA http://t.co/VL‚Ä¶ RT @mat_kelcey: "Introduction to Neural Machine Translation with GPUs (Part 2)" http://t.co/ayrEs7dtGw RT @DRMacIver: As an idle observation, I've been running Python 3.4 as my default Python for months now and hardly run into any problems at‚Ä¶ RT @rgbkrk: Sweet new GIF provided by @steve_silvester for Jupyter sidecar https://t.co/FY6ikaK0H0 http://t.co/efThgCCV4C RT @CharlesOllion: to workshop #2 attendees: here's some character generation code you can run directly on the amazon dl-machine! https://t‚Ä¶ RT @dataiku: Simple introduction to using ensemble methods in the context of kaggle competitions http://t.co/SkyclZIecr RT @andyhickl: [1506.03478] Generative Image Modeling Using Spatial LSTMs http://t.co/SsDwTG8Bnt RT @pydatalondon: 2 free tix gone in 3 min, you lucky ninjas! 22% discount running *only* for the next two days. pydata-2fast2furious2 http‚Ä¶ RT @ylecun: It's official: Kyunghyun Cho is joining NYU as an Assistant Professor in the Computer Science Department and the... http://t.co‚Ä¶ @pwendell so you can do the PR for next release to update the URL,  sha256 digest and the version number. @pwendell FYI,here is the recipe: https://t.co/ZpSHmNzoYo there is a jenkins bot that checks the execution. @pwendell we were 3 people attempting to get the PR for the 1.4.0 release in :) RT @benhamner: "What's wrong with deep learning?" Theory, memory, reasoning, unsupervised. Great slides by @ylecun for CVPR keynote https:/‚Ä¶ Under OSX: brew update &amp;&amp; brew install apache-spark to install Apache Spark / PySpark 1.4.0 RT @rxin: Spark 1.4 is out with Python 3 support! http://t.co/WvtIVrGTG2 RT @npinto: Baidu Fires Researcher Tied to Contest Disqualification (!) http://t.co/Bt2pXvZyJl RT @michoo_42: total course of deep learning freitas http://t.co/ypg51wifrt RT @amiconfusediam: AMD GPUs &amp; Deep Learning, finally happening...? CLTorch almost finished. Super awesome! Thx Hugh Perkins. 
https://t.co‚Ä¶ @gosmej yes I understood, this is why I said it's related only. @gosmej related: http://t.co/5ar4Q8UFKC @gosmej no but it might be possible to implement them on top of existing trees using the `apply` method + Ridge / LinearRegression. RT @pyconfr: PyConFr 2015 aura lieu √† Pau du 17 au 20 Octobre ! jobs.py #1 meetup June 17th in Paris by startups hiring Python devs: http://t.co/Yn0OAotdH6 RT @pagetable: xhyve ‚Äì Lightweight Virtualization on OS X Based on bhyve http://t.co/7Ei2E4s4oJ @teoliphant @ncoghlan_dev @ContinuumIO mesos and possibly also yarn sound like interesting backends. RT @fastml_extra: Yet another Python deep learning library: 
http://t.co/xV8TZacQwV https://t.co/9HaizEdyhX @kastnerkyle you can always base64 the wav files and put that has comments in the latex source along with the source code on arXiv ;) @kastnerkyle do you have speech samples online? RT @kastnerkyle: Our paper for variational RNNs is out! http://t.co/zG4PKitlNO RT @evolvingstuff: Visualizing and Understanding Recurrent Networks http://t.co/1XDdDPiii5 RT @benhamner: The state of hyperparameter optimization: bayesian optimization trumps random search and grid search http://t.co/UpmJiEVHFY RT @karpathy: A Compilation of Robots Falling Down at the DARPA Robotics Challenge https://t.co/UxFlWV8kuG "The singularity has been delaye‚Ä¶ RT @CharlesOllion: Ongoing #deeplearning workshop on RNN and LSTM at @LaPaillasse. Sponsored by @heuritechdata and @nvidia! @tarek_ziade congrats again! RT @brunobord: thank you, @firefox, the reader mode works really great.
(sorry, online crippled-with-ads-newspapers) @wesmckinn @RideImpala @laserson that's great news :) @wesmckinn @RideImpala @laserson or Tachyon FS as done in ImpalaToGo fork but available as a configuration option for the official Impala? @wesmckinn @RideImpala @laserson any plan for an official JVM-free distribution of impala? E.g. single host on local FS or S3-backed? RT @PyDataParis: .@PyDataLondon (June 19-21) schedule includes scikit-learn,Pandas,deep learning,vizualisation,Numba &amp; Spark: http://t.co/s‚Ä¶ RT @dakami: OH: "We used to leak kilobytes, then megs, then even gigs. Now, we leak EC2 instances.  Someday, we'll leak entire datacenters." RT @ClouderaEng: RT @wesmckinn: impyla (Python client for @RideImpala) 0.10 is out, with Python 3 support by @laserson! https://t.co/oZOxCd‚Ä¶ @mgershoff you can monitor objective function on a validation set and stop as soon validation improvements disappear. @mgershoff Leon Bottou recommend: start with SGD / minibatch K-means and finish convergence with batch K-means: http://t.co/XFjaKs5nhv @mgershoff minibatch k-means is fast to converge on first epochs but slower later &amp; some Yinyang might be adapted to speedup minibatch too @leonpalafox this is a net improvement over our current implementation. @leonpalafox this one should probably be contributed to scikit-learn if the claims are confirmed. @attud_bidirt @rasbt fitting == training, those are synonyms. RT @ebarroca: Stunning! &gt; Researchers Find Textbook-Altering Link Between Brain, Immune System http://t.co/LCi5pmgYms Yinyang K-Means: same clusters but 2x to 25x faster than Standard K-Means: http://t.co/gsR5FHwVs7 #machinelearning #ICML2015 @cournape I found it, thanks. @attud_bidirt @rasbt call fit_transform on training set and transform on validation and test sets. @attud_bidirt @rasbt fit_transform builds a new mapping from words to features (column indices) while transform reuses the previous mapping. @cournape thanks! @cournape I think this is the paper we discussed some time ago: http://t.co/fyerPCPch0 any related ref I should check on this topic? RT @balazskegl: The #datascience ecosystem: actors, incentives, challenges. @SaclayCDS @U_ParisSaclay 

https://t.co/2uiwmk8y2X http://t.co‚Ä¶ RT @t3kcit: All scikit-learn API reference pages now have a link to the corresponding user guide section: http://t.co/5AAqBuZ7Xj [only for ‚Ä¶ RT @googleresearch: A Multilingual Corpus of Automatically Extracted Relations from Wikipedia - learn more at https://t.co/AUjVaDORkF http:‚Ä¶ Data Science Engineer position at @SaclayCDS: http://t.co/xbXTaA247a #job RT @MrChrisJohnson: Inspired by @karpathy's recent blog post, I trained a char-by-char RNN on Eminem lyrics then let it generate its own ht‚Ä¶ RT @stefanvdwalt: We'd love to hear your feedback on these proposed colormaps for #matplotlib: http://t.co/9LBJsnQruM @fulhack @RadimRehurek @wdong397 @srchvrs @abursuc @pixelogik would be interesting to add RandomizedPCA n_components=10 + KDTree or BallTree RT @SciPyConf: Intermediate &amp; Advanced scikit-Learn tutorials by @t3kcit &amp; @kastnerkyle July 6th. Register!  http://t.co/udqS5I2Sbm RT @chris_bour: AXA is hiring its Chief Data Scientist. Pls RT and ping me ;) #job #datascience #ML https://t.co/vEwQTgczNI @antoine_petit_ pour √©viter cet effet l'usage est de pr√©fixer le tweet par le caract√®re '.' @antoine_petit_ tip: qd un tweet commence par un mention, seule l'intersection des followers des deux comptes le verront dans leur timeline. RT @chris_bour: Summary and descriptive statistics in Spark 1.4 Dataframes https://t.co/3GOxkjA86B #Protip: run "ctags -R sklearn" in your scikit-learn source folder, then alt-ctrl-r + class name to jump to class def in @AtomEditor @RadimRehurek nope. RT @ylecun: Big announcement today: we are creating a new branch of Facebook AI Research in Paris!... http://t.co/MU4Uvp2AmZ RT @syhw: Dear ILSVRC community http://t.co/8eYFjXwxxo (overfitting the test set, or why you should only look at results of the challenge o‚Ä¶ @__genji__ @mblondel_ml interesting. RT @xtimv: @mblondel_ml It's just a special case of the FD-trick for directional derivatives (http://t.co/a2Qb7jJtu4). RT @mblondel_ml: Finite difference trick I learned today: Hessian(x) * vector ‚âà [grad(x + epsilon * vector) - grad(x)] / epsilon. Error is ‚Ä¶ @mat_kelcey i am not sure I get it just from those slides. Any other good ref besides the code? @patriktormanen nope as the API would not fit well. RT @mblondel_ml: Didn't realize that SciPy has a randomized SVD module... http://t.co/fe1eWhbVkS RT @RichardSocher: Fixed low-pass filtering of word vectors as good as recurrent #neuralnet and #LSTM at lang. modeling http://t.co/Es49nEl‚Ä¶ RT @cfregly: RDDs are the new bytecode of Apache Spark https://t.co/96vbBpXED3 RT @mat_kelcey: @ogrisel ( also the code very closely related to this deck came out recently, https://t.co/U2AEBeMZWq ) Great slides by Kyunghyun Cho on Neural Machine Translation with encoder / decoder RNN architectures: https://t.co/HLS5A5f2MG #dlearn @egocentric_bird @abursuc @syhw there is also a German company, cloud&amp;heat that offers PaaS, IaaS &amp; water heating: http://t.co/6EiceKf5K4 RT @rgaidot: hyper.sh - Hypervisor-agnostic Docker Engine https://t.co/D3uxp9iBLk RT @marcua: Asking readers to draw what they think a trend is before showing the reality seems like a powerful educational tool: http://t.c‚Ä¶ RT @pjacock: http://t.co/Fc5qCqJ9tO @sourceforge throwing out tagline "Your Trusted Source for Open Source"? Trust is gone now, eg http://t‚Ä¶ @_JayAlan_ thanks :) @ebottabi @samklr read http://t.co/6fM4ESfeoq then try http://t.co/wsmKVqQNuJ or http://t.co/wsmKVqQNuJ RT @peter_rud: Introduction to Neural Machine Translation with GPUs (part 1) http://t.co/VXQpk0PPJd RT @DataParis: We started a Slack channel to geek around Data in Paris and all things related. Get an invite here : http://t.co/UWDiu1uaTH RT @rgaidot: Rust for Python programmers http://t.co/TjcoiGjnYk @amicel @HiddenOrchestra no pbm, I just hope they will have a date in Paris soon. Their sound is 10x better live especially in small venues. Piano Interrupted - Cross Hands remixed by @HiddenOrchestra  https://t.co/OaSsphighb #music #doubledrums RT @esc___: #blosc/#bcolz is looking for donations and/or sponsors: http://t.co/qjhveZPyvG please RT. RT @alexey_r: Practical Numerical Methods with Python: an IPython-notebook-based MOOC https://t.co/esrIIA7x5r RT @adrienchauve: Fast Immutable Python Deployments with pip 7 and virtualenv https://t.co/1PTN8ShXD2 RT @adapteva: Supercomputer.io reached 1,000 cores in &lt;24hrs. Have you connected your Parallella yet? http://t.co/O4PYCfOCWy http://t.co/oA‚Ä¶ RT @yoavgo: .@karpathy's RNNs post is great, but generating Shakespeare and Paul Graham isn't that impressive. Generating code is http://t.‚Ä¶ RT @pydatalondon: PyDataLondon meetup now *largest* Python group in London! Blimey, that only took a year! (Also-conf in a month!) @EdwardRaffML @mblondel_ml it says that there exists a trivial smooth path so it's likely that the path taken is smooth enough too. RT @fastml_extra: 6 Tricks I Learned From The OTTO Kaggle Challenge:
https://t.co/bKW6ZoewDY RT @johnplattml: #seattletimes interview with @kentarotoyama on his book about technology and #socialchange . http://t.co/Mge9VwV0YN @andreabedini and experimental wheels from https://t.co/ryMRXx1xTq @andreabedini some older osx whl for numpy &amp; scipy I think RT @hackernewsbot: LLVM OpenMP Support... http://t.co/a4EvohvB1E @mblondel_ml it is not a proof but a strong evidence that narrow valleys or plateaus (saddle points) are not frequent Qualitatively characterizing neural network optimization problems by I. Goodfellow, O. Vinyals &amp; A. Saxe: http://t.co/6QaJhlVqNo #ICLR2015 RT @dstufft: I just released pip 7.0 and virtualenv 13.0. We now auto build wheels and cache them - a massive speed up. See: https://t.co/w‚Ä¶ RT @CharlesOllion: My company @heuritechdata and I organize the Deep Learning Workshop #2 on RNN &amp; LSTM in Paris June 06! http://t.co/T1q0l‚Ä¶ RT @ylecun: Geoff Hinton talks to the Guardian in advance of his speech at the Royal Society.

Obligatory sniping joke from... http://t.co/‚Ä¶ RT @jedisct1: Nobody ever got fired for using Hadoop, but single big-memory servers may simply be more efficient than clusters http://t.co/‚Ä¶ RT @TwitterEng: Learn about the @ApacheParquet v1.7 release and its graduation from @TheASF incubator: https://t.co/HviMNQOXD0 #hadoop RT @deliprao: . @karpathy strikes again, this time generating LaTex papers and @paulg essays from RNNs.
http://t.co/QroeIRc8oc RT @franckcuny: % git add -p
...
Stage this junk [y,n,q,a,d,/,K,g,e,?]? @fmbutt @fchollet @pmarca they did not say 'soon' and speech recognition models in today's smartphones are derivatives of the Perceptron. RT @AlecRad: The "triangle" activation function in http://t.co/6kJaBltPlB seems surprisingly like a precursor to parts of batchnorm and rec‚Ä¶ RT @kastnerkyle: Our ICML workshop paper on image recognition with recurrent nets 

http://t.co/xbDujdEIOb RT @jeremyphoward: Initialization of deep networks http://t.co/gl9pQcSKT4 Summary of most important recent papers RT @chris_bour: My mini notebook on why @scikit_learn calibration worked on OTTO @kaggle challenge https://t.co/gt9CKJCTjm RT @kaggle: #nofreehunch Early this month, #Kagglers in Paris hacked the Otto Group Challenge. Read about their experience here http://t.co‚Ä¶ RT @DeepLearningHub: #ICYMI Deep Learning for Image Understanding in Planetary Science http://t.co/SN83ZnzSNt RT @dwf: Remember kids, 1e-8 is representable in float32. 1 - 1e-8, on the other hand, is just 1. RT @sedielem: Lasagne docs are starting to look half decent: http://t.co/KNB49KfROx almost ready for the 0.1 release! (let us know what's m‚Ä¶ RT @DataParis: Les inscriptions pour le prochain datageeks c‚Äôest dans 10 minutes http://t.co/YdzaaFxNoY RT @garybernhardt: Reminder to people whose "big data" is under a terabyte: servers with 1 TB RAM can be had about $20k. Your data set fits‚Ä¶ RT @russpoldrack: The @openfmri project is looking for a new data curator - more info here: https://t.co/ToO8diUQG2 RT @PaulMineiro: ICLR 2015 Review: The ambition, quality, and (small) community of ICLR combine to make this my new favorite co... http://t‚Ä¶ RT @2noame: "Self-Driving Trucks Are Going to Hit Us Like a Human-Driven Truck" by @2noame https://t.co/HD87QdCBXb #basicincome http://t.co‚Ä¶ RT @treycausey: Great answer by @xamat to "What are the advantages of different classification algorithms?" http://t.co/XdYHZUOwY7 RT @jiffyclub: What are some sw skills new data scientists need to know? All the things @swcarpentry teaches. http://t.co/ItZyeme1ac by @tr‚Ä¶ RT @kaa1oo: What a fantastic idea to have a @papers_we_love meetup here in Paris. Thank you @fbiville @mathieuravaux and @samklr http://t.c‚Ä¶ RT @DataParis: Registrations open Wednesday May 20th at 5.00pm. http://t.co/YdzaaFxNoY #datageeks RT @DataParis: CRDTs in #Riak, Real time bidding, and Sentiment Trading with HBase, for the next @DataParis meetup. http://t.co/mvJh3aIhYn RT @jedisct1: An introduction to Clear Containers http://t.co/m1U3J8LxBd RT @edwyplenel: Pour neutraliser vraiment les soci√©t√©s-√©crans. @mediapart soutient cette Initiative citoyenne europ√©enne. Signez !
http://t‚Ä¶ RT @jedisct1: Google Cloud: Introducing Preemptible VMs, a new class of compute available at 70% off standard pricing http://t.co/WYEV2AWMjV RT @jakevdp: ML tip: train a model to distinguish between your training set &amp; unlabeled data. If it works, your training data may be incomp‚Ä¶ @aescaffre not as a readily available library... @id_wildflowers http://t.co/tqqTlFLR9m RT @shoyer: My numpy.stack PR just got merged! Really looking forward to seeing {h,v,d}stack disappear from common use https://t.co/zszwjYf‚Ä¶ RT @ProjectJupyter: Exciting announcement! Jupyter Notebooks rendered on @GitHub https://t.co/DE3ayRRFOZ https://t.co/sYOjbVBGua @ChrisDiehl i want to try docker machine / swarm / compose. @ChrisDiehl don't use starcluster much nowadays. @mikiobraun appveyor might not cover all cases, esp. 32 bit Linux is harder to cover but should still catch most of the bugs. @benoitc http://t.co/6fYs6oYq13 @mikiobraun have you tried http://t.co/o39ItNmJUZ? Travis can also build under osx with "language: objective-c" RT @asmeurer: Lot's of nice small and a few big changes slated for Python 3.5 already https://t.co/ljUM99a14F RT @sdouche: Under the hood: Facebook‚Äôs cold storage system #facebook #hardware - http://t.co/dhGIxLGRNa @echarles @datamusing see http://t.co/ViVvA1pokM RT @johnplattml: Antoine Bordes argues that series of increasingly difficult artificial tasks is the path towards AI: http://t.co/YBAG6XeaF‚Ä¶ RT @johnplattml: You can get rid of maxpooling in #deeplearning and keep accuracy. http://t.co/Ya0ZkKut8w #iclr2015 http://t.co/siOTdQSFh1 RT @johnplattml: Parameterized deep autoencoder learns hidden style factors http://t.co/1QerBy3nfV #iclr2015 http://t.co/5G6fyAyplX RT @bugraa: Jupyter notebooks are rendered within @github . This is so awesome! https://t.co/uSCDwZadgW @jperras @dwf i have used vim for 10 years but hated to have to configure. Now switched to atom for that reason. RT @trevs: gplearn is released! https://t.co/Fv2WePwpqd - Genetic Programming in Python, with a @scikit_learn inspired API... http://t.co/f‚Ä¶ RT @samklr: Next @DataParis meetup, May 27th at @ValtechTechno #parisdatageeks http://t.co/Qiwn3PmUl5 RT @icml15: Check out the accepted papers this year at http://t.co/Tju4eOfk3p. Early registration is open until May 15 at http://t.co/rz6RE‚Ä¶ @DataScienceLA @glouppe Nice :) @DataScienceLA @glouppe have you tried to use integer encoding for categorical variables instead of one-hot? RT @betatim: My first (not totally trivial) contribution to @scikit_learn has been merged! https://t.co/FBF6UOILsj #whoop #otherhappyemoji RT @fulhack: I'm skeptic about this "black box ML in the cloud" trend ‚Äì problem formulation and data integration is the big problem, not mo‚Ä¶ RT @TomAugspurger: This seems useful. http://t.co/6t2nNHI7uH RT @quantombone: Deep Learning vs Big Data: Who owns what? http://t.co/SVP72hVtkA #deeplearning #computervision #bigdata #AI http://t.co/en‚Ä¶ @DeepLearningHub cc @kastnerkyle RT @utopiah: Deployment in Python is the theme of this Friday evening Meetup, come join us in Brussels! http://t.co/OESsqmufkZ with @GMLudo‚Ä¶ RT @datoinc: On the fence about submitting a proposal for @PyDataConf Seattle? The deadline has been extended to 5/11! http://t.co/wjzT1x13‚Ä¶ @KevinLDavenport yes as the tree fitting code is written in Cython we can release the GIL and use threads efficiently. @KevinLDavenport it actually it does not broadcast the data since the original data is just an array shared by concurrent Python threads. @KevinLDavenport RF bootstrap can be disabled with RandomForestClassifier(bootstrap=False) (True by default) @KevinLDavenport joblib RF does not do data partitioning. It broadcast the full data and then RF do bootstrap in inner trees. RT @karpathy: Awesome writeup of SGEMM that runs at 98% of Maxwell architecture throughput, achieved with a custom assembler https://t.co/2‚Ä¶ RT @fperez_org: We have full-time positions open for work on @IPythonDev/@ProjectJupyter at CalPoly w/ @ellisonbg: https://t.co/S78u1ZQjiW #NowPlaying 10th Circle Of Winnipeg par Venetian Snares http://t.co/BEA8R4NZQP RT @w3c: We strongly oppose pervasive surveillance, and are very concerned by France's intelligence bill. http://t.co/7XCe3tTWB8 #NiPigeons‚Ä¶ RT @atpassos_ml: The most depressing fact about the successes of ML/AI is not that we're being replaced by smart computers, but that dumb o‚Ä¶ RT @rgbkrk: The new https://t.co/hMUPcTfFeR is now backed by conda, with kernels for Python, R, and Julia. @DJVadim please put the mentions at the beginning of your tweets otherwise you spam the timeline of your followers and they will unfollow RT @FrancescAlted: PyTables: worth keeping it alive? https://t.co/j2ZiKfAEnl
Please help broadcasting the message to interested communities‚Ä¶ RT @EGouillart: Only 3 days left to submit an abstract to @EuroSciPy 2015! https://t.co/FAUtnd4DvV Looking forward to your submissions of t‚Ä¶ @VolodymyrK no, it's the same tutorial I gave at last strata / PyData / pycon. RT @abursuc: CVPR program is up - lots of deep and 3D http://t.co/Cqjmx4nDso Feels weird to have read already some of them and think of the‚Ä¶ RT @rgaidot: Debian jessie is out
https://t.co/iIQ770JeCS @AlecRad works for me, but it's always a good idea to picnic instead of reading the doc :) @asmeurer @t3kcit but bento is not ready for that yet. In the mean time it would be great if conda package would include .dist-info folders. @asmeurer @t3kcit this would make it easy to install with .dist-info metadata and generate whl packages. @asmeurer @t3kcit the long term solution will be to get rid of setup.py / distutils and instead use a tool like bento to build and install RT @treycausey: Wow, very interesting! @YhatHQ releases Rodeo, a Python IDE for data science. http://t.co/lmnNcWOwri RT @GaelVaroquaux: MLOSS: machine learning open source software workshop @Icml15
http://t.co/WmEWmWitX4

Submit a talk before Apr 28th
http‚Ä¶ @AlecRad I like those visualizations. Is this a feature part of Passage? @esc___ @t3kcit recent versions of pip already issue a deprecation warning when trying to upgrade a package installed without metadata. RT @AlecRad: Attempting to train a face detection regressor CNN - nice to see it handle scale variance quite well! http://t.co/D9SeeCjLvl @esc___ @t3kcit future versions of pip will refuse to upgrade a package that is missing .dist-info or .egg-info metadata. I will be in Strata London on May 5-8 for a tutorial on sklearn http://t.co/LDRFBaND1k and office hours: http://t.co/uOH96DXITF @asmeurer @t3kcit @ContinuumIO de moved the discussion upstream and it's complicated... https://t.co/3xf01oCrnW RT @FrancescAlted: Blosc just received support for AVX2 SIMD instructions in newer CPUs.  Expect 10% of improvement of speed: https://t.co/‚Ä¶ RT @petewarden: Why GEMM is at the heart of deep learning: http://t.co/auA5ZsLHc9 RT @shannonxyw: paper on the scikit-learn API: http://t.co/uRbivBW5sR @ebottabi subsample to find best model / features faster. Restrict to model classes that support out of core with partial_fit. @CharlesOllion @syhw Nice! Waiting for the multigpu support in theano. Good for param-search though. RT @awscloud: Introducing new, larger G2 instances - the g2.8xlarge with massive parallel processing power! http://t.co/5nhdbKJjeu http://t‚Ä¶ RT @pycon: A huge thanks to the speakers, sponsors, volunteers &amp; attendees for another fantastic PyCon!! PyCon 2015 Videos: https://t.co/b0‚Ä¶ RT @patrickc: I think the biggest systemic improvement we could make to software and products would be to have a general way to measure use‚Ä¶ @rgbkrk @ivanov @Delta +1 @ebottabi @simonmaby please feel free to  start a sklearn-pmml project under the same license as scikit-learn itself. @rgbkrk @ivanov @Delta I wonder if that was the first 'twine upload' that is actually a download. @ivanov @rgbkrk @Delta i tried a bit this dill, but dill by itself does not do it all by default. RT @rxin: Python 3 support merged into @ApacheSpark! In 1.4, you can use both 2.x and 3.x. https://t.co/enjCMHK3g3 @birdsarah @BokehPlots that's what PyCon sprints are for :) @gawel_ thanks! glad you liked it. RT @rgbkrk: Cloudpickle 0.1.0 now out! Released in the ‚òÅ, via @delta. Thanks team! https://t.co/b6yKcCmgvA http://t.co/7NFxL1Y8xc RT @ebottabi: recommendation on scikit-learn to #PMML anyone on my timeline with ideas ? RT @GaelVaroquaux: .@scikit_learn code as example in high-profile general-public news against mass-surveillance French laws
http://t.co/gcN‚Ä¶ RT @jbeda: Borg paper is finally out. Lots of reasoning for why we made various decisions in #kubernetes. Very exciting. http://t.co/ZZbvWX‚Ä¶ @M_T_Patterson @treycausey @jsundram and here is @birdsarah's #PyCon2015 talk on bokeh: https://t.co/FGSK983WXF RT @M_T_Patterson: @treycausey @ogrisel @jsundram Here's the link to the Singapore Street talk in case anyone's interested.. 
https://t.co/‚Ä¶ RT @ContinuumIO: Scikit-learn 0.16.1 is now available (all platforms, all Python versions). Use "conda install scikit-learn" to get it! #co‚Ä¶ @caleb_hattingh CI all the things! RT @ianozsvald: PySpark's Python 3.4 support is in testing, they're asking for folk to come bash the latest PRs: https://t.co/AQLmSrhWVs RT @iskander: A non-magical introduction to Pip and Virtualenv for Python beginners: http://t.co/NYuyDhMAsH @jsundram I liked the one on Singapore street names and the one on bokeh. @yarikoptic @asmeurer @cournape scikit-learn 0.16.1 is out! @chriswithers13 https://t.co/vVY4JUWIqG docker-machine can now provision EC2 spot request instances: https://t.co/KX7hAKfBzW RT @t3kcit: Scikit-learn 0.16.1 is out! https://t.co/pOY12jGCTf changelog: http://t.co/G1JFjUMIOP @juancarlospaco nuitka has a runtime dependency on a C compiler. This is a different issue. @m1keil thank you for the thank you :) @johnstamford it depends on the models but 1K records is not a small dataset. 100k samples are fine for models with linear scalability. @juancarlospaco btw, building linux wheels to run them on hosts you control works fine. What is not yet solved is how to publish them. @juancarlospaco no this problem is too hard to solve for the v1 of the spec so support is explicitly delayed to a later version. @juancarlospaco there is currently no platform tag / binary compat spec across Linux distributions Merged the first PR to sklearn for the #PyCon2015 sprints https://t.co/MypY7Tq0SI @aisipos what are you talking about? RT @thecity2: Here are 6 hours worth of scikit-learn tutorials from PyCon 2015: https://t.co/9AK2gy211F and https://t.co/QIEBQPJYIA RT @ellisonbg: Import python modules directly from GitHub (from github.username.repo import foo) with antipackage: https://t.co/fOPIn4tAFb RT @jakevdp: "Losing Your Loops: Fast Numerical Computing with NumPy"
My #PyCon2015 talk!
video: https://t.co/X5opxW0t5L
slides: https://t.‚Ä¶ Building and testing OSX &amp; Windows wheel packages with @travisci and @appveyor 
https://t.co/DDAXqSrhsp
https://t.co/vVY4JUWIqG #PyCon2015 @treycausey @arnicas @sarah_guido http://t.co/NmwYk2dz5q (looking forward to it) RT @cournape: I will be at the numfocus booth today from 11 a.m if you want to talk numpy, scipy #PyCon2015 @yarikoptic @kastnerkyle @cournape alright. Some of us are at Patrick's pub, others at saint bock. @ogrisel actually it's already packed. Trying something else. On my way to les 3 brasseurs in rue saint denis with a bunch of other ml people. #PyCon2015 @kastnerkyle @cournape @yarikoptic plan to leave in 5min? Meeting point at the back of the 517a room? @kastnerkyle @yarikoptic @cournape I am at the back of the 517a room (lightning talks) RT @aisipos: qgrid : An interactive python pandas dataframe grid visualizer for the IPython notebook. https://t.co/iB60VAQgN2 #pycon @kastnerkyle @yarikoptic soon heading to the 3 brasseurs in rue saint denis. @benoitc @sdouche @marmoute @matrixise @lothiraldan @GMLudo @apoirier1 je suis au fond de la room 517a (lightning talks) avec @sdouche @benoitc @sdouche @marmoute @matrixise @lothiraldan @GMLudo @apoirier1 Les 3 Brasseurs

http://t.co/UuR1qw88TZ @benoitc @matrixise @marmoute @lothiraldan @GMLudo @sdouche @apoirier1 3 brasseurs rue saint denis ? RT @sfermigier: Lettre ouverte aux tra√Ætres √† la R√©publique | La Quadrature du Net http://t.co/h78lCMp433 @Chris never tried so far. Maybe coveralls can do it. RT @simonfunk: Why do most academic paper pdfs lack dates?¬† I find it rather annoying. RT @crypticsea: Not exactly what I was going for... http://t.co/5aS6KII8FZ RT @rgbkrk: It has been done. The Jupyter Notebook has now been split from the main ipython/ipython repo: https://t.co/g3XKuHhbRD! RT @huitseeker: "Scikit-Learn : the Vision" @GaelVaroquaux 's Keynote at #PyData2015 https://t.co/Gn2GgWBlmJ -- great general engineering i‚Ä¶ @arnicas @fmailhot @JoelKuiper you can do great RNN demos on MNIST https://t.co/rzzVY1NaHz :) Great talk by @michelleful on sklearn text feature, pipelines, parameter tuning and geopandas on Singapore street names OSM data #PyCon2015 RT @EGouillart: Image processing with Python and scikit-image: my teaser talk at @PyDataParis is on YouTube! https://t.co/tzrsf4t0Iq .@kastnerkyle (the tiny dot in the middle) introducing machine learning to a large #PyCon2015 crowd http://t.co/uSjbmKihvm RT @kastnerkyle: PyCon! Machine Learning 101 talk at 10:50am . Slides and code here: https://t.co/ZPIq0y7K5N RT @strataconf: New Early Release- Advanced Analytics w/ #Spark- Patterns for Learning from Data at Scale http://t.co/0MQU5YNwtf http://t.c‚Ä¶ RT @teoliphant: New features in Blaze: http://t.co/ephFcW1Ux4.  Blaze lets you write data expressions the same way regardless of runtime or‚Ä¶ RT @jakevdp: Kicking off #PyCon2015 in a few minutes! See my @scikit_learn tutorial notebooks here: http://t.co/SXUCceu9fi @tarek_ziade les marketeux de nos jours disent technologies digitales. Je suis pas sur que ca soit mieux. @tarek_ziade les - de 40 ans ne disent pas NTIC. RT @karpathy: "An Empirical Evaluation of Deep Learning on Highway Driving" http://t.co/MKYPELu4gr from Brody et al. in Andrew's lab RT @benhamner: Want to learn machine learning with Python and scikit-learn? Check out our new tutorial videos with @justmarkham http://t.co‚Ä¶ @aya_tan1994 sklearn is more of a generic ml library. NLTK and Stanford NLP (java) are probably better starting points. RT @karpathy: Videos of PR2 performing a few learned tasks also shown here: https://t.co/cWUAISkxRN RT @karpathy: Sergey being awesome as usual: End-to-End Training of Deep Visuomotor Policies http://t.co/Xwf2AYL2eC Learns Image-&gt; Raw join‚Ä¶ RT @roycoding: This tutorial by @pprett on gradient boosted regression trees with @scikit_learn is immensely useful: http://t.co/Mv40zHYktt RT @davideroverso: Interesting webcast by @ogrisel &amp; @t3kcit "News from Scikit-Learn 0.16 and Soon-To-Be Gems for the Next Release" http://‚Ä¶ RT @fastml_extra: A Simple Way to Initialize Recurrent Networks of Rectified Linear Units (Hinton):
http://t.co/DN5erKQIVC

Reddit:
http://‚Ä¶ @adnothing you can probably make it count with a LSTM based attentional model such as DRAW. @julien_cabot congrats! RT @julien_cabot: Very proud to launch BlueDME a new startup dedicated to Data Monetization for the Big Data Age RT @chalmer_lowe: In [42]: run iipcy.py
is-it-pycon-yet   False
days-until-tutorials 4
days-until-conference 6 RT @strataconf: Live in 1 Hour: Webcast @t3kcit @ogrisel @bigdata- News from Scikit-Learn 0.16 &amp; Soon-To-Be Gems for the Next Release http:‚Ä¶ @SciPyConf no I won't come this year. Conflicting agendas... RT @gmarkall: "If you love Python and you're still coding in Python 2, well, then fuck you." https://t.co/uzE3kRPsup cc @ianozsvald :-) RT @deltheil: cuDNN: "Available strategies include 'prefer fastest' and 'use no additional working space'" http://t.co/OZMj2xGUv6 RT @t3kcit: Yesterdays talk about pipelining, randomized parameter search and out-of core learning is online: https://t.co/2zY8DZhQOh thank‚Ä¶ RT @namebrandon: Criteo Releases 1TB ML Dataset - http://t.co/pZ3tv8WK5b #machinelearning #datascience @jroussin007 this is the website of the dev version: not released yet. 0.16.0 is the last released version. RT @ArchUpdates: python-scikit-learn 0.16.0-1 x86_64 http://t.co/DYEVWEXNzo #Archlinux @michaelaye sourceforge had troubles over the past couple of days. RT @NeuroDebian: Amazinly enriched #sklearn release (0.16.0) is now available! RT @DebianUpload: New upload: Accepted scikit-learn 0.16.0-1 (source all amd64) into experimental by Yaroslav Halchenko... http://t.co/1Wyg‚Ä¶ Another replication of the DRAW paper, this time using Lasagne: https://t.co/XCHiKrUpsE via @meickenberg #dlearn #theano RT @ContinuumIO: The new scikit-learn 0.16.0 is now available (also the MKL-linked version), use "conda install scikit-learn" to get it! #c‚Ä¶ RT @karpathy: New blog post: "Breaking Linear Classifiers on ImageNet" http://t.co/kH5NISrYLz RT @leonpalafox: Yoshua Bengio Deep Learning Book has a new draft! http://t.co/pYWaE954K7 RT @nvaroqua: Only 2 days left to submit a proposal for #Scipy: http://t.co/ClfqLFxruE RT @bigdata: #pydata &amp; machine-learning geeks: @ogrisel &amp; @t3kcit online http://t.co/3J6xjR1LNR  &amp; in person http://t.co/nI4GRXRDLn RT @delphinel: The ICML Workshop on #MachineLearning #OpenSource Software (MLOSS) will be held in Lille on July 10th 2015 http://t.co/Y0ke0‚Ä¶ RT @samklr: Submit a lightning talk @DataParis BOF at  @devoxxfr 2015  April 09th  http://t.co/PcoXgrLxfD https://t.co/EFMcPqieRe @EzyDiscount @asmeurer should soon update it. RT @GaelVaroquaux: Euroscipy 2015: Call for paper
http://t.co/V7dYhD2FWF
Join uys in Cambridge Aug 26-29 for Python in Science

Propose tal‚Ä¶ RT @sdouche: 2015 Facebook F8 Michael Abrash 'Why Virtual Reality Will Matter to You' #video #vr - http://t.co/rYIoh5yskC @leonpalafox some work on the cross-validation API but it's getting closer. RT @lzamparo: NIPS workshop videos are up.  Rejoice! #nips2014 https://t.co/CvMLmNeLc0 @michoo_42 j'ose pas :) @michoo_42 je serai a MTL du 7 au 18. @michoo_42 cool ! A bientot ! @pierrelux @rllabmcgill would love to. Let's get back in touch next week to plan this. scikit-learn 0.16 is out! http://t.co/3m8Yv2yuFW faster DBSCAN, LSHForest, out of core PCA &amp; Birch clustering #machinelearning in #Python RT @gravitate_to_me: #SciPy John Hunter Excellence in Plotting Contest now open! $1000 cash prize, submit by April 13th http://t.co/tR5DFlp‚Ä¶ @mtlpy @jpetazzo @nnja @marmoute @notman great line-up :) @treycausey is missing from your tweet though. RT @mtlpy: #mp53 with @jpetazzo @ogrisel @nnja @marmoute on April 13th at @notman http://t.co/knaYLDlR7v @bizintelwebinar @brighttalk really busy for the coming months, let's get back in touch in June. RT @chrisemoody: RT @AlecRad Beautiful &amp; concise Gated RNN scans MNIST digits left to right with 99% acc in under 30 lines of code. http://‚Ä¶ RT @AlecRad: https://t.co/v2ggM7HNJx
Passage now supports real valued data in addition to text! Example on MNIST - 98.9% to 99.3% for GRU r‚Ä¶ @inancgumus @benhamner @kaggle exactly as described in the blog post: by monitoring train vs validation score. RT @PyDataParis: The event will probably be sold out. Hurry up. http://t.co/TYg0NdjQFW @utopiah and I meant "meant" instead of "meand" ;) I need a coffee^W club-mate. @utopiah I meand @JolyArnaud instead of arjoly. @utopiah c'est peut etre jouable pour moi si c'est apres ICML. You should ask @sedielem and @arjoly. @fpedregosa @lawrennd @walkingrandomly @Grallator LinearSVR is going to be part of release 0.16 (due today or tomorrow). @kastnerkyle is it useful on non-conv (fully connected) layers too? RT @kastnerkyle: Batch normalization is, so far, nearly as good as advertised. Easy and effective. See https://t.co/RyNSEU6kjP RT @derpapst: "Building Machine Learning Systems with Python - Second Edition" is out! https://t.co/ANq5BqQtYv #machinelearning #python RT @kastnerkyle: @AlecRad @glowingfreezer It's the same with speech. Pretty annoying! I think a majority of the gains in image tasks are du‚Ä¶ RT @lemire: Good ideas are overrated http://t.co/CUeDVlADeO @humanfromearth yes @humanfromearth our AMI has cuda and theano installed. you can git clone or wget any tutorial material from a jupyter terminal on the server RT @drfeifei: http://t.co/QARBCA4EYD has released my talk about Computer Vision at #TED2015 . http://t.co/UTDuseYuSe RT @SciPyTip: Pandas 0.16 released this weekend http://t.co/AkDjhwFHmn RT @j_gauthier: My @cs231n project on drawing human faces with conditional generative adversarial nets (cGAN): http://t.co/JsGgUxNqWR http:‚Ä¶ @DRMacIver also blog post with a couple of code snippets that highlight what's new and unique to your project. @DRMacIver release often and announce release with explicit descriptive yet concise summary on reddit/Programming and twitter. RT @Cmrn_DP: Percentile Estimation of Big Data: the t-Digest http://t.co/LofCXYGpL6 source code at: https://t.co/fEUDQpy1HO RT @strataconf: Free Webcast April 2: News from Scikit-Learn 0.16 &amp; Soon-To-Be Gems for the Next Release http://t.co/pQBafvKHxs @ogrisel @t‚Ä¶ Theano re-implementation of DRAW: A Recurrent Neural Network For Image Generation by J√∂rg Bornschein https://t.co/QpophULBpw #dlearn RT @ctitusbrown: #opensci15 @minrk referenced 'into' Python package for data conversion, https://t.co/lz73Qpg2V5 RT @graphific: Spin up a pre-installed cloud image with #Theano, #Caffe, #Torch + GPU support working: https://t.co/OBD8r2F4rA Tnx @ogrisel‚Ä¶ RT @EGouillart: I will give a talk on scikit-image #skimage at @PyDataParis in two weeks! http://t.co/gKsSxXFma7
Still time to register :-) @ylecun so it seems that FAIR is becoming the new AT&amp;T Bell Labs :) @samuelcharron @t3kcit can you please add a comment telling so show other core devs that this PR is important? https://t.co/QjCH2tPQgp RT @mblondel_ml: Protip: if you want your GSOC application to be accepted, start contributing  *one year* before to get familiar w/ the cod‚Ä¶ RT @ylecun: Our cafeteria *is* awesome. Thank you Chef Nate Eckhaus!

Also you will notice Leon Bottou and Wojciech Zaremba... http://t.co/‚Ä¶ RT @kaggle: Congrats to the Deep Sea team for winning the #datascibowl! Read an incredible blog post on their approach here: http://t.co/x4‚Ä¶ @arnicas @treycausey no pbm. But feel free to stop by if you are around at that time :) @arnicas @treycausey good news, see you there. Will you both come and sprint with us on sklearn? RT @t3kcit: I'll be speaking at OpenDataScienceCon in Boston May 30-31, together with all the awesome open data science folks! http://t.co/‚Ä¶ RT @aterrel: W. Schroeder: open source is not giving away something it's harvesting ideas collaboratively #SIAMCSE15 @Ted_Underwood @mjp39 "dark knowledge" has been rebranded as "knowledge distillation" in the newest version of the paper. RT @GaelVaroquaux: My talk on Machine Learning for Brain Imaging
http://t.co/ft3zlZV9cb
at the Machine Learning for Personalized Medicine s‚Ä¶ RT @esc___: #thisjustin "Learning SciPy for Numerical and Scientific Computing" 2nd Ed. - I acted as tech. reviewer cc @PacktPub http://t.c‚Ä¶ RT @ncoghlan_dev: Thanks to @rbtcollins, Python 3.5 will be able to easily show local variables in test failures out of the box: https://t.‚Ä¶ RT @ananelson: Hermeneutic style academic Python making me cry. RT @NLTK_org: NLTK 3.0.2 is out: adds Senna, BLLIP, python-crfsuite interfaces, transition-based dependency parsers https://t.co/iU7qTH93L8 RT @johnsheehan: Teach a dev to commit, and you‚Äôll help them once. Teach a team to rebase and you‚Äôre forever in-house Git tech support. RT @fulhack: Juergen Schmidhuber's answers from the Reddit AMA http://t.co/yD7sAgOP0V RT @sean_r_owen: Spark 1.3.0 has landed. Get it while it's hot! https://t.co/Dml7s1gDdd RT @EuroSciPy: The website for 2015 is online! https://t.co/e1R1DP0Xc2 @asmeurer @cournape would still love to get feedback from you if you encounter any test failures with the beta on your CI platforms though. @asmeurer @cournape alright, no pbm. @asmeurer @cournape I think you did last time (for sklearn 0.15b1). @asmeurer @cournape any plan to get scikit-learn 0.16b1 into your respective distributions? We would love user feedback before 0.16 final. RT @deltheil: FaceNet achieves impressive face recognition results: "a new record accuracy of 99.63%" on LFW http://t.co/ek8GPqftzi #deeple‚Ä¶ @bballstrategy don't know anything about applications to sports and NBA data, sorry. Deep Convolutional Inverse Graphics Network http://t.co/qI9gM7R9CV - Variational Autoencoders are a powerful paradigm. via @meickenberg Karol Gregor presenting Variational Autoencoders and Deep Recurrent Attentive Writers https://t.co/KLMx6fSo8f #dlearn Alex Graves presenting his work on Generating Sequences with GMM RNNs: https://t.co/Nk13cDfSd6 #dlearn RT @nvaroqua: We're organizing a small 1 day #sklearn sprint, just before #pydata paris. More information: https://t.co/HZHv6t16lN RT @hiro_asari: So, this is happening‚Ä¶ (Not public yet, obviously.) #travisci #docker http://t.co/5tiIwNKm0a New ontology (Aspects != Topics) and classification API for webpages by @Prismatic http://t.co/MKV9Pnfh5c cc @bradfordcross #data #curation RT @t3kcit: (cont.) Did I mention this is the job I have and it is the best job ever and also in NYC? Apply on the CDS website or message m‚Ä¶ RT @t3kcit: Reminder: NYU is hiring research engineers in data science. Work on your favorite stats/ml/vis etc software full time http://t.‚Ä¶ RT @Reza_Zadeh: Spark Streaming survives Chaos Monkey attacks at Netflix. A great overview of Spark fault-tolerance. http://t.co/Dcaz3AsGJc‚Ä¶ RT @bigdata: #pydata stars @ogrisel @t3kcit in a @OReillyMedia scikit-learn webcast on April 2nd! http://t.co/3J6xjQKaWj http://t.co/pguw2m‚Ä¶ RT @chrisemoody: A Word is Worth a Thousand Vectors http://t.co/meXLQQv4CD @mat_kelcey that's dedication. People dealing with class imbalance will appreciate your efforts :) @antoinedurieux congrats! @jakevdp congrats Jake :) @samuelcharron there is a PR for that. RT @t3kcit: Pre-release scikit-learn 0.16b1 is out: https://t.co/P5JoFZum5H test it with "pip install scikit-learn==0.16b1" and let us know‚Ä¶ RT @bigdata: ImpalaToGo will have fans: fork of @Cloudera Impala, separated from #Hadoop, optimized to work with S3 storage http://t.co/ELm‚Ä¶ RT @InfoQFR: Anomaly Detection with Apache Spark, par @sean_r_owen √† @DataParis - http://t.co/iKOqtUMgbD #ParisDataGeeks RT @planetpython: Ian Ozsvald: Scikit-learn training in London this April 7-8th http://t.co/2T3SNz4Ren RT @kikoland: Nice talk from @ogrisel introducing deep learning main concepts https://t.co/M1mtU2DqNj RT @tpinville: Deep Learning Workshop #1 @LaPaillasse : excellent tutorial by @syhw! ~40 deeplearners @heuritechdata @ogrisel http://t.co/I‚Ä¶ RT @syhw: We're doing a day-long tutorial/workshop on deeplearning with @ogrisel and Charles Ollion, at @lapaillasse 11am-&gt;7pm. RT @syhw: LT: feel free to follow along with:
https://t.co/U4ozJPgJ3W

http://t.co/tH6x9aUcBy

http://t.co/whVW19giGK

and ask questions he‚Ä¶ RT @jaykreps: @jaykreps You: "Really, to be RESTful, all our GETs should be POSTs" Youthful hipster: "Noooooo!" [sobbing in waves of horror‚Ä¶ RT @jaykreps: @jaykreps GETing anything produces activity data that feeds back into relevance, security, etc, which impacts the display of ‚Ä¶ RT @jaykreps: More advanced REST troll: point out that most internet services have so many activity based feedback loops there's no such th‚Ä¶ RT @SciPyConf: From SciPy 2013: Statistical Data Analysis in Python 2hr Tutorial by @fonnesbeck https://t.co/0cKAXqbVa3 @fxcoudert @EGouillart I got a couple of "Dear Professor" in the past while I am not even a PhD. Dear Firstname is fine, flattery is tiring. RT @EGouillart: Dear student applying for an internship, stop addressing me as "Dear Sir". It's 2015 and some researchers are female, yes t‚Ä¶ RT @PyDataParis: A few more talks have been announced: http://t.co/m3rVodUWkX stay tuned for more soon. RT @SciPyConf: Call for talk &amp; tutorial submissions now open for #SciPy2015! Submit today: http://t.co/2Ilt3oQrAi #Python #scipy http://t.c‚Ä¶ RT @RichardSocher: https://t.co/Vxh8h4paya -- Registration just opened for the Deep Learning Summer school! @Hyperion_HQ yes by default. There is a build option to exclude lapack though. @AlecRad thanks! RT @AlecRad: @ogrisel It's guided-bp from the all conv paper http://t.co/6ql4Gc7zTp Grad is very indistinct as positive and negative signal‚Ä¶ RT @t3kcit: New hyperparameter optimization paper uses a GP to optimize a neural net that optimizes another neural net. http://t.co/bEj1HAB‚Ä¶ @AlecRad nice. Is it the absolute value of the gradients of the class wrt the input features? Probably not because of the uniform bg @sergecell its seems to me that it won't fit in your laptop. RT @raymondh: #python 2-to-3 tip:
In Python 2, None is less than all other objects.
In Python 3, None is non-comparable to other objects. RT @astrofrog: Scientists: if you use Python, please fill out the following short survey! http://t.co/PyBWUWsNaa #python #science (please R‚Ä¶ @benjamindavies it's the k of "k-means", the number of means aka centroids aka clusters. @benjamindavies n_clusters RT @AlecRad: http://t.co/apa92Nz41l 
Name should be "Revenge of the hyperparameter optimizers" or "The hyperparameter optimizer strikes bac‚Ä¶ RT @AlecRad: Took a week of dumb mistakes to figure it out, but CNN vis algo now working :P http://t.co/lzGJvqgUSb RT @JustGlowing: 7 common mistakes when doing ML in practice http://t.co/NanzR8HJyL @edersantana @kastnerkyle @AlecRad interesting. Why model / data / classification task have you tried it on? @edersantana @kastnerkyle @AlecRad VAE reconstruction cost + VAE likelihood cost + GAN discrimination cost (with Lagrange multipliers) @edersantana @kastnerkyle @AlecRad but it's likely that the discriminative network has high level representation on its last hidden layer. @mlhamel @kastnerkyle thanks :) @kastnerkyle @AlecRad so as to fine tune the quality of the decoder / generative model w/ additional info (resilience to discrimination). @kastnerkyle @AlecRad Interesting. It might be interesting to combine the training of the 2 kinds of models in a single architecture. RT @pgbovine: congrats to @IPythonDev for 3.0!! -- http://t.co/h4LDc7M9fY -- i'll be all hipster and claim that i used IPython back before ‚Ä¶ RT @awscloud: New post on AWS #BigData Blog: "Analyzing Data in Amazon EMR using IPython" http://t.co/HJnapSMEyJ http://t.co/XXS0Iq3TvY RT @karpathy: Surprised that move from HDD-&gt;SDD only gives -30ms. And driver version/ECC/Overclock/cudnn actually make quite a lot of diffe‚Ä¶ RT @karpathy: I ran some optimizing experiments with Caffe AlexNets one step at a time, got K40 from 1800ms/fwdbwd pass -&gt; 950ms https://t.‚Ä¶ @octonion @kastnerkyle why ensembles? Any reference in my mind? @kastnerkyle @pycon maybe @mlhamel can answer that question. Would love to sprint with you. RT @kastnerkyle: Parameterizing probability densities and mixtures using neural nets is a really,  really powerful trick. @benjamindavies What about reverse ranking new documents by cosine sim to closest k-means centroid with k=100 or 1000? RT @syhw: One my farewell gifts (superposed to INRIA Paris ;-)) IMG_20150227_161535167 https://t.co/TkCjlHwVOz @GaborMelis it is precisely Adam if you read the source code :) RT @edthink: Some very nice examples of use of @scikit_learn with #opendata from Montreal (by @McGillU students too) http://t.co/BwBA67Vw2Z Open source numpy &amp; theano code to reproduce Semi-Supervised Learning with Deep Generative Models by Kingma et al: https://t.co/PhRHWK33C8 Semi-Supervised Learning with Deep Generative Models by Kingma et al. http://t.co/VWxETOHB0g impressive results on label starved problems. @glouppe @t3kcit @GaelVaroquaux You can pass existing centroids as `init` to KMeans / MinibathKMeans. @AlecRad @kastnerkyle any insights (pros and cons) to share on VAE vs GAN? RT @jedisct1: Docker Containers at Scale (Our Take on Docker Swarm) http://t.co/jJW34XVPvu @ajnu I am not familiar with R packages. RT @GaelVaroquaux: nilearn 0.1.1, with connectome plotting
http://t.co/BNctnux7XQ
Functional connectivity and machine learning for fMRI htt‚Ä¶ RT @docker: Orchestrating @Docker with #Machine, #Swarm and #Compose  http://t.co/RB9Spk6a9L by @bfirsh @aanand @vieux @aluzzardi @ehazlett @dwf i have seen that in Paris a couple of times... RT @rasbt: Python 3.4.3 was released today! Need more time to tinker my may through the changes but `enum` looks pretty neat! https://t.co/‚Ä¶ RT @haikuman: Proving that Android‚Äôs, Java‚Äôs and Python‚Äôs sorting algorithm is broken (and showing how to fix it) | http://t.co/zurAlr3ooy RT @PyDataParis: Francesc Alted (@francescalted) will be our second keynote speaker. RT @jakevdp: First new post in a while: "Optimizing Python in the Real World: NumPy, Numba, and the NUFFT" https://t.co/o4TEZsUYc5 @mat_kelcey hehe :) Still working on it: I want mosh and jupyter by default :) RT @medriscoll: "In the land of distributed systems, commutativity is king. Model your queries with [this], and they will scale." 

http://‚Ä¶ RT @mat_kelcey: i know it's a contrived example but x150 speedup by running on a gpu is just crazy cool :) http://t.co/2W7lik7S43 @DiegoKuonen @notmisha see also: http://t.co/th3ZJAvlai . the take away is that we do not know how to train wide shallow nets efficiently. @DiegoKuonen @notmisha no, DL performance is always measured on train / test split. Depth &amp; dropout is a good inductive bias / regularizer. dl-machine: AMI to run IPy notebooks with theano &amp; torch preconfigured with CUDA &amp; OpenBLAS on AWS spot instances https://t.co/LVMPtz5ioZ @EdwardRaffML good suggestion RT @stefanvdwalt: #Cython and coverage.py now play nicely together! #python Probability calibration has landed in scikit-learn master! http://t.co/rl3FwkJL3e RT @rgbkrk: Need to hire another engineer. Please join me so I don't have to talk to myself all day. RT @t3kcit: jupyterhub is a multi-user jupyter (was ipython) server deployable via docker. Awesome https://t.co/vbb4THnYvr RT @t3kcit: Try out jupyterhub at https://t.co/PyXlsFd02N which will give you full python stack, julia or R notebooks. RT @nvaroqua: #scipy's call for submissions is now open! More information on our new website: http://t.co/QOU34ZIGb6 and the video of the talk by @AlecRad on RNNs for text analysis is also online: https://t.co/Mdp0c6W9ao @Collinsjo12 @mrocklin but prototyping will happen outside of the sklearn codebase for the time being. @Collinsjo12 @mrocklin working on tooling for OOC predictive modeling w/ sklearn is on my personal roadmap for 2015. Updated .bashrc to "workon py35" as default venv. Quick check: all sklearn tests pass with #Python 3.5.0a1 #importfuture @Collinsjo12 @mrocklin that being said dask, blaze and bcolz are all very interesting building blocks. @Collinsjo12 @mrocklin we want to keep sklearn a library and not a framework. @Collinsjo12 @mrocklin out-of-core framework probably best done around sklearn partial_fit as 3rd party projects. @Collinsjo12 @mrocklin no: no more dependency besides numpy / scipy to control maintenance cost. @jakevdp +1, corollary: never pass a kwarg as positional argument when calling a python function. RT @heuritechdata: #deeplearning meetup #2 paris 
by @heuritechdata with @jdemouth, @meickenberg, and @endyfourbe
@BeMyAppFR http://t.co/xk‚Ä¶ RT @GaborMelis: @ogrisel @AlecRad The beta_1 decay was added in v2 of the Adam paper. In my tests so far it did help with convergence. Slides of RNNs for text analysis by @AlecRad at NEXT.ML: https://t.co/L6OUIwFyIg #GRU #Adam #gradclip #orthoinit #theano  #machinelearning @AlecRad your Adam implementation does not decay beta_1. Is this not necessary in practice? Just a theoretical argument for the proof? At the deep learning meetup @meickenberg is merging his theano loader 4 pretrained GoogLeNet https://t.co/kwuqJhxuOi http://t.co/rl5fqdTTJT RT @karpathy: "DRAW: A Recurrent Neural Network For Image Generation" from DeepMind http://t.co/DdaF37HxNm and video of generation https://‚Ä¶ RT @dcudel: #Python is now supported in #Azure #MachineLearning http://t.co/K7DHgySkLw @ogrisel @GaelVaroquaux RT @YhatHQ: #Python Sparse Random Projections| ≈∑hat | http://t.co/hcC9TM2nzA @fhuszar http://t.co/SrzN9K298h does not work either at the moment so it's not a DNS configuration problem. @fhuszar argl. We have to migrate the website from sourceforge to github. Will probably do that for the next release. RT @ncoghlan_dev: Running PySpark on @gluster with @ContinuumIO: http://t.co/GggQ8BpPvr RT @peteskomoroch: Introducing DataFrames in Spark for Large Scale Data Science - Databricks http://t.co/oeD1K9EJB5 via @nuzzel thanks @oce‚Ä¶ RT @peteskomoroch: [1502.04623v1] DRAW: A Recurrent Neural Network For Image Generation http://t.co/3rnfH8QqNw via @nuzzel thanks @karpathy RT @openhatch: New to @pycon sprints? We're running an (optional!) introductory workshop to help you get started. Register here: https://t.‚Ä¶ RT @ten_photos: Embed your brain... for neuro nerds, how to easily embed an interactive viewer in your HTML, courtesy NeuroVault: http://t.‚Ä¶ RT @InfoQ: AWS Release Example EC2 Container Service Scheduler Driver for Mesos http://t.co/lOzBEFC8Pd RT @LordEpzylon: #Python 3.5 is out: https://t.co/VON1moE3lG RT @ylecun: A new paper by Xiang Zhang from my NYU Lab about learning to classify texts with a convolutional net.

The new... http://t.co/e‚Ä¶ RT @esc___: CUDA &lt;-&gt; Numpy interface that looks interesting: https://t.co/rcgQqvTkOS @abhi1thakur this is why we require applicants to submit one or two PRs on EasyFix issues prior or as part of their GSoC application. @abhi1thakur we will favor GSoC students that already have a demonstrated their ability to be productive with our github-based workflow 1/2 RT @pycon: We're down to our last 250 tickets for PyCon 2015! https://t.co/1LVgjfH0Gw @abhi1thakur you still have time to contribute some fixes to apply for this summer session RT @sdouche: Project Jupyter: evolution of the language-agnostic parts of IPython into an open platform #coding - http://t.co/DXnD1QCcqH @abhi1thakur it is very important to contribute substantial (bug or documentation) fixes to the code bases before applying for a GSoC. @abhi1thakur we are currently thinking about the list topics. Have you read https://t.co/DBLTimk0TG ? @Maciej_Kula Also the learning rate should scale the gradient of the sum of the loss and the l2 penalty so I don't understand the pbm. @Maciej_Kula stop training when the validation error stops improving. Some notes by @karpathy on human performance level on imagenet in reaction to recent papers by MSRA and Google: https://t.co/NbR4MElJ0k Show, Attend and Tell: Neural Image Caption Generation with Visual Attention http://t.co/S3HgeHvqhh #computervision #dlearn #LSTM RT @karpathy: 4.8% test error on ImageNet with "batch normalization", new paper from Google http://t.co/M4UsW9i0Zo At this rate, we'll see ‚Ä¶ @Maciej_Kula fixed l2 penalty but lower from the start. @Maciej_Kula why not starting with a lower fixed value in the first place? RT @amicel: Docker 1.5: IPv6 support, read-only containers, stats, ‚Äúnamed Dockerfiles‚Äù and more http://t.co/26AAGKQTHz RT @t3kcit: I'll be speaking at  http://t.co/hnhcLkqkgJ in Boston April 27.  Great speakers from academia and industry, and are free for st‚Ä¶ RT @peteskomoroch: Robert Reich blasts Mechanical Turk http://t.co/pWmIvfp6oz RT @johnplattml: Piotr Dollar summarizes informal workshop at @UCBerkeleyBVLC about #deeplearning for #imagecaptioning https://t.co/uC88g1w‚Ä¶ @sergecell @karpathy the trained leaky relu vs relu effect is not that important. Good init seems more important for very deep nets. @karpathy the human race need bagged Andrejs. It's our only hope at this point. RT @karpathy: MSR achieves 4.94% error on ImageNet, surpassing my human accuracy of 5.1% http://t.co/XVcvVoviaQ (uses learnable leaky ReLU ‚Ä¶ RT @__mharrison__: There should soon be a glut of scientists learning python http://t.co/fJN57YSaEc @kastnerkyle @AlecRad ah this is good news! Any new results I might have missed since Ian's paper? RT @n_hidekey: Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification http://t.co/l4YvpMZDO4 „Å§„ÅÑ„Å´5%Âàá„Å£„Åü‚Ä¶ RT @kastnerkyle: @AlecRad I got your code running as a standalone script - https://t.co/rRK4mgUBNi . Though it seems you are moving to GAN ‚Ä¶ @AlecRad @kastnerkyle what is GAN? @aterrel some candidates here: http://t.co/L0GiMEEvwb RT @aterrel: Is your Data Stack Python powered? Working on my talk for @strataconf w/ @mrocklin love to hear how Python is deployed in prod‚Ä¶ Blocks helps you build neural networks using Theano and live @BokehPlots based monitoring http://t.co/nLu6Tgk18N MIT licensed flake8 &amp; doc @goulagman @meickenberg @GaelVaroquaux yes he is! @goulagman @meickenberg @GaelVaroquaux I run Python 3, I will wait for 0.2 next month ;) #previousRT to be considered in light of @AndrewNg_'s worries about robot truck drivers vs the Terminator: http://t.co/ScKfiBmQI4 RT @randal_olson: Interactive #dataviz: The Most Common Job In Every State: http://t.co/xhwgwS8qZt http://t.co/2B9V8LdAwD RT @GaelVaroquaux: Looking at the nilearn gallery: a few lines of Python generate different pretty brain images
http://t.co/MRG0FXEeOB http‚Ä¶ RT @strataconf: #StrataHadoop London's #DataScience Track features experts @ogrisel @bmlever @kevinschmidtbiz @lvicentesanchez + more http:‚Ä¶ @DRMacIver glad I could help. I can tell you that a lot of suffering went into those 10 lines of windows batch file. RT @bradfordcross: super excited to release the first endpoint in our interest graph api -- you give us a url, we give you its topics http:‚Ä¶ RT @PyDataParis: We're going to organise a sprint dedicated to Scikit-Learn the day before PyData Paris (= April 2). RT @PyDataParis: The website announcing PyData Paris 2015 is live: http://t.co/TYg0NdjQFW I gave to http://t.co/udJAhGbdyd (french) to support the development of targeted cancer therapies, immunotherapy and data driven research. @amicel @utopiah @acemtp @marmoute @bagnier J arrive RT @cournape: Intersting talk on property testing #fosdem2015 python lib implementation avalaible on github 
https://t.co/oOVdIkFMkQ RT @deltheil: How many parameters has the 16-layer Deep Conv Net from VGG? (take care of padding!) http://t.co/U03iKwCztv @marmoute @amicel @acemtp @utopiah OK j arrive dans votre coin. @amicel @acemtp @utopiah Bon c est mort la salle python. Je vais faire un tour au b√¢timent k. @amicel @utopiah @acemtp En fait je vais essayer d aller au talk sur les objets cpython 3.4 @amicel @utopiah @acemtp Je vais essayer de te retrouver en janson @amicel @utopiah Vous etes ou vous? je suis actuellement dans la salle Infrastructure as a Service. Je veux voir Ceph et Cinder cet aprem. RT @asymfree: you are scientist and submit to #arxiv? ever checked its exponential growth?
http://t.co/ovUeKEpMZc

#openscience http://t.co‚Ä¶ RT @rasbt: #Python usage survey 2014 via @randal_olson trend goes (slowly) into the "right" direction!http://t.co/hiDfGmJAeN http://t.co/nG‚Ä¶ @ametaireau o/ RT @cote: In Brussels for #FOSDEM. Gonna need a bigger boat... http://t.co/qJd5KMR6G5 RT @fonnesbeck: @oceankidbilly in the absence of direct notebook rendering by GitHub, @jiffyclub 's browser extension is great: http://t.co‚Ä¶ RT @Serianox_: LLVM with C (https://t.co/iJJO1f3Knl), Python (http://t.co/EHbw6P2aDU) or Haskell (http://t.co/wt5SHfvN4U). Now everyone is ‚Ä¶ RT @blattnerma: Advice for applying Machine Learning #machinelearning http://t.co/KDs4cQz1Ul @pchapuis Will arrive at the delirium caf√© around midnight as usual. @jon1012 I am. Are you on the afpy train? @t3kcit I remember @minrk implemented it while seating at the back in the audience of a talk at PyData SV a couple of years ago. @bradarndt No, not as far as I know. RT @AlecRad: @ogrisel @kastnerkyle https://t.co/pcQn3aDSLo
Ugly prototype gist - warning - was written from 12 to 8 am during blizzard - co‚Ä¶ @utopiah @amicel Ca marche ! @AlecRad @kastnerkyle impressive demo. Do you have the code somewhere online? RT @AlecRad: Random paths through latent space of deep conv/deconv variational autoencoder on LFW.

https://t.co/mfoK8hcop5 RT @benhamner: Spotify's shuffle used to be random. Users complained, so they created a less-random version that felt truly random https://‚Ä¶ @rgbkrk I want to debug it a bit further as a gist then open an issue on the dill github to discuss where it should go. RT @rasbt: "Visualizing DBSCAN Clustering" -- That's so cool (btw. my fav. Swiss Army knife clust. algo) http://t.co/pm8llLC2KY http://t.co‚Ä¶ @rgbkrk The legacy 2 focused design motivated me to start anew based on a active project (dill) that covers 90% of the needs. @muratk3n AWS EC2 g2.2xlarge are not too bad and cheap $0.07/h as spot instances. @muratk3n efficient distributed CPU computing is not necessarily simpler to implement than single GPU computing. @t3kcit I played a bit with mrec: https://t.co/iJRAnaDXGc but there is no more development on it. @arnicas @OlivierBot_13H Haha @rgbkrk Nope @rgbkrk It's still buggy but I am making some good progress. @rgbkrk i started experimenting with a dill based alternative to cloupickle in this gist: https://t.co/CazI5B8tTk RT @ylecun: The Deep Learning Summer School sponsored by CIFAR, will take place early August. http://t.co/kunv0ZsEL7 RT @bigdata: Fantastic lineup of speakers &amp; sessions at #pydata #stratahadoop in San Jose http://t.co/VHlCDhT2zz @wesmckinn @t3kcit @teolip‚Ä¶ RT @chris_bour: How Spark MLlib achieves distributed Random Forests and Boosting http://t.co/nf5Qjt0dCO RT @t3kcit: New York times uses scikit-learn https://t.co/VbvAxqHGLP RT @philshapiro: Center for Data Science at NYU, working on open source and open science. Follow at @t3kcit @rgbkrk dill does not pickle the source of functions defined in __main__ by default as cloudpickle does. Very important for interactive use. @dwf @kastnerkyle sorry I just found it. I had not seen the 50 items per page limit. Swichted the paging model to "all" instead. @karpathy what are your thoughts on "Superintelligence"? worth reading? @dwf @kastnerkyle I cannot find the Adam paper on CMT although it is tagged ICLR2015 on arxiv. Any reason why this is the case? @rgbkrk please let me know if you ever port cloudpickle.py to work w/ Python 3 ;) RT @maryjofoley: MS has acquired Revolution Analytics, a vendor of R language-based open- and closed-source analytics solutions: http://t.c‚Ä¶ RT @twiecki: Learn #DataScience with #Python in your browser https://t.co/WWn389dQWI Very cool project. @dmarthal although I don't think it can have any significant impact on the cross-validated score of your NB models. @dmarthal numpy has historically stemmed from a numerical engineering culture (matlab-like). For scikit-learn it might be considered a bug. KiKu - Marcher sur la t√™te (feat. Blixa Bargeld) https://t.co/Q0Y8xcsA5g #music @ds_ldn the choice of the AGPL is crappy though: it goes against the spirit of the liberal licenses of packaged libs (Apache, BSD, MIT...). RT @ds_ldn: This Docker container&gt; JetPack: Deeplearning4j+GraphLab+H2O+Julia+MLlib+Theano+Torch7+VowpalWabbit https://t.co/6YSJGIB9bg RT @t3kcit: Vistrails, a provenance focused visualization and data processing system now has scikit-learn integration: http://t.co/8YPNjwyX‚Ä¶ RT @AlecRad: Evidence of general language representations emerging from purely supervised RNN trained on binary sentiment labels! http://t.‚Ä¶ @AlecRad also skip predictions if confidence is too low. Or at least report the confidence of the prediction in the UI. @AlecRad nice. I think it would be interesting to add a "garbage" class trained with blurry images from moving or out of focus cameras. RT @kastnerkyle: Pytables EArray with compression filter is amazing. I don't know why I never tried this  before! RT @karpathy: Caffe now evaluates AlexNet in 24ms / image on mobile GPU (NVIDIA Jetson TK1), on 10W budget, nice. http://t.co/BgooDMCMEV (v‚Ä¶ RT @OriPekelman: Ok temps de comitter votre code. #ParisDataGeeks commence dans une heure. http://t.co/MecT9r7Gq8 RT @fastml_extra: Adam: A Method for Stochastic Optimization (good for sparse gradients):
http://t.co/wrjoa9pPmG

Python/Theano code:
https‚Ä¶ RT @mblondel_ml: Kernel ridge regression is now in scikit-learn master (better late than never!) http://t.co/NXiHNpCIoM RT @ch402: With deep learning and dimensionality reduction, we can visualize the entirety of Wikipedia. http://t.co/SlHZe8fOpv http://t.co/‚Ä¶ RT @peteskomoroch: After warning us of SkyNet, @elonmusk is building a giant network of satellites which "hasn't yet been given a name" htt‚Ä¶ RT @ted_dunning: Evolving high-efficiency wind turbines without CFD computations.  Very cool idea.

http://t.co/IJ2Z1ZVGBz RT @kdnuggets: With general #AI the most interesting ideas (recursive self-improvement) are also the most dangerous @SamHarrisOrg http://t.‚Ä¶ RT @TechCrunch: French Startup Dataiku Grabs $3.6M To Continue Developing Big Data Software http://t.co/Puy165YKs1 by @ron_miller RT @ewanbirney: Really disappointed in the H2020 cuts to the ERC (European Research Council) - undoubtedly the best Pan-European funding sc‚Ä¶ @ionelmc nope, but I have not searched extensively myself. @AlecRad @ted_dunning see also nbsvm: http://t.co/uv8p6RWL13 @AlecRad @ted_dunning A blend of multinomial NB and LR or linear svm might be even better. @AlecRad @ted_dunning Which dataset? Sentiment140? Amazon reviews? @AlecRad @ted_dunning Use the hashing vectorizer instead (no min_df for this guy though) RT @AlecRad: The issue with RNN text analysis is small datasets - not the model itself - 1m examples = 30% error reduction on LR. http://t.‚Ä¶ Facebook AI Research released fast deep learning primitives implemented as torch modules: https://t.co/78243KCv9U #dlearn #machinelearning RT @tcurdt: FOSDEM has a "search dev room" this year - with some interesting talks covering lucene, elasticsearch, spark, hadoop and solr Lazytweet: can anybody suggest a diagnostic / profiling tool for GIL lock contention in #Python w/ Cython code? RT @TlkngMchns: Ep2 Ilya Sutskever, magical thinking in ML, we start thinking about talking about ethical q's in AI #machinelearning http:/‚Ä¶ RT @balazskegl: .@SaclayCDS #DataScience bootcamp on the #HiggsML challenge at @PROTO204, backend #Juju #Azure #ubuntu #canonical http://t.‚Ä¶ RT @jakevdp: This just might be the largest single file available on the internet... http://t.co/9NMrd2OSbH #CtU2015 http://t.co/xpGaquzxyr RT @utopiah: That scary moment when @FLIxrisk writes about AI 1 month ahead of the current date... http://t.co/XBWk6PJAzS Ilya Sutskever on large and deep neural nets: http://t.co/gtLA0eeuCo good intuitions and important implem tricks. #machinelearning #dlearn @karpathy thanks, I had not read it. @karpathy do you plan to publish on this? @karpathy looks like a better net architecture will be required to reach your own (human) score. RT @karpathy: Baidu gets 6% ImageNet top 5 error (prev state of art 6.6% by Google) http://t.co/YReQXQONY2 lessons: moar GPU, moar data aug‚Ä¶ @nkbuduma @arnicas Thanks. RT @nkbuduma: @muratk3n @arnicas @ogrisel @mikeloukides Here's the updated link: http://t.co/p982QrGYme @mikeloukides @nkbuduma Indeed the title change has changed the URL... @utopiah @amicel Ah c est chez Le√≥n cette ann√©e. Du coup je sais pas trop. @utopiah @amicel Cool. Tu pr√©vois d aller a l ap√©ro / repas le samedi soir ? @utopiah tu vas au FOSDEM? Faudrait au on se rencontre IRL depuis le temps qu on se follow sur twitter... /CC @amicel RT @nkbuduma: Check out my new post on recursive neural nets! http://t.co/Pav1YeWuYp
#deeplearning #neuralnets When will the #ICLR2015 review will open on http://t.co/2HV8ZOFShZ ? Or are they already happening somewhere else? @mblondel_ml http://t.co/D24mRPSjwQ @abderhasan I used a blend of atom, sublime and vim. Mostly atom at the moment. @stonebigdotdot hopefully yes. RT @cvaartjes: https://t.co/ZLURDcCka8 performance in 1.5-3.0x range of in-mem systems. Building on the excellent #bcolz by @FrancescAlted ‚Ä¶ RT @DataParis: http://t.co/0v7QRKmeBH waiting list for next #ParisDataGeeks at 276. So If you don‚Äôt plan on coming .. please liberate your ‚Ä¶ RT @databricks: Introducing ML Pipelines: A New High-Level API for MLlib http://t.co/SZit7tTIYt RT @pycon: The @PyCon tutorials are filling up fast! Have you registered? https://t.co/Pc5HGcyGlG RT @fastml_extra: LSTM library benchmarks:
http://t.co/CzcQbT17Q5

Also
https://t.co/UnLFXr1YjC

Reddit:
http://t.co/6EgkzXewZU @t3kcit @Collinsjo12 @mrocklin thanks for the heads up.  Will have a look when back from semi-offline break. RT @twiecki: On demand cluster in #Docker containers using multyvac (previously picloud) https://t.co/XlYI41rPbs #HPC @beaucronin indeed. @beaucronin but where is the microphone? @xamat congrats! RT @ConanMcMurtrie: Excellent! Brand new machine learning podcast, Talking Machines. http://t.co/Z8ZfQeftvd
#machinelearning #datascience #‚Ä¶ RT @jakevdp: In case you were curious why there's a KDTree in both scipy and scikit-learn: https://t.co/WgrYjfuyK8 RT @mblondel_ml: Short summary of solvers currently implemented in lightning http://t.co/IE9C2pD4TB lightning on github: https://t.co/tXRoO‚Ä¶ RT @benhamner: A Caffe implementation of the GoogLeNet model from this year's ImageNet challenge https://t.co/f3cFcNXv4X http://t.co/Po889V‚Ä¶ @swagKumar @agramfort good luck for your future endeavours RT @glouppe: Looks like physicists are already using Scikit-Learn for real HEP analysis! http://t.co/m2oxXrb8Kc CC: @betatim @balazskegl @p‚Ä¶ RT @sguada: @karpathy now GoogleNet is public within Caffe http://t.co/NpceVrht83 I just released wheelhouse-uploader 0.7.1 with proper handling of PEP 440 version numbers: https://t.co/PdGJ5aGVwK #python #pypa @johnplattml Platt-scaling and isotonic calibration are being implemented in sklearn: https://t.co/SLNr524EKi feel free to pitch in :) Stochastic Gradient Boosting: Choosing the Best Number of Iterations by Yanir Seroussi: http://t.co/WTGbKAargq #machinelearning #previoustweet did not compare to the NBSVM shallow baseline though:  http://t.co/WauoMPajwZ Text classification with 1D CNNs directly trained on high dimensional sparse input (BoW features): http://t.co/jKooNMqryw #dlearn FitNets: Hints for Thin Deep Nets http://t.co/1BS2xZ6p1t #dlearn #machinelearning #knowledgedistillation RT @minrk: pip 6 brings some important fixes for binary wheels on OS X. https://t.co/fumqz6nxoB @Hyperion_HQ note that recent versions of debian / ubuntu include recent enough versions of the openblas package. RT @sergecell: Yann LeCun @ylecun choice of ICLR papers
https://t.co/e0FlAXPXDc @Hyperion_HQ I have a dockerfile: https://t.co/W59AVCOPlI @gawel_ hehe RT @ylecun: New ICLR 2015 submission from FAIR on fast ConvNets using FFT on CUDA: http://t.co/9Yh7p10eUO

"Fast... http://t.co/nUC7ehjXo1 @id_butterflies correct @id_butterflies http://t.co/ZVFt0a8YZm RT @jedisct1: With a sub-10% image classifier, a decent face detector, here comes ccv 0.7 http://t.co/Ou6mGjJ8tU #computervision RT @notmisha: Go playing convnets getting better http://t.co/bCziMMTRnf @twiecki congrats! RT @pdebuyl: The proceedings of #euroscipy 2014 are online on #arxiv http://t.co/CucDug7jy5 #scipy happy reading! RT @pwang: Open-Sourced Advanced Analytics is increasing‚Ä¶ #PyData http://t.co/M07ggqNpbX RT @andyhickl: [1412.6115] Compressing Deep Convolutional Networks using Vector Quantization http://t.co/Bez4dxq3GM RT @_onionesque: Video of a lecture by Geoff Hinton at MIT BCS - What's wrong with convolutional neural nets? http://t.co/gjA76epCdL Interview with @ylecun on ConvNets in general and the Kaggle CIFAR-10 challenge http://t.co/pqIAhjCP5h RT @josephmisiti: Cause And Effect: The Revolutionary New Statistical Test That Can Tease Them Apart https://t.co/qGXDYJ3fbT RT @yokofakun: dual arm robot in lab automation = Robots will steal your jobs ? (via @torstenseemann ) https://t.co/reVjeDXQzB RT @sergecell: DNN with random weight preserve distance between data
http://t.co/VHpjrcKlQi @bluetouff 9kbps √ßa suffit pour utiliser tmux sur le C&amp;C d'un botnet. RT @karpathy: Yesterday I released code for training Deep RNN/LSTM in Javascript (because why not). Fun sentence-memorization demo: http://‚Ä¶ RT @pwendell: Spark 1.2 is out! GraphX graduates from alpha, full H/A mode for Spark streaming, new ML pipelines API &amp; much more! http://t.‚Ä¶ RT @mdreid: Another NIPS round up, this time from my colleague at NICTA, Justin Domke: http://t.co/LwVfLuF17D @jtaylor108 @cyrillerossant @WilsonEricN @jakevdp I use python 3 since more than 1 year and it's very rare to have to switch to py2 venv. @WilsonEricN @jakevdp it's easy to write python 3 code with python 2 compatible so that you can switch when you need. Approximate Nearest Neighbors search with LSH Forest has landed in sklearn master: http://t.co/eucBZgraKc @_olivier_ ok, enjoy :) RT @_olivier_: Please update your #git client! https://t.co/f1MFHOJtnF @_olivier_ j'ai pris des billets pour 2001 a space odyssey, et toi? @dsblanch @jakevdp for that case you have the pasteurize.py script in http://t.co/xmvGPfp9Yi :) @dsblanch @jakevdp and the futurise.py script can automatically rewrite all those pesky print statements as function calls for you! Baidu Research joins the club of end to end Neural Automatic Speech Recognition developers: http://t.co/sLBgUBvvha #dlearn #augmenteddata RT @ContinuumIO: Numba 0.16.0 is now available on all platforms- it now depends on the new llvmlite library. Use "conda update numba" to ge‚Ä¶ RT @mlhamel: Congratulation to the @postgresql team for their 9.4 released. This one again looks like amazing (jsonb, scalability and perfo‚Ä¶ RT @rgbkrk: Another brilliant bit from folks at @quantopian - IPython/Jupyter notebook storage on Postgres: https://t.co/Vs57MCTcuM @jakevdp I would add: are you the maintainer of that code base? yes =&gt; convert to Python 3 w/ futurize.py from http://t.co/MjlEG1lnv3 RT @jakevdp: Should I use Python 2 or Python 3? A comprehensive flow-chart: http://t.co/1weawXPkwQ RT @dvdgrs: A Test Collection for Email Entity Linking http://t.co/p59pFBThkh RT @asmeurer: @tdhopper in SymPy we subclass DeprecationWarning and enable it by default https://t.co/DPrAkfTdkx. All library authors ought‚Ä¶ RT @travisci: On our new stack build times for the JRuby project have been cut down by 40% thanks to caching and more resources: http://t.c‚Ä¶ RT @scharrier: I've just tested the new Docker based @travisci architecture. Works like a charm : builds are really launched instantly ! RT @syhw: The secret to the Uber economy is wealth inequality http://t.co/Bu3GzwnWMA (no self-driving cars yet‚Ä¶) RT @PatrickMcFadin: Freebase is closing down; data going to WikiData https://t.co/5iAApPhb1p @dnouri this is a very good tutorial. I like the explicit iterative process of designing the model and the intuitions given at each stage. @PythonStatus it seems that pypi login does not work. RT @dnouri: Wrote a tutorial: Using convolutional neural nets to detect facial keypoints. http://t.co/wYFMt6Mp8o
#DeepLearning #Lasagne #Th‚Ä¶ @ynd thanks! RT @fastml_extra: Machine Learning Trends from NIPS 2014, by John Platt:
http://t.co/lQfLyHnc13 @ShlomoArgamon @johnmyleswhite @talyarkoni @davidjayharris use sklearn if it's fast enough. @ShlomoArgamon @johnmyleswhite @talyarkoni @davidjayharris the sklearn implementation is known not to be the most efficient. RT @pycon: Registration for tutorials is now enabled! Check out the schedule and reserve your spots today: http://t.co/dhS3WlZkik RT @magicleap: Father of the "Metaverse" - &amp; our inspiration - @nealstephenson joins Magic Leap http://t.co/4jvBjeoflK http://t.co/czGNuPLE‚Ä¶ @sedielem @AlecRad @kastnerkyle @johnmyleswhite @tcovert on train or validation? @sedielem @AlecRad @kastnerkyle @johnmyleswhite @tcovert I thing nag is often the best if you grid search learning rate schedules properly. Learning from sequences to sequences w/ LSTM for machine translation by Ilya Sutskever¬†/ Deep Learning Master Class http://t.co/OXMHF7CGN4 RT @JolyArnaud: You can now tame your favorite SLURM or SGE supercomputer with clusterlib. The 0.1 is out. https://t.co/qFOCYLYBOn @mfcabrera I don't know, @t3kcit might know better. RT @bugraa: My tutorial on Scikit-Learn is up: http://t.co/KaM6Hgjghv | Python Notebooks: http://t.co/juZHQVD4cC

Great to be at NY again! @tsoulcie h√©h√© Nice, personal overview of a slice of #NIPS2014  by @PaulMineiro: http://t.co/LO31WKuRCV via @mdreid #machinelearning RT @LorenaABarba: Find out if your employer will match your donation to @numfocus‚Äîdiversity scholarships for #SciPy &amp; coding for girls http‚Ä¶ @ynd nice paper on SFN! Have you evaluated the training error of (Nesterov) momentum SGD or adagrad / adadelta / rmsprop on this data? @twiecki yes, at the time I wanted to experiment with a bugfix that was only available in the latest version. RT @vsbuffalo: John von Neumann's quote about the merit of empirical ideas in mathematics is terrific: http://t.co/KRL2Segh31 RT @benoitc: also the full schedule of #pycon has been released https://t.co/7SKcgFDmJ8 #python @__genji__ yes it's @AlecRad, see the rest of the conversation: https://t.co/p8XjbQKutk RT @t3kcit: Deeppy, a "keep it simple" approach to doing deep learning in Python https://t.co/L0gZ8JqSI6 Early stages but looks pretty swee‚Ä¶ RT @steve_silvester: @asmeurer writing a travis.yml file feels like debugging using a telegraph machine with a monkey on the other end RT @lzamparo: Hannes Breitschneider's take home message: deep learning on DNA sequences works.  #NIPS2014 RT @sedielem: sklearn-theano + colour histograms + t-SNE on a bunch of album covers taken from my website http://t.co/ONMBjPg78q http://t.c‚Ä¶ RT @sedielem: A guest post by Jan Schl√ºter: The fastest convolutions in Theano with meta-optimization! http://t.co/A9VcYflmbJ @stanfordnlp it does mean that adadelta will reach a solution as good as a properly tuned NAG. RT @vsmolyakov: non-convex in a good way: overcoming saddle point attractors in deep net objective functions: http://t.co/Oh7OtqFL6s #NIPS2‚Ä¶ 