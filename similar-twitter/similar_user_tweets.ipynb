{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from birdy.twitter import UserClient, BirdyException \n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "key = [\n",
    "    [\"\", \"\", \"\", \"\"]\n",
    "]\n",
    "\n",
    "client = UserClient(key[0][0], key[0][1], key[0][2], key[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['karpathy', 'AndrewYNg', 'drfeifei', 'AlecRad', 'KirkDBorne', 'hmason', 'hadleywickham\\n']\n",
      "4\n",
      "567\n",
      "['NateSilver538', 'googleresearch', 'googleanalytics', 'dez_blanchfield', 'TwitterData', 'HansRosling', 'MongoDB', 'ValaAfshar', 'avinash', 'analyticbridge', 'IBMBigData', 'dr_morton', 'DARPA', 'KirkDBorne', 'simonlporter', 'hortonworks', 'AndrewYNg', 'GreatAnalytics', 'Informatica', 'IBMcloud', 'IBMAnalytics', 'tableau', 'cloudera', 'MaxCRoser', 'jeremywaite', 'hmason', 'DataScienceCtrl', 'AKQA', 'BernardMarr', 'Ronald_vanLoon', 'gruset', 'dhawaldamania', 'kdnuggets', 'mitsmr', 'MSPowerBI', 'kaggle', 'DataSciFact', 'flowingdata', 'JDavidMorris', 'mikejulietbravo', 'plotlygraphs', 'drfeifei', 'BigDataGal', 'EvanSinar', 'SebastianThrun', 'karpathy', 'mgualtieri', 'Mark_Carey', 'Doug_Laney', 'OttLegalRebels', 'EXAGolo', 'ylecun', 'ipfconline1', 'simplystats', 'Rbloggers', 'mapr', '7wData', 'BigDataBlogs', 'MrDataScience', 'hadleywickham', 'DimensionData', 'pivotal', 'BoozAllen', 'rstudio', 'RLangTip', 'dpatil', 'Teradata', 'novaspivack', 'data_nerd', 'bigdata', 'peteskomoroch', 'MicroStrategy', 'SASsoftware', 'DJ44', 'ML_toparticles', 'WesleyGransden', 'strataconf', 'LinkedInEng', 'SmartDataCo', 'DodgeData', 'odsc', 'stanfordnlp', 'DeepLearningHub', 'jtoy', 'mrogati', 'MIT_CSAIL', 'hackingdata', 'Datafloq', 'bobehayes', 'merv', 'SASanalytics', 'v_vashishta', 'TDWI', 'DBaker007', 'ML_NLP', 'Data_Informed', 'TheTerminal', 'justincutroni', 'AirbnbEng', 'marcusborba', 'ralphopinions', 'revodavid', 'fchollet', 'NandoDF', 'ddjournalism', 'DataKind', 'gp_pulipaka', 'rdpeng', 'DadiCharles', 'ds_ldn', 'cvo_website', 'deeplearning4j', 'howarddresner', 'EXASOLAG', 'hugo_larochelle', 'PaulTDenham', 'wesmckinn', 'AlexanderD_Beck', 'jameskobielus', 'magentobi', 'drewconway', 'NoSQLDigest', 'goodfellow_ian', 'johnmyleswhite', 'databricks', 'galvanize', 'williammcknight', 'antgoldbloom', 'jure', 'ogrisel', 'dabeaz', 'jackclarkSF', 'AnthonyNystrom', 'soumithchintala', 'apachekafka', 'bevelson', 'shawnrog', 'jaykreps', 'NeilRaden', 'GilPress', 'Dataversity', 'm_sendhil', 'gdb', 'PyData', 'ContinuumIO', 'DataJunkie', 'michael_nielsen', 'ThoHeller', 'BecomingDataSci', 'stoyanstefanov', 'FrankPasquale', 'BaiduResearch', 'amuellerml', 'WeAreManthan', 'alexjc', 'trieloff', 'ema_research', 'JohnSnowLabs', 'rasbt', 'jhurwitz', 'ProjectJupyter', 'datagenius', 'jakevdp', 'hspter', 'VanRijmenam', 'lisachwinter', 'tableaupublic', 'nprviz', 'seanjtaylor', 'alteryx', '_krisjack', 'thinkmariya', 'sedielem', 'mjcavaretta', 'iamtrask', 'jeremyphoward', 'shakir_za', 'jaimefitzgerald', 'datameer', 'MikeTamir', 'fperez_org', 'chrisalbon', 'DiegoKuonen', 'UCSDNeuro', 'tlipcon', 'TonyBaer', 'xamat', 'DataCamp', 'josh_wills', 'petewarden', 'benhamner', 'ch402', 'teoliphant', 'IPythonDev', 'Datawatch', 'dataandme', 'samim', 'johnlmyers44', 'nehanarkhede', 'edd', 'deeplearningldn', 'ctricot', 'drob', 'dataiku', 'pmddomingos', 'AnalyticsCloud', 'mikeferguson1', 'YhatHQ', 'gcosma1', 'michellewetzler', 'ryan_p_adams', 'datastories', 'Nigel_Shadbolt', 'Springboard', 'pwang', 'AnalyticsVidhya', 'sachinuppal', 'storywithdata', 'BerkeleyData', 'hardmaru', 'HealthCatalyst', 'arnicas', 'dennybritz', 'samcharrington', 'gtcomputing', 'VizWizBI', 'DeepSpiker', 'etzioni', 'lawrennd', 'CVCND', 'deanwampler', 'gwenshap', 'haldaume3', 'captainsafia', 'biogerontology', 'dtunkelang', 'FILWD', 'KenOConnorData', 'GaelVaroquaux', 'WalterReade', 'Medidata', 'wiseanalytics', 'earnmyturns', 'nschaetti', 'Bilafer', 'simplilearn', 'jennifermarsman', 'rmelody', 'notmisha', 'siah', 'tom_e_white', 'DataRobot', 'hannawallach', 'binaryloom', 'data_hpz', 'DominoDataLab', 'MIRIBerkeley', 'tomaspetricek', 'BigData_paulz', 'Affinio', 'Onalytica', 'h2oai', 'echen', 'erichorvitz', 'rockyd', 'egrefen', 'twiecki', 'Texata', 'mike_schatz', 'plamere', 'janexwang', 'clarecorthell', 'juliasilge', 'quantombone', 'rachelreese', 'fhuszar', 'RobertsPaige', 'm_learningnews', 'Smerity', 'AaronAuldDE', 'turiinc', 'AiGameDev', 'genekogan', 'RProgLangRR', 'ericoguizzo', 'ClouderaEng', 'thedatacrunch', 'partiallyd', 'mrnews', 'Reza_Zadeh', 'jonathonmorgan', 'CognitiveClass', 'teamrework', 'SnowflakedUK', 'deliprao', 'math_rachel', 'ellisonbg', 'startupml', 'BigDataSpeaker', 'jonesabi', 'Miles_Brundage', 'PyImageSearch', 'DamianMingle', 'DSI_Columbia', 'benjamingaines', 'AngeBassa', 'fastml_extra', 'DataMinerUK', 'Numenta', 'clarifai', 'im_sam007', 'MLconf', 'graphific', 'VisualFSharp', 'LorenaABarba', 'KaggleCareers', 'mdreid', 'CMastication', 'rbukralia', 'nervanasys', 'IBMdatamag', 'joelgrus', 'bigmlcom', 'ImDataScientist', 'ClouderaU', 'clmt', 'enjalot', 'BarryDevlin', 'SciPyConf', 'holdenkarau', 'BoozDataScience', 'brendan642', 'tonyojeda3', 'justmarkham', 'hire_ai', 'BYAnalytics_en', 'davidwhogg', 'kopshtik', 'syhw', 'robdthomas', 'Afafa_h', 'fulhack', 'suchisaria', 'thisismetis', 'PredictionIO', 'deanabb', 'indicoData', 'IgorCarron', 'infolabUK', 'john_lam', 'mattmayo13', 'Cmrn_DP', 'kchonyc', '__DataTau__', 'kscottz', 'acangiano', 'socialmetrix', 'ApacheDrill', 'berkeleyjess', 'thedatascilab', 'HarlanH', 'aheineike', 'mblondel_ml', 'beaucronin', 'JordiTorresBCN', 'aria42', 'dustinvtran', 'DataCommunityDC', 'RadimRehurek', 'boudicca', 'galois', 'RandomlyWalking', 'datiobd', 'lucene_solr', 'wzchen', 'nuclai', 'josephmisiti', 'Ellen_Friedman', 'yoavgo', 'DataSciGuide', 'tgrall', 'datapopup', 'mat_kelcey', 'databaseguru', 'jeroenhjanssens', 'julie_craig', 'allafarce', 'OReillyAI', 'mtyka', 'dwf', 'AllenDowney', 'gjreda', 'nikete', 'IndiaAM', 'metabrew', 'ch_doig', 'Alan_D_Duncan', '__ReJ__', 'QatarComputing', 'compvision', 'glouppe', 'frnsys', 'VizPainter', 'dreasoning', 'conormyhrvold', 'EddieACopeland', 'ericcolson', 'zelandiya', 'evelgab', 'NuanceInc', 'HITAnalytics', 'tonyshaw', 'noahmp', 'KyleCranmer', 'katsnelson', 'tamaramunzner', 'agramfort', 'jonathandrummey', 'jacobeisenstein', 'thefreemanlab', 'BayesianNetwork', 'Lumidatum', 'abunchofdata', 'minrk', 'DataVizAI', 'WiMLworkshop', 'splicemachine', 'kestelyn', 'DistrictDataLab', 'harrism', 'mrtz', 'seb_ruder', 'AshDamle', 'profjsb', 'dribnet', 'Securonix', 'analysis_factor', 'jeffheaton', 'jesolem', 'CiscoAnalytics', 'oceankidbilly', 'lmthang', 'techmilind', 'InfoMgmtExec', 'wahalulu', 'vkrakovna', 'brandondamos', 'honnibal', 'atveit', 'ryanswanstrom', 'dataquestio', 'DrexelPooja', 'louisdorard', 'ilparone', 'TedOBrien93', 'LUCA_D3', 'Mbussonn', 'eturner303', 'ArnoCandel', 'mpd37', 'chrisemoody', 'TungstenBigData', 'tianhuil', 'jeremyjkun', 'LH', 'abursuc', 'bkanber', 'srisatish', 'txoinc', 'DataScientistFr', 'rgbkrk', 'agibsonccc', 'bigdatalondon', 'abestanway', 'BigDataBuzzNet', 'arjones', 'BBVAData', 'wiseio', 'alexip', 'catherinebuk', 'edersantana', 'WIOMAX_PA', 'mrocklin', 'benm', 'davidandrzej', 'communicating', 'mhmazur', 'stefanvdwalt', 'bibleviz', 'MrChrisJohnson', 'chrshmmmr', 'umlujnews', 'ShellyFan', 'smly', 'VasantDhar', 'cyrillerossant', 'drivendataorg', 'zacharylipton', 'BigDataGeeks', 'CloudsWithCarl', 'adamlaiacano', 'CompSciOxford', 'sgrifter', 'TomAugspurger', 'gethue', 'kbreitman', 'born2data', 'WIOMAX_DC', 'donnaburbank', 'dnouri', 'JustGlowing', 'hmCuesta', 'rbhar90', 'abock', 'kastnerkyle', 'cfregly', '_onionesque', 'acrahen', 'jseabold', 'BenSullins', 'WithTheBest', 'alxndrkalinin', 'zaxtax', 'RodyZakovich', 'arnonrgo', 'fpjunqueira', 'JasonPunyon', 'SpeedeonData', '_DaveSullivan', 'EvaNahari', 'gideonmann', 'josh_tapley', 'YadFaeq', 'npinto', 'jeremystan', 'schooltds', 'aficionado', 'dmm613', 'jimmfleming', 'SVAIresearch', 'abhi1thakur', 'theagilist', 'PayrollJohnson', 'maxsklar', 'yncywy', 'mattfogel', 'j_trajkovic', 'jiffyclub', 'BlinkfireStats', 'simonchannet', 'dbamman', 'diazf_acm', 'darrenjw', 'demartsc', 'MaxKemman', 'd1ca1', 'stonse', 'ro_hit_', 'vincent_spruyt', 'ruchowdh', 'atpassos_ml', 'echenty', 'NicholasHould', 'leonpalafox', 'gorban', 'MatthewGombolay', 'dsacademybr']\n"
     ]
    }
   ],
   "source": [
    "# 0. id_str\t\t\t\t: ID of the user\n",
    "# 1. screen_name\t\t: Screen name of the user (@screen_name)\n",
    "# 2. followers_count\t: # Followers\n",
    "# 3. friends_count\t\t: # Following\n",
    "# 4. favourites_count\t: # Likes\n",
    "# 5. listed_count\t\t: Total number of list subscription and membership (?)\n",
    "# 6. statuses_count\t\t: # Tweets\n",
    "# 7. verified\t\t\t: True or False \n",
    "# 8. protected\t\t\t: True or False / if true can't crawl the account\n",
    "# 9. created_at\t\t\t: Creation time of the account / (2009-10-30 12:11:39)\n",
    "\n",
    "baseUsers = []\n",
    "baseLists = []\n",
    "usernames = []\n",
    "\n",
    "f = open(\"SimilarUsers.txt\", 'r', encoding='utf-8')\n",
    "\n",
    "# read base users\n",
    "baseUsers = f.readline().split(',')\n",
    "print(baseUsers)\n",
    "\n",
    "# read lists\n",
    "listNumber = int(f.readline())\n",
    "print(listNumber)\n",
    "for i in range(listNumber):\n",
    "    baseLists.append(f.readline().split(','))\n",
    "\n",
    "# read similar users\n",
    "for l in f.readlines():\n",
    "    if l.split(',')[8] == \"False\":\n",
    "        usernames.append(l.split(',')[1])\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(len(usernames))\n",
    "print(usernames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrankPasquale\n",
      "BaiduResearch\n"
     ]
    }
   ],
   "source": [
    "userTweets = []\n",
    "\n",
    "keyInd = 1\n",
    "client = UserClient(key[keyInd][0], key[keyInd][1], key[keyInd][2], key[keyInd][3])\n",
    "\n",
    "for user in usernames:\n",
    "    filename = \"tweets/\" + user + \".txt\"\n",
    "    f = open(filename, 'w', encoding='utf-8')\n",
    "\n",
    "    print(user)\n",
    "    protec = False\n",
    "    twe = []\n",
    "    \n",
    "    while(True):\n",
    "        try:\n",
    "            response = client.api.statuses.user_timeline.get(screen_name=user, count=200)\n",
    "            break\n",
    "        except Exception as err:\n",
    "            print(err.status_code)\n",
    "            print(err)\n",
    "            if err.status_code == 429:\n",
    "                sleep(60)\n",
    "                keyInd = (keyInd + 1)%len(key)\n",
    "            elif err.status_code == 404:\n",
    "                protec = True\n",
    "                break\n",
    "            else:\n",
    "                sleep(15)\n",
    "            \n",
    "            client = UserClient(key[keyInd][0], key[keyInd][1], key[keyInd][2], key[keyInd][3])\n",
    "            \n",
    "    if protec or len(response.data) == 0:\n",
    "        userSubs.append([])\n",
    "        print('protected!')\n",
    "        continue\n",
    "        \n",
    "    for d in response.data:\n",
    "        twe.append(d['text'])\n",
    "        f.write(d['text'])\n",
    "        f.write(\" \")\n",
    "        \n",
    "    maxID = response.data[-1]['id']\n",
    "    \n",
    "    while(True):\n",
    "        while(True):\n",
    "            try:\n",
    "                response = client.api.statuses.user_timeline.get(screen_name=user, count=200, max_id = maxID-1)\n",
    "                break\n",
    "            except Exception as err:\n",
    "                print(err.status_code)\n",
    "                print(err)\n",
    "                if err.status_code == 429:\n",
    "                    sleep(60)\n",
    "                    keyInd = (keyInd + 1)%len(key)\n",
    "                else:\n",
    "                    sleep(15)\n",
    "                client = UserClient(key[keyInd][0], key[keyInd][1], key[keyInd][2], key[keyInd][3])\n",
    "        \n",
    "        if len(response.data) == 0:\n",
    "            break\n",
    "        \n",
    "        for d in response.data:\n",
    "            twe.append(d['text'])\n",
    "            f.write(d['text'])\n",
    "            f.write(\" \")\n",
    "            \n",
    "        maxID = response.data[-1]['id']\n",
    "            \n",
    "    userTweets.append(twe)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
